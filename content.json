{"meta":{"title":"swolf的博客","subtitle":"记录我的学习生活","description":"欢迎光临，这里是swolf的博客~","author":"swolf","url":"https://mrswolf.github.io","root":"/"},"pages":[{"title":"About","date":"2022-03-27T14:12:14.858Z","updated":"2022-03-27T14:12:14.858Z","comments":false,"path":"about/index.html","permalink":"https://mrswolf.github.io/about/index.html","excerpt":"","text":"我是一名生物医学工程在读博士，主要研究方向为脑-机接口、机器学习和神经科学理论。 当然喽， 搞脑-机接口是找不到工作的， 咱又不是大佬~ 因此， 为了毕业能恰饭，也在自学CS的相关知识，希望能够转行T_T。 博客主要记录我在学习过程中的心得体会，也会抽空写一些BCI领域的知识，开心为主。 文章转载只需注明出处即可～ Enjoy!"},{"title":"Links","date":"2022-03-20T06:41:20.079Z","updated":"2022-03-20T06:41:20.079Z","comments":false,"path":"links/index.html","permalink":"https://mrswolf.github.io/links/index.html","excerpt":"","text":""},{"title":"Categories","date":"2022-03-20T06:42:42.326Z","updated":"2022-03-20T06:42:42.326Z","comments":false,"path":"categories/index.html","permalink":"https://mrswolf.github.io/categories/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2022-03-20T06:39:10.873Z","updated":"2022-03-20T06:39:10.873Z","comments":false,"path":"repository/index.html","permalink":"https://mrswolf.github.io/repository/index.html","excerpt":"","text":""},{"title":"Tags","date":"2022-03-21T07:21:55.131Z","updated":"2022-03-21T07:21:55.131Z","comments":false,"path":"tags/index.html","permalink":"https://mrswolf.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"矩阵微积分","slug":"matrix-calculus","date":"2022-03-27T16:00:00.000Z","updated":"2022-03-30T16:00:00.000Z","comments":false,"path":"matrix-calculus/","link":"matrix-calculus","permalink":"https://mrswolf.github.io/matrix-calculus/","excerpt":"矩阵微分和矩阵求导几乎是求解优化问题不可避免的必学内容，这一方面的内容老实说我很难完全掌握。这里记录一下一些常用的矩阵微分求导的规范和技巧。","text":"矩阵微分和矩阵求导几乎是求解优化问题不可避免的必学内容，这一方面的内容老实说我很难完全掌握。这里记录一下一些常用的矩阵微分求导的规范和技巧。 符号约定和布局规范 首先微分（differential）是在自变量微小变化下造成的因变量的微小变化，而导数（derivative）则是这种变化的速率。联系导数和微分的方程式叫做微分方程（differential equation）。比如函数y=f(x)y=\\mathrm{f}(x)y=f(x)，xxx的微小变化用符号dxdxdx表示，导致的yyy的微小变化用dydydy表示，xxx的变化引起的yyy的变化的速率用∂y∂x\\frac{\\partial y}{\\partial x}∂x∂y​表示，函数f(x)\\mathrm{f}(x)f(x)的微分方程就是： dy=∂y∂xdx\\begin{equation} dy=\\frac{\\partial y}{\\partial x} dx \\end{equation} dy=∂x∂y​dx​​ 矩阵微分/导数同函数微分/导数基本一致，只不过现在输入输出都是矩阵（向量也是矩阵的一种）的表现形式，比如y=f(x)\\mathbf{y} = \\mathrm{f}(\\mathbf{x})y=f(x)。这里用加粗大写字母表示矩阵，例如A\\mathbf{A}A；用加粗小写字母表示向量，例如a\\mathbf{a}a；用不加粗小写字母表示标量，例如aaa。 矩阵微分求导的难点在于没有固定的布局规范，导致有些文章和教材看起来互相冲突。比如对于∂y∂x\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}∂x∂y​，其中y∈Rm×1\\mathbf{y} \\in \\mathbb{R}^{m \\times 1}y∈Rm×1，x∈Rn×1\\mathbf{x} \\in \\mathbb{R}^{n \\times 1}x∈Rn×1，向量对向量的导数用矩阵形式可以有两种表示布局方案： 分子布局（numerator layout），这种布局要求y\\mathbf{y}y是按列排的，x\\mathbf{x}x是按行排列的（即xT\\mathbf{x}^TxT），最终输出∂y∂x∈Rm×n\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} \\in \\mathbb{R}^{m \\times n}∂x∂y​∈Rm×n，我觉得可以简单理解为与分子上矩阵元素相关的输出排列规则不变（跟原来一致），而与分母上矩阵元素相关输出排列规则应当是原来元素的转置。 分母布局（denominator layout），这种布局要求y\\mathbf{y}y是按行排的（即yT\\mathbf{y}^TyT），x\\mathbf{x}x是按列排列的，最终输出∂y∂x∈Rn×m\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} \\in \\mathbb{R}^{n \\times m}∂x∂y​∈Rn×m，正好跟分子布局相反。 为了方便推导和记忆，我选择采用分子布局，那么矩阵间导数的形式应当是这样的： yyy y∈Rm×1\\mathbf{y} \\in \\mathbb{R}^{m \\times 1}y∈Rm×1 Y∈Rm×n\\mathbf{Y} \\in \\mathbb{R}^{m \\times n}Y∈Rm×n xxx ∂y∂x\\frac{\\partial y}{\\partial x}∂x∂y​ ∂y∂x∈Rm×1\\frac{\\partial \\mathbf{y}}{\\partial x} \\in \\mathbb{R}^{m \\times 1}∂x∂y​∈Rm×1 ∂Y∂x∈Rm×n\\frac{\\partial \\mathbf{Y}}{\\partial x} \\in \\mathbb{R}^{m \\times n}∂x∂Y​∈Rm×n x∈Rp×1\\mathbf{x} \\in \\mathbb{R}^{p \\times 1}x∈Rp×1 ∂y∂x∈R1×p\\frac{\\partial y}{\\partial \\mathbf{x}} \\in \\mathbb{R}^{1 \\times p}∂x∂y​∈R1×p ∂y∂x∈Rm×p\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} \\in \\mathbb{R}^{m \\times p}∂x∂y​∈Rm×p X∈Rp×q\\mathbf{X} \\in \\mathbb{R}^{p \\times q}X∈Rp×q ∂y∂X∈Rq×p\\frac{\\partial y}{\\partial \\mathbf{X}} \\in \\mathbb{R}^{q \\times p}∂X∂y​∈Rq×p 微分规则 矩阵求导常需要用到sum rule、product rule和chain rule，其中链式法则不适用于matrix-by-scalar或scalar-by-matrix的形式，所以直接复合函数求导蛮麻烦的。wiki上说可以先用微分的规则求微分，然后再转换成导数的形式。 sum rule： h(x)=f(x)+g(x)dh(x)=df(x)+dg(x)\\begin{equation} \\begin{split} h(x) &amp;= f(x) + g(x)\\\\ dh(x) &amp;= df(x) + dg(x)\\\\ \\end{split} \\end{equation} h(x)dh(x)​=f(x)+g(x)=df(x)+dg(x)​​​ product rule： h(x)=f(x)g(x)dh(x)=df(x)g(x)+f(x)dg(x)\\begin{equation} \\begin{split} h(x) &amp;= f(x)g(x)\\\\ dh(x) &amp;= df(x)g(x) + f(x)dg(x)\\\\ \\end{split} \\end{equation} h(x)dh(x)​=f(x)g(x)=df(x)g(x)+f(x)dg(x)​​​ chain rule： h(x)=f(g(x))dh(x)=f(g(x+dx))−f(g(x))=f(g(x)+dg(x))−f(g(x))=df(y)∣y=g(x),dy=dg(x)\\begin{equation} \\begin{split} h(x) &amp;= f(g(x))\\\\ dh(x) &amp;= f(g(x+dx)) - f(g(x))\\\\ &amp;= f(g(x)+dg(x)) - f(g(x))\\\\ &amp;= df(y)|_{y=g(x),dy=dg(x)}\\\\ \\end{split} \\end{equation} h(x)dh(x)​=f(g(x))=f(g(x+dx))−f(g(x))=f(g(x)+dg(x))−f(g(x))=df(y)∣y=g(x),dy=dg(x)​​​​ trace trick： a=tr(a)tr(A)=tr(AT)tr(A+B)=tr(A)+tr(B)tr(ABC)=tr(BCA)=tr(CAB)\\begin{equation} \\begin{split} a &amp;= \\mathrm{tr}(a)\\\\ \\mathrm{tr}(\\mathbf{A}) &amp;= \\mathrm{tr}(\\mathbf{A}^T)\\\\ \\mathrm{tr}(\\mathbf{A}+\\mathbf{B}) &amp;= \\mathrm{tr}(\\mathbf{A}) + \\mathrm{tr}(\\mathbf{B})\\\\ \\mathrm{tr}(\\mathbf{A}\\mathbf{B}\\mathbf{C}) &amp;= \\mathrm{tr}(\\mathbf{B}\\mathbf{C}\\mathbf{A})\\\\ &amp;= \\mathrm{tr}(\\mathbf{C}\\mathbf{A}\\mathbf{B})\\\\ \\end{split} \\end{equation} atr(A)tr(A+B)tr(ABC)​=tr(a)=tr(AT)=tr(A)+tr(B)=tr(BCA)=tr(CAB)​​​ 总结矩阵常用的微分规则如下： 说明 表达式 微分结果 A\\mathbf{A}A不是X\\mathbf{X}X的函数 d(A)d\\left(\\mathbf{A}\\right)d(A) 0\\mathbf{0}0 aaa不是X\\mathbf{X}X的函数 d(aX)d(a\\mathbf{X})d(aX) adXad\\mathbf{X}adX d(X⊗Y)d(\\mathbf{X} \\otimes \\mathbf{Y})d(X⊗Y) (dX)⊗Y+X⊗(dY)(d\\mathbf{X}) \\otimes \\mathbf{Y} + \\mathbf{X} \\otimes (d\\mathbf{Y})(dX)⊗Y+X⊗(dY) d(X⊙Y)d(\\mathbf{X} \\odot \\mathbf{Y})d(X⊙Y) (dX)⊙Y+X⊙(dY)(d\\mathbf{X}) \\odot \\mathbf{Y} + \\mathbf{X} \\odot (d\\mathbf{Y})(dX)⊙Y+X⊙(dY) d(XT)d(\\mathbf{X}^T)d(XT) (dX)T(d\\mathbf{X})^T(dX)T 共轭转置 d(XH)d(\\mathbf{X}^H)d(XH) (dX)H(d\\mathbf{X})^H(dX)H d(X−1)d(\\mathbf{X}^{-1})d(X−1) −X−1(dX)X−1-\\mathbf{X}^{-1}(d\\mathbf{X})\\mathbf{X}^{-1}−X−1(dX)X−1 nnn是正整数 d(Xn)d(\\mathbf{X}^n)d(Xn) ∑i=0n−1Xi(dX)Xn−1−i\\sum_{i=0}^{n-1} \\mathbf{X}^i(d\\mathbf{X})\\mathbf{X}^{n-1-i}∑i=0n−1​Xi(dX)Xn−1−i d(eX)d(e^{\\mathbf{X}})d(eX) ∫01eaX(dX)e(1−a)Xda\\int_0^1 e^{a\\mathbf{X}}(d\\mathbf{X}) e^{(1-a)\\mathbf{X}}da∫01​eaX(dX)e(1−a)Xda d(log(X))d(\\mathrm{log}(\\mathbf{X}))d(log(X)) ∫0∞(X+zI)−1(dX)(X+zI)−1dz\\int_0^{\\infty} (\\mathbf{X}+z\\mathbf{I})^{-1}(d\\mathbf{X})(\\mathbf{X}+z\\mathbf{I})^{-1}dz∫0∞​(X+zI)−1(dX)(X+zI)−1dz d(tr(X))d(\\mathrm{tr}(\\mathbf{X}))d(tr(X)) tr(dX)\\mathrm{tr}(d\\mathbf{X})tr(dX) d(det⁡(X))d(\\det(\\mathbf{X}))d(det(X)) det⁡(X)tr(X−1dX)\\det(\\mathbf{X})\\mathrm{tr}(\\mathbf{X}^{-1}d\\mathbf{X})det(X)tr(X−1dX) d(log⁡(det⁡(X)))d(\\log(\\det(\\mathbf{X})))d(log(det(X))) tr(X−1dX)\\mathrm{tr}(\\mathbf{X}^{-1}d\\mathbf{X})tr(X−1dX) 微分-导数转换 在获得表达式的微分形式后，可以按如下规则进行导数形式的转化： 微分形式 导数形式 dy=adxdy=adxdy=adx ∂y∂x=a\\frac{\\partial y}{\\partial x}=a∂x∂y​=a dy=aTdxdy=\\mathbf{a}^Td\\mathbf{x}dy=aTdx ∂y∂x=aT\\frac{\\partial y}{\\partial \\mathbf{x}}=\\mathbf{a}^T∂x∂y​=aT dy=tr(AdX)dy=\\mathrm{tr}(\\mathbf{A}d\\mathbf{X})dy=tr(AdX) ∂y∂X=A\\frac{\\partial y}{\\partial \\mathbf{X}}=\\mathbf{A}∂X∂y​=A dy=adxd\\mathbf{y}=\\mathbf{a}dxdy=adx ∂y∂x=a\\frac{\\partial \\mathbf{y}}{\\partial x}=\\mathbf{a}∂x∂y​=a dy=Adxd\\mathbf{y}=\\mathbf{A} d\\mathbf{x}dy=Adx ∂y∂x=A\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}=\\mathbf{A}∂x∂y​=A dY=Adxd\\mathbf{Y}=\\mathbf{A}dxdY=Adx ∂Y∂x=A\\frac{\\partial \\mathbf{Y}}{\\partial x}=\\mathbf{A}∂x∂Y​=A","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://mrswolf.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://mrswolf.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"矩阵分解","slug":"矩阵分解","permalink":"https://mrswolf.github.io/tags/%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/"},{"name":"深度学习","slug":"深度学习","permalink":"https://mrswolf.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"主成分分析PCA","slug":"pca","date":"2019-12-10T16:00:00.000Z","updated":"2022-03-20T16:00:00.000Z","comments":false,"path":"principle-component-analysis/","link":"principle-component-analysis","permalink":"https://mrswolf.github.io/principle-component-analysis/","excerpt":"主成分分析（Principle Component Analysis，PCA）是常用的一种矩阵分解算法，PCA通过旋转原始空间来使得数据在各个正交轴上的投影最大，通过选择前几个正交轴可以实现数据降维的目的。","text":"主成分分析（Principle Component Analysis，PCA）是常用的一种矩阵分解算法，PCA通过旋转原始空间来使得数据在各个正交轴上的投影最大，通过选择前几个正交轴可以实现数据降维的目的。 PCA数学原理 优化问题 PCA的优化问题如下： arg max⁡Wtrace(WTXXTW)s.t.WTW=I\\begin{equation} \\begin{split} \\argmax_{\\mathbf{W}}\\quad &amp;\\mathrm{trace}\\left(\\mathbf{W}^T\\mathbf{X}\\mathbf{X}^T\\mathbf{W}\\right)\\\\ \\textrm{s.t.}\\quad &amp;\\mathbf{W}^T\\mathbf{W} = \\mathbf{I} \\end{split} \\end{equation} Wargmax​s.t.​trace(WTXXTW)WTW=I​​​ 其中X∈RM×N\\mathbf{X} \\in \\mathbb{R}^{M \\times N}X∈RM×N是数据，W∈RM×M\\mathbf{W} \\in \\mathbb{R}^{M \\times M}W∈RM×M是投影矩阵，NNN是样本点个数，MMM是特征个数。PCA要求数据X\\mathbf{X}X做零均值处理，优化问题的解可以转化为如下特征值分解问题的解： (XXT)W=WΛ\\begin{equation} \\left(\\mathbf{X}\\mathbf{X}^T\\right)\\mathbf{W} = \\mathbf{W}\\mathbf{\\Lambda} \\end{equation} (XXT)W=WΛ​​ 这里假设W\\mathbf{W}W的列向量按相应特征值的大小从大到小排列，保留W\\mathbf{W}W前K列即前K个成分的列向量W^\\hat{\\mathbf{W}}W^，降维后的数据特征为： X^=W^TX\\begin{equation} \\hat{\\mathbf{X}} = \\hat{\\mathbf{W}}^T\\mathbf{X} \\end{equation} X^=W^TX​​ 其中X^∈RK×N\\hat{\\mathbf{X}} \\in \\mathbb{R}^{K \\times N}X^∈RK×N。 实现分析 svd替代eig sklearn中的PCA实现并未使用eig而是使用svd，主要原因是svd比eig具有更好的数值稳定性（当然代价是其计算时间要比eig更长）。使用svd代替eig也是很多学者如Andrew Ng建议的策略，在StackExchange上也有关于svd和eig的相关讨论讨论1、讨论2。sklearn中直接对数据矩阵X\\mathbf{X}X而不是协方差矩阵XXT\\mathbf{X}\\mathbf{X}^TXXT做svd，其等价关系如下： X=UΣVTXXT=UΣ2UTW=UΛ=Σ2\\begin{equation} \\begin{split} \\mathbf{X} &amp;= \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T\\\\ \\mathbf{X}\\mathbf{X}^T &amp;= \\mathbf{U} \\mathbf{\\Sigma}^2 \\mathbf{U}^T\\\\ \\mathbf{W} &amp;= \\mathbf{U}\\\\ \\mathbf{\\Lambda} &amp;= \\mathbf{\\Sigma}^2 \\end{split} \\end{equation} XXXTWΛ​=UΣVT=UΣ2UT=U=Σ2​​​ sign ambiguity问题 sklearn的PCA代码中还考虑了svd的sign ambiguity问题，即每个奇异向量的符号在求解过程中是不确定的（例如，将uk\\mathbf{u}_kuk​和vk\\mathbf{v}_kvk​同时乘以-1也满足求解条件）。svd算法（包括eig）中的奇异向量符号只是确保数值稳定性的副产品，类似随机分配符号，并无实际意义。 sklearn使用svd_flip(u, v, u_based_descision=True)函数来确保输出确定性的奇异向量符号，例如，如果u_based_decision=True，则要求U\\mathbf{U}U的每一列奇异向量中绝对值最大的元素的符号始终为正，V\\mathbf{V}V也要相对的做出调整。 鉴于MATLAB是算法开发的标准之一，我很好奇MATLAB是如何处理SVD的sign ambiguity问题的。MATLAB的svd函数的官方文档中有这样一句话: Different machines and releases of MATLAB® can produce different singular vectors that are still numerically accurate. Corresponding columns in U and V can flip their signs, since this does not affect the value of the expression A = USV’. MATLAB的eig函数的官方文档中亦提到： For real eigenvectors, the sign of the eigenvectors can change. 可以看出MATLAB也未保证符号的确定性。同样在MATALB的社区里也有人问了这个问题，并引导我看了这篇Resolving the Sign Ambiguity in the Singular Value Decompostion的文献。 文献中指出，sklearn的svd_flip方法是一种临时方案（ad hoc），并未从数据分析或者解释的角度来解决sign ambiguity问题。解决sign ambiguity的核心是如何为奇异向量选择一个“有意义”的符号。什么叫“有意义”？比方说我们要研究4种品牌汽车的每公里耗油量，做了4次抽样，构成数据矩阵： X=[4223515111169101411691014]\\begin{equation} \\begin{split} \\mathbf{X} = \\begin{bmatrix} 4 &amp;22&amp;3 &amp;5 \\\\ 1 &amp;5 &amp;1 &amp;1 \\\\ 11&amp;69&amp;10&amp;14 \\\\ 11&amp;69&amp;10&amp;14 \\\\ \\end{bmatrix} \\end{split} \\end{equation} X=⎣⎡​411111​2256969​311010​511414​⎦⎤​​​​ 其中每一列是一种品牌汽车的耗油量，每一行为抽样情况，svd分解有X=UΣVT\\mathbf{X}=\\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^TX=UΣVT。我们来看一下numpy.linalg.svd计算得到的V\\mathbf{V}V的第一个奇异向量： v0=[−0.15−0.96−0.14−0.20]\\begin{equation} \\begin{split} \\mathbf{v}_0 = \\begin{bmatrix} -0.15 \\\\ -0.96 \\\\ -0.14 \\\\ -0.20 \\\\ \\end{bmatrix} \\end{split} \\end{equation} v0​=⎣⎡​−0.15−0.96−0.14−0.20​⎦⎤​​​​ v0\\mathbf{v}_0v0​实际指明了耗油量空间的一个向量，然而我们知道耗油量没有负值（如果有的话人类就拥有无限能源了），一个完全指向负的方向意义不大，如果改变v0\\mathbf{v}_0v0​的符号，变为： v0=[0.150.960.140.20]\\begin{equation} \\begin{split} \\mathbf{v}_0 = \\begin{bmatrix} 0.15 \\\\ 0.96 \\\\ 0.14 \\\\ 0.20 \\\\ \\end{bmatrix} \\end{split} \\end{equation} v0​=⎣⎡​0.150.960.140.20​⎦⎤​​​​ 结果就合理多了。 文献中指出，奇异向量的符号应当与大多数数据样本向量的符号相同，从几何上来看，奇异向量应当指向大多数向量指向的方向。下图是我从文献中截取的，深色蓝线是正确的奇异向量方向，浅色蓝线是数据向量。 翻译成数学语言（我按照自己的理解和习惯转化成优化问题，与文献的原始表述并不一致，不一定对，有兴趣的读者可以看原始文献😃)，纠正符号算法的核心是对于每一对奇异向量uk\\mathbf{u}_kuk​和vk\\mathbf{v}_kvk​，寻找符号sks_ksk​优化以下目标函数 arg max⁡sk∈{1,−1}sk(∑j=1NukTX⋅,j+∑i=1MXi,⋅vk)\\begin{equation} \\argmax_{s_k \\in \\{1,-1\\}}\\quad s_k \\left(\\sum_{j=1}^N \\mathbf{u}_k^T\\mathbf{X}_{\\cdot,j} + \\sum_{i=1}^M\\mathbf{X}_{i,\\cdot}\\mathbf{v}_k\\right) \\end{equation} sk​∈{1,−1}argmax​sk​(j=1∑N​ukT​X⋅,j​+i=1∑M​Xi,⋅​vk​)​​ 根据两项求和项的符号即可决定sks_ksk​的符号。对于可能存在的左右奇异向量符号冲突的情况（例如单独看左奇异向量有意义的符号是-1，单独看右奇异向量有意义的符号为1），该算法选择以求和绝对值最大的一项的符号为主（反应在上式就是两项求和）。文献中指出，该算法仅在上述求和项不为0的情况下有效（即在0附近奇异向量的符号可以为任意情况）。 Python版具体算法实现如下，Matlab可以使用这个版本： 123456789101112131415161718192021222324import warningsimport numpy as npdef sign_flip(u, s, vh=None): &quot;&quot;&quot;Flip signs of SVD or EIG. &quot;&quot;&quot; left_proj = 0 if vh is not None: left_proj = np.sum(s[:, np.newaxis]*vh, axis=-1) right_proj = np.sum(u*s, axis=0) total_proj = left_proj + right_proj signs = np.sign(total_proj) random_idx = (signs==0) if np.any(random_idx): signs[random_idx] = 1 warnings.warn(&quot;The magnitude is close to zero, the sign will become arbitrary.&quot;) u = u*signs if vh is not None: vh = signs[:, np.newaxis]*vh return u, s, vh","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://mrswolf.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://mrswolf.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"矩阵分解","slug":"矩阵分解","permalink":"https://mrswolf.github.io/tags/%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/"}]},{"title":"manjaro踩坑记(2022更新版)","slug":"manjaro踩坑记","date":"2019-05-23T16:00:00.000Z","updated":"2022-03-20T16:00:00.000Z","comments":false,"path":"my-manjaro-log/","link":"my-manjaro-log","permalink":"https://mrswolf.github.io/my-manjaro-log/","excerpt":"从2019年到2022年，manjaro发行版渡过了我的整个博士生涯。最近毕业重新装了系统，依然选择了最新的manjaro KDE Plasma 21.2.4（本来装了arch，大小问题不断被劝退了😜）。基本上这台linux主机要跟着我进入人生下一阶段，作为主力台式机也不打算再折腾了。安装过程中有一些新的学习体会（坑），在这里更新记录一下，希望能帮到有需要的朋友～","text":"从2019年到2022年，manjaro发行版渡过了我的整个博士生涯。最近毕业重新装了系统，依然选择了最新的manjaro KDE Plasma 21.2.4（本来装了arch，大小问题不断被劝退了😜）。基本上这台linux主机要跟着我进入人生下一阶段，作为主力台式机也不打算再折腾了。安装过程中有一些新的学习体会（坑），在这里更新记录一下，希望能帮到有需要的朋友～ 三年manjaro使用感悟 总体而言，作为一个linux系统小白，我对manjaro还是相当满意的，基本上，manjaro能满足我日常工作、娱乐的需要。manjaro系统安装显卡驱动和切换内核确实简单，只需在系统设置里改变即可，此外，arch文档翔实、aur软件丰富，大部分问题和需求都能找到对应的解决方案。linux下的开发、科研等涉及编程的工作确实要比windows下爽很多，一行命令搞定一堆安装包，然后用就完了，计算速度上似乎还比window下快一点（也许是心理作用？😁）。游戏方面steam上兼容linux的游戏还是挺多的，我常玩的饥荒、博德之门3等游戏都还能运行，偶尔有问题的话去protonDB上查一查还是能找到解决方案的，Valve不愧是要搞Steam Deck，估计这方面的兼容性支持会更好。 manjaro的缺点也是大多数linux系统的通病。为了满足正常使用，用户要做一些文本方面的配置，因此至少需要知道一些命令行的基础知识，这一点上远不如windows点点点直观，而且经常会出现一些奇奇怪怪的小问题，很影响使用体验。此外，linux系统的驱动相对windows依然是个大问题，驱动（尤其是显卡）出问题小白用户就直接GAME OVER了。最后，尽管9成需求我都能在manjaro下解决，仍有1成的需求由于各种原因必须要使用windows，我通常都是挂个windows虚拟机以备不时之需。 综上所述，我感觉manjaro系统适合满足以下条件的小白群体使用： 至少有1台独立的windows笔记本 不惧怕查阅资料，能科学上网 有一段完整的折腾时间 没有大型3A游戏或windows专用软件需求 安装manjaro的硬件不是最新的 不满足第一条的朋友还是老老实实用windows，在虚拟机里尝尝鲜得了😜，至于mac用户，俺们不跟土豪做朋友😢 系统选择与安装 manjaro提供了XFCE、KDE和GNOME三个桌面的环境的安装iso，我个人偏向于KDE，用着舒服，看着也不赖，倒是没必要再去捣鼓桌面美化啥的。这里建议下载Minimal LTS（长期支持版内核）安装镜像，以最大限度的避免硬件驱动等各种乱七八糟的问题。我选用的5.4版内核会一直支持到2025年12月，虽然听说kernel版本越高，硬件支持越好，但我实际装的时候最新版本各种诡异驱动问题（咱也不懂，就很玄学），所以还是从LTS出发，先达成一个基本可用的环境，再慢慢升级比较靠谱。 最好进BIOS把内存频率调低，比如2400MHz或2666MHz，我的内存一开始是3000MHz，很容易卡在进图形界面的步骤，查资料好像是啥显卡驱动没有加载上，需要Early Loading，但是我没有成功过，后来发现把内存频率调低就可以了，就很玄学 下载ISO文件后，用空余的U盘制作启动盘，插上U盘，进BIOS里关掉安全启动（Secure Boot）选项，把U盘的启动顺序调到前面，保存退出后就能进入manjaro的启动界面环境。 这里设置一下时区为Asia/Shanghai，选择以开源驱动boot，其它的选项都不用改，反正后期都能调整，核心目的是进入live环境。 进去之后会弹出欢迎界面，选择中文语言，一路点击下一步直到分区步骤（这里有时候会卡一会，可能是在联网检测啥东西）。 分区界面根据实际硬件的不同会有各种选项，抹除磁盘是自动分区安装的意思，适合不太清楚什么是分区的朋友，一路点点点就行。如果硬盘上还装了windows，manjaro还会有双引导的安装选项，可以说挺简单智能的了，桌面上的Installation Guide会有这些选项的详细介绍。由于manjaro会将所有可用空间全部归到root分区，我想单独划个home分区出来，所以选择了手动方法。这里我分了一个500MB的efi分区，2GB的swap分区（感觉用不上），64GB的root分区，剩下的都划到home分区了。划好分区后点击下一步，设置一些用户名、密码啥的，就可以进入安装过程，安装结束后重启、拔U盘，一切顺利的话就进入manjaro系统了～ 基础设置 设置manjaro更新镜像源 由于众所周知的原因，不更改镜像源和设置科学上网，大部分的开发工具在国内基本没法用。所以进系统的第一步是更改manjaro的系统更新镜像源，选择所在地区的镜像。 1sudo pacman-mirrors -c China -m rank &amp;&amp; sudo pacman -Syyu 该命令选择China地区的镜像源，并对系统做一次更新，因此可能需要等待一会，更新完最好直接重启。 安装Nvidia显卡驱动 重启过后，可以选择安装显卡驱动了。在系统设置-硬件设定里选择闭源驱动。我的显卡是GTX1080,好几年前的老卡了，video-nvidia-470xx驱动比较靠谱，如果想用最新的驱动，选择video-nvidia驱动就可以。 右键安装，输入管理员密码，安装完毕后重新启动，如果一切顺利进入桌面就表明没问题啦！！！ 科学上网 国内软件安装的大部分问题都是因为众所周知的原因，并且优秀开发和参考资料多为英文，因此科学上网属于一切学术研究和开发工作的必要条件，将科学上网作为终身学习的课题，花时间研究是值得的。 我采用的是proxychains结合v2ray的方式，不再使用之前的shadowsocks： 1sudo pacman -S proxychains-ng v2ray proxychains的配置文件为/etc/proxychains.conf，用kate打开该文件，修改最后一行： 1socks5 127.0.0.1 1080 v2ray的配置文件在/etc/v2ray目录下，这一部分有很多的学习资料了，我写了一个脚本自动获取生成配置文件config1.json，启动部分我采用手动挡输入命令，以后有空再研究自动挡的方式： 1v2ray -c /etc/v2ray/config1.json 科学上网的基本设置就结束了，浏览器可以在网络设置中选择socks5代理，转发本地1080端口；想要代理命令行程序，可以采用proxychains+命令的方式，比如： 1proxychains -q wget www.google.com 安装yay 安装yay yay可以当作pacman使用，也是用来安装AUR里软件包的工具，尽管manjaro自带的软件包管理器Pamac可以开启aur选项，以图形化界面的形式安装软件，但是Pamac似乎有许多bug，所以还是使用yay这一更常用的命令行工具。 manjaro下yay的安装非常简单，甚至不需要自己去编译: 1sudo pacman -S yay proxychains+yay 至此yay已经可以正常使用了，不过AUR里的软件包经常需要下载github等外网代码、文件，由于众所周知的原因，速度会慢的跟龟爬一样，所以最好还是搭配proxychains等工具使用。默认的yay采用go编译，这一版本同proxychains等代理工具有冲突，解决方案是用gcc-go重新编译，但是目前的v11.1.2版本的yay编译过不去，我没有能力解决问题，只能选择v11.1.1的yay。 1234yay -S base-devel gcc-gomkdir build &amp;&amp; cd build &amp;&amp; git clone https://aur.archlinux.org/yay.gitcd yayproxychains -q wget https://github.com/Jguer/yay/archive/v11.1.1.tar.gz 然后用kate修改PKGBUILD里如下部分： 123456789101112pkgname=yaypkgver=11.1.1 # 修改版本为11.1.1...makedepends=(&#x27;gcc-go&gt;=1.16&#x27;) # 修改为gcc-go&gt;=1.16source=(&quot;$&#123;pkgname&#125;-$&#123;pkgver&#125;.tar.gz::https://github.com/Jguer/yay/archive/v$&#123;pkgver&#125;.tar.gz&quot;)sha256sums=(&#x27;31ed6d828574601e77b8df90c6e734a230ea60531b704934038d52fe213c0898&#x27;) # 修改sha256的值... 由于yay会下载一些go的依赖，所以也要设置go的代理（众所周知😥），最后yay的目录下直接编译安装，此时的yay就可以跟proxychains完美配合啦，接下来我基本都使用yay安装manjaro官方和AUR的软件~ 123export GO111MODULE=onexport GOPROXY=https://goproxy.cnmakepkg -sic 忽略yay的更新 由于目前11.1.2版本的yay是manjaro默认的版本，更新系统时会自动替换老版本11.1.1，如果不想更新yay，可以在/etc/pacman.conf中忽略yay的更新，添加如下内容： 1IgnorePkg = yay 中文字体和中文输入法 开源中文字体 国内习惯了用windows自带的中文字体，比如楷体、宋体等，而这些在linux上因为版权问题发行版不会默认自带，需要我们自己“安装”使用（毕竟已经买了windows的笔记本了，用就完了哈哈哈）。当然有些字体是免费开源的： 1yay -S wqy-microhei wqy-microhei-lite wqy-zenhei noto-fonts-cjk adobe-source-han-sans-cn-fonts adobe-source-han-serif-cn-fonts 中文输入法 中文输入法采用fcitx5，输入以下命令安装： 1yay -S fcitx5 fcitx5-configtool fcitx5-chinese-addons fcitx5-qt fcitx5-gtk fcitx5-lua 安装完毕后用kate打开/etc/environment文件，在其中输入如下变量，然后注销再重新登陆，就可以使用中文输入法了： 123456GTK_IM_MODULE=fcitxQT_IM_MODULE=fcitxXMODIFIERS=@im=fcitxINPUT_METHOD=fcitxSDL_IM_MODULE=fcitxGLFW_IM_MODULE=ibus 默认拼音和英文的切换快捷键是ctrl+shift，不喜欢的话可以在系统设置-区域设置-输入法里进行调整，更多的相关设置也可以参考arch的中文输入法。 win10字体安装 想要安装win10的字体（十分有必要），AUR提供了ttf-ms-win10的安装包，不过不提供字体文件，需要自己从已有的win10系统（拷贝所有C:\\Windows\\Fonts下的字体文件）或从win10镜像中抽取字体文件，这里介绍如何抽取字体，首先从AUR拷贝tff-ms-win10： 12mkdir -p build &amp;&amp; cd buildgit clone https://aur.archlinux.org/ttf-ms-win10.git 然后挂载win10安装镜像，manjaro下只需要右键选择挂载ISO，Dolphin的左边即可出现ISO的访问文件路径，找出source文件夹下的install.esd或install.wim文件，把该文件拷贝到ttf-ms-win10文件夹下，执行如下命令解锁所有字体文件： 1wimextract install.esd 1 /Windows/&#123;Fonts/&quot;*&quot;.&#123;ttf,ttc&#125;,System32/Licenses/neutral/&quot;*&quot;/&quot;*&quot;/license.rtf&#125; --dest-dir . 然后修改PKGBUILD如下，添加的字体文件表示仿宋、黑体和楷体： 1234567_ttf_ms_win10_zh_cn=(simsun.ttc simfang.ttf simhei.ttf simkai.ttf # 增加这行内容simsunb.ttf msyh.ttc msyhbd.ttc msyhl.ttc 最后在ttf-ms-win10目录下执行安装命令，注意如果报错，大概率是当前抽取的字体文件中没有该字体，可以按照错误提示从网上下载ttf文件加入其中，或者在PKGBUILD里删掉该字体，毕竟只有中文字体比较重要： 1makepkg -sic --skipchecksums 安装完成后可以在系统设置-外观-字体管理中检查字体安装是否正确。 Ryzen随机卡死问题 这个问题三年前就遇到了，当时系统会随机卡死无响应（切terminal什么都没用）。这个问题是Ryzen处理器的一个bug，不知道现在的Ryzen系列有没有解决这个问题（我是AMD Ryzen 5 1600，也是老处理器了），总之我重装系统后依然有这个问题。解决方案就是disable C6 state，最好重启后再执行如下命令： 12yay -S disable-c6-systemdsudo modprobe msr 编辑/etc/modules-load.d/modules.conf，添加msr这一行，以便在启动时加载msr模块： 1msr 最后，启动如下service： 12sudo systemctl enable disable-c6.servicesudo systemctl start disable-c6.service 过去三年里基本没有出现这种随机卡死的问题了，感恩大佬。 其他优化 SSD优化 12sudo systemctl enable fstrim.timersudo systemctl start fstrim.timer 切换登陆终端 manjaro默认的zsh十分好用，不过非图形界面下的terminal还是bash，可以设为zsh： 12cat /etc/shellschsh -s /bin/zsh 切换内核版本 在系统设置-内核中点点就好啦，会安装一大堆东西，装完重启一下。 我切回了5.4的内核，新换内核后原内核最好保留一段时间，避免系统挂掉，还可以在初始启动界面选择从哪个内核进入系统。 常用软件安装 miniconda+python python作为我科研的主力编程语言，我选择用conda管理不同的python版本，首先安装miniconda，运行后一路回车或yes,最后会询问要不要把conda加入环境变量，这里可以选择no： 123wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.shchmod +x Miniconda3-latest-Linux-x86_64.sh./Miniconda3-latest-Linux-x86_64.sh 注意在terminal中还是没法直接使用conda的，因为不知道conda安装在哪，这里执行如下命令写入环境变量： 1～/miniconda3/bin/conda init zsh 退出terminal再重开，就能使用conda啦～ manjaro原来的terminal使用的是bash，2022版konsole使用了zsh（自带颜色、命令记忆补全，超级赞😃），如果想在bash中使用conda，将上面的zsh换成bash即可完成初始化的操作。bash的相关设置在.bashrc里，zsh的相关设置在.zshrc里，两者是默认不互通的。 老规矩，由于众所周知的原因，需要更换conda的镜像源，这里用清华tuna的镜像： 1conda config --set show_channel_urls yes 在.condarc文件里粘贴以下内容： 123456789101112131415channels: - defaultsshow_channel_urls: truedefault_channels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2custom_channels: conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch-lts: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud 另外pip的源最好也更改一下： 1pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple wps office办公 linux上最好用的office软件套装，搭配前面的win10字体可以做很多文字工作，不用切换到windows，安装如下AUR软件包： 1yay -S wps-office wps-office-mui-zh-cn ttf-wps-fonts 装完最好重启一下，目前我没遇到啥大问题，如果有问题的话可以看看arch的wiki。 vscode编辑器 超好用的编辑器，建议安装微软的二进制版本，可以多搜到一些好用的扩展包： 1yay -S visual-studio-code-bin virtualbox虚拟机 如果要使用微信之类国产软件的话，用虚拟机装个windows就行了，还能跟主系统隔离开来，台式机也不太考虑性能问题，这里建议参考manjaro的wiki，注意自己的linux内核版本，比如我的是linux54，别的版本需要替换下面命令中的linux54。 1sudo pacman -S virtualbox linux54-virtualbox-host-modules 安装完成后最好重启再完成后续工作。重启后需要安装扩展增强包，先看一下自己的virtualbox版本： 1vboxmanage --version 比如我是6.1.32r149290，那我就需要安装对应版本的扩展包，使用yay搜索可用的扩展包： 1yay virtualbox-ext-oracle 会弹出很多选项，要安装哪个输入序号回车。 最后需要将当前的用户加入vboxuser组，重启或注销就可以使用虚拟机啦～ 1sudo gpasswd -a $USER vboxusers LaTex论文写作 即使作为科研垃圾，也不得不产出论文😟。科技论文写作里LaTex可比word好用多了（前提是有模板），manjaro安装LaTex也很简单，配合vscode的LaTex扩展写论文不要太爽。 1yay -S texlive-most texlive-lang biber texlive-bibtexextra texlive-fontsextra 安完之后去vscode扩展里装LaTex Workshop，就可以开始写作了。如果要写中文论文，可以在vscode的settings.json里输入如下内容（一般vscode敲完latex-workshop.latex.tools之后就会自动补全后面值，在最前面添加一个就好）： 123456789101112131415161718192021222324252627&#123; &quot;latex-workshop.latex.tools&quot;: [ &#123; &quot;name&quot;: &quot;xelatexmk&quot;, &quot;command&quot;: &quot;latexmk&quot;, &quot;args&quot;: [ &quot;-synctex=1&quot;, &quot;-interaction=nonstopmode&quot;, &quot;-file-line-error&quot;, &quot;-xelatex&quot;, &quot;-outdir=%OUTDIR%&quot;, &quot;%DOC%&quot; ], &quot;env&quot;: &#123;&#125; &#125; ], &quot;latex-workshop.latex.recipes&quot;: [ &#123; &quot;name&quot;: &quot;xelatexmk 🔃&quot;, &quot;tools&quot;: [ &quot;xelatexmk&quot; ] &#125; ], &quot;latex-workshop.view.pdf.viewer&quot;: &quot;external&quot;, &quot;latex-workshop.latex.recipe.default&quot;: &quot;lastUsed&quot;&#125; 然后使用扩展菜单中的xelatexmk就可以编译中文内容啦～ OneDrive网盘 一直用onedrive习惯了，配合代理速度也还行，不太想用国内的其他网盘🤐。manjaro下使用这个项目的onedrive命令行来同步： 12yay -S onedrive-abrauneggonedrive 按照提示进行设置，设置完成后就可以使用了，因为我比较懒，没有研究自动同步功能，所以都是配合代理手动同步： 1proxychains -q onedrive --synchronize 反正又不是不能用，有空再看看自动同步咋搞。 xmind思维导图 AUR仓库里自带xmind8，直接输入以下命令即可： 1yay -S xmind 不过这一版本的xmind需要openjdk8的依赖才能运行，执行以下命令安装openjdk8： 1yay -S jdk8-openjdk 然后用kate或code打开/usr/share/xmind/XMind/XMind.ini文件，在文件开头添加如下文本： 12-vm/usr/lib/jvm/java-8-openjdk/bin 保存退出，xmind就能正常运行啦！ hexo博客管理 我的博客部署在github pages上，采用hexo管理，首先需要安装nodejs，用AUR的nvm管理不同的node版本： 1234yay -S nvm#使用nvm前需要运行这一句，可以将其写入.zshrc或.bashrcsource /usr/share/nvm/init-nvm.shnvm install node 然后安装npm及hexo： 123yay -S npmnpm install hexo-cli -gnpm install hexo-deployer-git --save hexo的使用方法可见参考文档。 其他软件 qBittorrent 还没有把硬盘填满吗？快使用qBittorrent吧～ 1yay -S qbittorrent yesplaymusic+spotify yesplaymusic是网易云音乐的替代，超漂亮的云音乐播放器，没有乱七八糟的功能，颜值党狂喜，安装简单（需要proxychains，老实讲大部分从github下载文件的都需要）: 1proxychains -q yay -S yesplaymusic 除此之外也可以安装spotify，让我们一起聆听IU的美妙歌声😍。 1proxychains -q yay -S spotify 大陆地区反正不挂代理能用，不能用再说，又没有交钱，要什么自行车～ PS：视频播放器直接用自带的VLC就好，功能强大，没啥不能播的。 文件名编码转换 windows默认GB2312，linux一般用UTF-8，从windows拷贝过来的中文文件名有时候是乱码，可以用convmv转化一下： 12345yay -S convmv# 测试转换是否成功，不实际执行转换convmv -f GBK -t UTF-8 -r your_folder_or_file# 执行实际转换convmv -f GBK -t UTF-8 -r --notest your_folder_or_file ufw防火墙服务 manjaro默认不带ufw防火墙，虽然我听说可以用iptables添加规则，但目前不懂怎么设置，先装了gufw： 123yay -S ufw gufwsudo systemctl enable ufw.servicesudo systemctl start ufw.service 开始菜单里就会出现防火墙配置的程序，先使用默认的就好，以后再研究。 远程桌面 如果有远程桌面的需求，比如连接windows笔记本、树莓派之类的，可以使用Remmina： 1yay -S remmina freerdp libvncserver spice-gtk caj2pdf 中国知网大部分论文都是caj格式（什么垃圾玩意），在linux下先转换成pdf格式再阅读比较方便，这里推荐caj2pdf工具，当然成功与否全部靠命。 1234proxychains -q yay -S caj2pdf# 转换caj到pdfcaj2pdf convert 某篇博士论文.caj -o 某篇博士论文.pdf colorpicker 还在为做PPT找不到好配色烦恼吗？安装colorpicker，运行命令，鼠标一点即可获取颜色的RGB和Hex值，获取完直接ctrl+c退出。 12proxychains -q yay -S colorpickercolorpicker Troubleshooting 这里放一些或许有的问题，方便大家排查，没事干时多看看KSystemlog~ spam log baloo limit","categories":[{"name":"linux","slug":"linux","permalink":"https://mrswolf.github.io/categories/linux/"}],"tags":[{"name":"manjaro","slug":"manjaro","permalink":"https://mrswolf.github.io/tags/manjaro/"},{"name":"linux","slug":"linux","permalink":"https://mrswolf.github.io/tags/linux/"}]},{"title":"MATLAB分布式集群搭建记录","slug":"MATLAB分布式集群搭建记录","date":"2019-05-19T16:00:00.000Z","updated":"2022-03-20T16:00:00.000Z","comments":false,"path":"matlab-mdce-log/","link":"matlab-mdce-log","permalink":"https://mrswolf.github.io/matlab-mdce-log/","excerpt":"本篇的内容可能过时啦 虽然我很久不用MATLAB处理日常工作，但是实验室主流依然是MATLAB（用Python的就那么几个T_T)。以前小伙伴们跑程序都是拷贝程序和数据到实验室的计算服务器上，手工开N个MATLAB窗口做运算。现在实验室规模扩大，这种手工的方式越来越繁琐。我从前用MATLAB时就想试试集群计算，奈何当时实验室没啥硬件条件，正好现在有机会，我干脆搭了个MATLAB集群供小伙伴使用。","text":"本篇的内容可能过时啦 虽然我很久不用MATLAB处理日常工作，但是实验室主流依然是MATLAB（用Python的就那么几个T_T)。以前小伙伴们跑程序都是拷贝程序和数据到实验室的计算服务器上，手工开N个MATLAB窗口做运算。现在实验室规模扩大，这种手工的方式越来越繁琐。我从前用MATLAB时就想试试集群计算，奈何当时实验室没啥硬件条件，正好现在有机会，我干脆搭了个MATLAB集群供小伙伴使用。 软硬件 硬件方面： 4核心, 16GB内存， 百兆网卡普通台式机(manage节点) 40核心, 128GB内存, 千兆网卡计算服务器(compute1节点) 346TB存储， 千兆网卡存储服务器(storage节点) 软件方面： Windows10专业版系统 centos7 matlab2017b 网络环境： 192.168.130.12(matlab-manage.xxx.org) – manage节点 192.168.130.11(matlab-compute1.xxx.org) – compute1节点 192.168.130.10 – storage节点 MATLAB的帮助文档中提出，想要使用集群计算服务应该满足以下条件： 推荐一个CPU核心最多创建一个worker 推荐每个worker最少可以使用2GB内存 最少5GB的硬盘空间容纳暂时性的数据文件 计算集群之间应当使用同构的计算架构(要求计算节点的硬件配置、系统和软件配置一致) 集群安装配置 ip域名设置 修改compute节点和manage节点的计算机名、ip地址以及DNS域名解析，例如compute1节点的计算机名为matlab-compute1.xxx.org(xxx.org为后缀域名)，DNS域名也应该为matlab-compute1.xxx.org，ip地址为192.168.130.11。 MATLAB分布式计算服务似乎要求计算机名要添加后缀域名(xxx.org)，否则在集群测试时会有解析不匹配的警告，Windows专业版可在这台电脑-属性-更改设置-更改-其他中添加主DNS后缀。 manage节点 在manage节点安装matlab2017b，manage节点在安装过程中应该勾选MATLAB License Server和MATLAB Distributed Computing Server工具箱，前者为集群提供license认证服务，后者是分布式计算的核心服务组件。对于破解版的MATLAB，应该输入floating license的key而不是standalone的key，才能安装MATLAB License Server。安装完毕（并破解）后，在Windows服务选项卡中启动MATLAB License Server服务。 同时修改C:\\Program Files\\MATLAB\\R2017b\\licenses\\network.lic为如下内容 12SERVER this_host ANYUSE_SERVER 修改C:\\Program Files\\MATLAB\\R2017b\\toolbox\\distcomp\\bin\\mdce_def.bat其中的security level为2 1set SECURITY_LEVEL=2 设置security level为2的效果是要求用户在使用分布式计算服务时输入用户名，从而可以监控集群使用情况。 启动MATLAB，切换到C:\\Program Files\\MATLAB\\R2017b\\toolbox\\distcomp\\bin目录下，在MATLAB命令行窗口输入如下命令安装并启动mdce服务 12!mdce install !mdce start 最好在MATLAB命令行窗口内启动mdce服务，如果在Windows服务选项卡中启动服务，会出现权限问题导致集群worker无法连接。 启动mdce服务后最好双击运行C:\\Program Files\\MATLAB\\R2017b\\toolbox\\distcomp\\bin\\addMatlabToWindowsFirewall.bat文件（我的做法是直接关闭Windows防火墙避免多余的问题） compute节点 compute节点的安装配置同manage节点，仅以下内容不同 安装matlab时无需勾选MATLAB License Server工具箱 修改C:\\Program Files\\MATLAB\\R2017b\\licenses\\network.lic为如下内容 12SERVER matlab-manage.xxx.org ANYUSE_SERVER 此外为了跟storage节点连接，compute节点需要安装NFS服务，在程序和功能-启用或关闭Windows功能中勾选NFS服务 storage节点 storage节点设置NFS服务，NFS服务端安装和配置网上都有，我就不写了。 添加集群节点 在manage节点运行C:\\Program Files\\MATLAB\\R2017b\\toolbox\\distcomp\\bin\\admincenter.bat，启动管理面板，点击Add or Find，添加manage节点和compute1节点，添加完毕后，点击Test Connectivity，测试通过如下图 在MATLAB Job Scheduler面板点击start启动scheduler，输入名称，选择scheduler的节点为matlab-manage.xxx.org，因为security level为2，还需要设置管理员的密码。 设置好scheduler后，右键scheduler点击Start Workers，勾选compute1节点，设置启动的worker数量（我只有40个核心，所以启动40个worker）。 客户端配置和使用 MATLAB集群计算要求客户端的matlab版本和服务端一致，因为我服务端安装的是2017b，客户端也应该是matlab2017b。客户端可以选择standalone安装方式，也需要安装mdce服务添加防火墙配置并启动。 如果客户端想直接使用NFS服务，也需要在程序和功能-启用或关闭Windows功能中勾选NFS服务。 安装完毕后，在MATLAB主页中的Parallel选项选择Discover Cluster，勾选On your network，点击Next等待发现集群mjs40_2，选择集群，点击Next，Finish，就可以使用集群了，集群的使用情况可以在Parallel选项里Monitor Jobs查看。 这里提供两个matlab并行计算脚本检测集群配置是否正确 12345678910%This demo shows how to use distributed computing serverprimeNumbers = primes(uint64(2^21));compositeNumbers = primeNumbers.*primeNumbers(randperm(numel(primeNumbers)));factors = zeros(numel(primeNumbers),2);tic;parfor idx = 1:numel(compositeNumbers) factors(idx,:) = factor(compositeNumbers(idx));endtoc 1234567891011%This demo shows how to load data from nfs server, target_folder is nfs server ip addresstarget_folder=&#x27;\\\\192.168.130.10\\pub\\data\\&#x27;;factors=zeros(400,2);tic;parfor i=0:399 tmp = load([target_folder,num2str(i),&#x27;.mat&#x27;]); data = tmp.data; factors(i+1, :)=factor(data);endtoc","categories":[{"name":"分布式计算","slug":"分布式计算","permalink":"https://mrswolf.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97/"}],"tags":[{"name":"matlab","slug":"matlab","permalink":"https://mrswolf.github.io/tags/matlab/"}]},{"title":"共空间模式CSP","slug":"CSP","date":"2019-01-05T16:00:00.000Z","updated":"2022-03-23T16:00:00.000Z","comments":false,"path":"common-spatial-pattern/","link":"common-spatial-pattern","permalink":"https://mrswolf.github.io/common-spatial-pattern/","excerpt":"共空间模式（common spatial pattern，CSP）是脑-机接口领域常用的一类空间滤波算法，尤其在运动想象范式分类上具有较好的效果，是运动想象范式的基准算法之一。","text":"共空间模式（common spatial pattern，CSP）是脑-机接口领域常用的一类空间滤波算法，尤其在运动想象范式分类上具有较好的效果，是运动想象范式的基准算法之一。 目前，CSP及其改进算法的发展速度放缓，看似到达了算法的瓶颈期，近几年鲜少有较大的突破。尽管如此，CSP中的一些思想对脑-机接口算法设计仍然具有一定的启示作用。本文从CSP原始算法出发，讨论其变形和一系列改进算法，试图阐明其中的数学思想与神经科学的联系。 CSP数学原理 原始形式 2000年Graz的论文中提出的CSP是为2分类问题设计的，形式较为简单，然而如果你读CSP相关论文，就会发现CSP存在至少三种表述形式。这三种方式相互联系，又有所区分，很容易让初学者陷入混乱，不知道哪一种是正确形式。我接下来从2000年Graz的论文中的算法出发，讨论三种形式间的联系和不同。 假设我们采集脑电数据为{(X(i),y(i))}i=1Nt\\{(\\mathbf{X}^{(i)},y^{(i)})\\}_{i=1}^{N_t}{(X(i),y(i))}i=1Nt​​，其中X(i)∈RNc×Ns\\mathbf{X}^{(i)} \\in \\mathbb{R}^{N_c \\times N_s}X(i)∈RNc​×Ns​是第iii个样本，NcN_cNc​是EEG导联个数，NsN_sNs​是采样时间点的个数，y(i)y^{(i)}y(i)是iii个样本的标签，NtN_tNt​为总样本个数。第iii个样本的协方差矩阵C(i)∈RNc×Nc\\mathbf{C}^{(i)} \\in \\mathbb{R}^{N_c \\times N_c}C(i)∈RNc​×Nc​为（所有样本均经过零均值处理）: C(i)=1Nt−1X(i)(X(i))T\\begin{equation} \\mathbf{C}^{(i)} = \\frac{1}{N_t-1}\\mathbf{X}^{(i)}\\left(\\mathbf{X}^{(i)}\\right)^T \\end{equation} C(i)=Nt​−11​X(i)(X(i))T​​ 第lll类的平均协方差矩阵C‾Cl\\overline{\\mathbf{C}}\\vphantom{C}^lCCl为： C‾Cl=1∣Il∣∑i∈IlC(i)tr(C(i))\\begin{equation} \\overline{\\mathbf{C}}\\vphantom{C}^l = \\frac{1}{|\\mathcal{I}_l|} \\sum_{i \\in \\mathcal{I}_l} \\frac{\\mathbf{C}^{(i)}}{\\mathrm{tr}\\left(\\mathbf{C}^{(i)}\\right)} \\end{equation} CCl=∣Il​∣1​i∈Il​∑​tr(C(i))C(i)​​​ 其中Il\\mathcal{I}_lIl​是标签为lll的样本索引集合，∣Il∣|\\mathcal{I}_l|∣Il​∣则是集合中样本的个数，tr(⋅)\\mathrm{tr}\\left(\\cdot\\right)tr(⋅)求矩阵的迹。 为什么要使用tr(⋅)\\mathrm{tr}\\left(\\cdot\\right)tr(⋅)来对协方差矩阵实现迹归一化？ 1990年Koles等人的文章中指出，迹归一化的目的是为了消除&quot;被试间脑电信号幅值的变化&quot;，注意到Koles等人的主要目的是区分健康人群和精神疾病人群，而个体的脑电幅值是有差异的。方差可以表征信号在时域上的能量高低，不同人群的协方差矩阵的绝对值不同。为了消除这种差异带来的影响，利用tr()\\mathrm{tr}()tr()函数求得所有导联的总体能量，并对协方差矩阵迹归一化，从而安排除不同个体带来的干扰。Graz小组对同一个体不同试次的数据沿用了这种归一化方式，试图消除试次间的差异，发现也有一定的作用，这种迹归一化方式就一直流传下来。 然而，有些分析显示这种归一化方式会不利于最终的空间滤波器排序，建议不要使用迹归一化。实践中使不使用迹归一化还是要具体问题具体分析。我的感觉是没有必要在这里加入迹归一化，因为很多时候EEG预处理阶段已经使用了各种归一化手段来减弱噪声的影响。 接下来构建复合协方差矩阵C‾Cc\\overline{\\mathbf{C}}\\vphantom{C}_cCCc​，并特征值分解，构建白化（whitening）矩阵P\\mathbf{P}P： C‾Cc=C‾C1+C‾C2=UcΛc(Uc)TP=(Λc)−1/2(Uc)T\\begin{equation} \\begin{split} \\overline{\\mathbf{C}}\\vphantom{C}^c &amp;= \\overline{\\mathbf{C}}\\vphantom{C}^1 + \\overline{\\mathbf{C}}\\vphantom{C}^2\\\\ &amp;= \\mathbf{U}^c \\mathbf{\\Lambda}^c \\left(\\mathbf{U}^c\\right)^T\\\\ \\mathbf{P} &amp;= \\left(\\mathbf{\\Lambda}^c\\right)^{-1/2}\\left(\\mathbf{U}^c\\right)^T \\end{split} \\end{equation} CCcP​=CC1+CC2=UcΛc(Uc)T=(Λc)−1/2(Uc)T​​​ 其中Uc\\mathbf{U}^cUc是特征向量矩阵（每一列是特征向量），Λc\\mathbf{\\Lambda}^cΛc是由特征值组成的对角矩阵。P\\mathbf{P}P是白化矩阵，使得PC‾Cc(P)T=I\\mathbf{P}\\overline{\\mathbf{C}}\\vphantom{C}^c\\left(\\mathbf{P}\\right)^T= \\mathbf{I}PCCc(P)T=I成立，注意到： I=PC‾Cc(P)T=PC‾C1(P)T+PC‾C2(P)T=S1+S2S1=PC‾C1(P)TS2=PC‾C2(P)T\\begin{equation} \\begin{split} \\mathbf{I} &amp;= \\mathbf{P}\\overline{\\mathbf{C}}\\vphantom{C}^c\\left(\\mathbf{P}\\right)^T\\\\ &amp;= \\mathbf{P}\\overline{\\mathbf{C}}\\vphantom{C}^1\\left(\\mathbf{P}\\right)^T + \\mathbf{P}\\overline{\\mathbf{C}}\\vphantom{C}^2\\left(\\mathbf{P}\\right)^T\\\\ &amp;= \\mathbf{S}^1 + \\mathbf{S}^2\\\\ \\mathbf{S}^1 &amp;= \\mathbf{P}\\overline{\\mathbf{C}}\\vphantom{C}^1\\left(\\mathbf{P}\\right)^T\\\\ \\mathbf{S}^2 &amp;= \\mathbf{P}\\overline{\\mathbf{C}}\\vphantom{C}^2\\left(\\mathbf{P}\\right)^T \\end{split} \\end{equation} IS1S2​=PCCc(P)T=PCC1(P)T+PCC2(P)T=S1+S2=PCC1(P)T=PCC2(P)T​​​ 对S1\\mathbf{S}^1S1或S2\\mathbf{S}^2S2做特征值分解，得到最终的空间滤波器W\\mathbf{W}W： S1=UΛ1(U)TS2=UΛ2(U)TW=PTU\\begin{equation} \\begin{split} \\mathbf{S}^1 &amp;= \\mathbf{U} \\mathbf{\\Lambda}^1 \\left(\\mathbf{U}\\right)^T\\\\ \\mathbf{S}^2 &amp;= \\mathbf{U} \\mathbf{\\Lambda}^2 \\left(\\mathbf{U}\\right)^T\\\\ \\mathbf{W} &amp;= \\mathbf{P}^T\\mathbf{U} \\end{split} \\end{equation} S1S2W​=UΛ1(U)T=UΛ2(U)T=PTU​​​ 其中S1\\mathbf{S}^1S1和S2\\mathbf{S}^2S2具有相同的特征向量U\\mathbf{U}U（这也是共空间模式名称的由来），这里假设U\\mathbf{U}U的每一列是按照Λ1\\mathbf{\\Lambda}^1Λ1中的特征值从大到小排列的，可以看出Λ2\\mathbf{\\Lambda}^2Λ2中的特征值是从小到大排列的，满足Λ1+Λ2=I\\mathbf{\\Lambda}^1+\\mathbf{\\Lambda}^2=\\mathbf{I}Λ1+Λ2=I的关系。 为什么S1\\mathbf{S}^1S1和S2\\mathbf{S}^2S2具有同样的特征向量和此消彼长的特征值关系？ 这一点可以简单的证明如下： 假设uj\\mathbf{u}_juj​和λj1\\lambda_j^{1}λj1​分别是S1\\mathbf{S}^1S1的特征向量和特征值，即： S1uj=λj1uj\\begin{equation} \\mathbf{S}^1\\mathbf{u}_j=\\lambda_j^{1}\\mathbf{u}_j \\end{equation} S1uj​=λj1​uj​​​ 注意到S1+S2=I\\mathbf{S}^1+\\mathbf{S}^2=\\mathbf{I}S1+S2=I，把上式中的S1\\mathbf{S}^1S1置换掉可得： (I−S2)uj=λj1uj\\begin{equation} \\left(\\mathbf{I}-\\mathbf{S}^2\\right)\\mathbf{u}_j=\\lambda_j^{1}\\mathbf{u}_j \\end{equation} (I−S2)uj​=λj1​uj​​​ 把上式变形一下可得： S2uj=(1−λj1)uj\\begin{equation} \\mathbf{S}^2\\mathbf{u}_j=(1-\\lambda_j^{1})\\mathbf{u}_j \\end{equation} S2uj​=(1−λj1​)uj​​​ 显然uj\\mathbf{u}_juj​也是S2\\mathbf{S}^2S2的特征向量，只不过其特征值为1−λj11-\\lambda_j^{1}1−λj1​。 脑-机接口中的空间滤波器是一组作用于EEG导联信号的向量，目的是为了加强空间分辨率或信噪比，可以简单理解为对导联信号的线性组合。事实上，不少空间滤波器本质上就是某些特征值分解问题的特征向量。 以上就是原始CSP算法的基本内容，在得到空间滤波器矩阵W\\mathbf{W}W后（W\\mathbf{W}W的每一列都是一个空间滤波器），选择前后各mmm个空间滤波器构建特征向量x~\\tilde{\\mathbf{x}}x~如下： W~=[w1,⋯ ,wm,wNc−m+1,⋯ ,wNc]x~=log(diag(W~TXXTW~)tr(W~TXXTW~))\\begin{equation} \\begin{split} \\tilde{\\mathbf{W}} &amp;= \\begin{bmatrix} \\mathbf{w}_1, \\cdots, \\mathbf{w}_m, \\mathbf{w}_{N_c-m+1}, \\cdots, \\mathbf{w}_{N_c} \\end{bmatrix}\\\\ \\tilde{\\mathbf{x}} &amp;= \\mathrm{log}\\left(\\frac{\\mathrm{diag}\\left(\\tilde{\\mathbf{W}}^T\\mathbf{X}\\mathbf{X}^T\\tilde{\\mathbf{W}}\\right)}{\\mathrm{tr}\\left(\\tilde{\\mathbf{W}}^T\\mathbf{X}\\mathbf{X}^T\\tilde{\\mathbf{W}}\\right)}\\right) \\end{split} \\end{equation} W~x~​=[w1​,⋯,wm​,wNc​−m+1​,⋯,wNc​​​]=log⎝⎛​tr(W~TXXTW~)diag(W~TXXTW~)​⎠⎞​​​​ 其中wm\\mathbf{w}_mwm​表示W\\mathbf{W}W的第mmm列，W~\\tilde{\\mathbf{W}}W~是最终选定的空间滤波器组，diag(⋅)\\mathrm{diag}\\left(\\cdot\\right)diag(⋅)是矩阵主对角线上的元素，log(⋅)\\mathrm{log}\\left(\\cdot\\right)log(⋅)对每个元素做对数变换，其主要目的是使数据近似正太分布。获得特征向量x~\\tilde{\\mathbf{x}}x~后，则可以使用线性判别分析（Linear Discriminant Analysis，LDA）、支持向量机（Support Vector Machine，SVM）等常见的机器学习模型构建分类器。 以上就是原始CSP算法的基本内容，简单回顾一下CSP算法，不难发现CSP实质求解的是这样一个问题，寻找正交矩阵W\\mathbf{W}W对角化C‾C1\\overline{\\mathbf{C}}\\vphantom{C}^1CC1和C‾C2\\overline{\\mathbf{C}}\\vphantom{C}^2CC2，使得以下条件成立： WTC‾C1W=Λ1WTC‾C2W=Λ2Λ1+Λ2=I\\begin{equation} \\begin{split} \\mathbf{W}^T\\overline{\\mathbf{C}}\\vphantom{C}^1\\mathbf{W} &amp;= \\mathbf{\\Lambda}^1\\\\ \\mathbf{W}^T\\overline{\\mathbf{C}}\\vphantom{C}^2\\mathbf{W} &amp;= \\mathbf{\\Lambda}^2\\\\ \\mathbf{\\Lambda}^1 + \\mathbf{\\Lambda}^2 &amp;= \\mathbf{I}\\\\ \\end{split} \\end{equation} WTCC1WWTCC2WΛ1+Λ2​=Λ1=Λ2=I​​​ 让我们对以上的公式做一些变换，把第一个和第二个公式相加： WT(C‾C1+C‾C2)W=Λ1+Λ2=I\\begin{equation} \\begin{split} \\mathbf{W}^T\\left(\\overline{\\mathbf{C}}\\vphantom{C}^1+\\overline{\\mathbf{C}}\\vphantom{C}^2\\right)\\mathbf{W} &amp;= \\mathbf{\\Lambda}^1 + \\mathbf{\\Lambda}^2\\\\ &amp;= \\mathbf{I}\\\\ \\end{split} \\end{equation} WT(CC1+CC2)W​=Λ1+Λ2=I​​​ 又因为W\\mathbf{W}W是正交矩阵，故W−1=WT\\mathbf{W}^{-1}=\\mathbf{W}^TW−1=WT，从而： (C‾C1+C‾C2)W=W\\begin{equation} \\left(\\overline{\\mathbf{C}}\\vphantom{C}^1+\\overline{\\mathbf{C}}\\vphantom{C}^2\\right)\\mathbf{W} = \\mathbf{W} \\end{equation} (CC1+CC2)W=W​​ 把上式代入C‾C1W=WΛ1\\overline{\\mathbf{C}}\\vphantom{C}^1\\mathbf{W}=\\mathbf{W}\\mathbf{\\Lambda}^1CC1W=WΛ1右边的W\\mathbf{W}W，可得： C‾C1W=(C‾C1+C‾C2)WΛ1\\begin{equation} \\overline{\\mathbf{C}}\\vphantom{C}^1\\mathbf{W} = \\left(\\overline{\\mathbf{C}}\\vphantom{C}^1+\\overline{\\mathbf{C}}\\vphantom{C}^2\\right)\\mathbf{W}\\mathbf{\\Lambda}^1 \\end{equation} CC1W=(CC1+CC2)WΛ1​​ 这个式子看起来很像特征向量定义的公式C‾C1W=WΛ1\\overline{\\mathbf{C}}\\vphantom{C}^1\\mathbf{W}=\\mathbf{W}\\mathbf{\\Lambda}^1CC1W=WΛ1，只不过等式右边多了一个矩阵C‾C1+C‾C2\\overline{\\mathbf{C}}\\vphantom{C}^1+\\overline{\\mathbf{C}}\\vphantom{C}^2CC1+CC2。这类形式的特征值求解问题叫广义特征值问题，求解广义特征值问题是脑-机接口领域传统空间滤波方法的基础，大量的算法都可以转化为这一形式。 第二种形式 CSP的第二种形式与C‾C1W=(C‾C1+C‾C2)WΛ1\\overline{\\mathbf{C}}\\vphantom{C}^1\\mathbf{W} = \\left(\\overline{\\mathbf{C}}\\vphantom{C}^1+\\overline{\\mathbf{C}}\\vphantom{C}^2\\right)\\mathbf{W}\\mathbf{\\Lambda}^1CC1W=(CC1+CC2)WΛ1密切相关，首先我们需要了解一个数学概念广义雷利商（generalized Rayleigh quotient）。 广义雷利商λ\\lambdaλ长这样： λ=wTAwwTBwA,B⪰0\\begin{equation} \\begin{split} \\lambda =\\frac{\\mathbf{w}^T\\mathbf{A}\\mathbf{w}}{\\mathbf{w}^T\\mathbf{B}\\mathbf{w}}\\\\ \\mathbf{A}, \\mathbf{B} \\succeq 0\\\\ \\end{split} \\end{equation} λ=wTBwwTAw​A,B⪰0​​​ 其中A\\mathbf{A}A和B\\mathbf{B}B为半正定矩阵，w\\mathbf{w}w是列向量。 如果我们求如下广义雷利商的优化问题，就会有一些有趣的结果： max⁡wwTAwwTBw\\begin{equation} \\begin{split} \\max_{\\mathbf{w}} \\frac{\\mathbf{w}^T\\mathbf{A}\\mathbf{w}}{\\mathbf{w}^T\\mathbf{B}\\mathbf{w}} \\end{split} \\end{equation} wmax​wTBwwTAw​​​​ 寻找w\\mathbf{w}w使得λ\\lambdaλ最大，通常令wTBw=1\\mathbf{w}^T\\mathbf{B}\\mathbf{w}=1wTBw=1，在数学上可以等价为求解下式： Aw=λBw\\begin{equation} \\mathbf{A}\\mathbf{w} = \\lambda\\mathbf{B}\\mathbf{w} \\end{equation} Aw=λBw​​ 这个公式就是上一节提到的广义特征值问题，也就是说，寻找w\\mathbf{w}w使广义雷利商最大的优化问题可以等价为求解A\\mathbf{A}A和B\\mathbf{B}B的广义特征值问题。如果我们继续寻找能够使λ\\lambdaλ第二大、第三大的w\\mathbf{w}w，就会发现只要解出广义特征值问题的矩阵形式即可： AW=BWΛ\\begin{equation} \\mathbf{A}\\mathbf{W} = \\mathbf{B}\\mathbf{W}\\mathbf{\\Lambda} \\end{equation} AW=BWΛ​​ 不难发现，上一节中推导的CSP求解问题可以变形为求解广义雷利商问题： C‾C1W=(C‾C1+C‾C2)WΛ1 ⟺ arg max⁡Wtr(WTC‾C1W)tr(WT(C‾C1+C‾C2)W)\\begin{equation} \\begin{split} \\overline{\\mathbf{C}}\\vphantom{C}^1\\mathbf{W} = \\left(\\overline{\\mathbf{C}}\\vphantom{C}^1+\\overline{\\mathbf{C}}\\vphantom{C}^2\\right)\\mathbf{W}\\mathbf{\\Lambda}^1 \\ \\ \\Longleftrightarrow \\ \\ \\argmax_{\\mathbf{W}} \\frac{\\mathrm{tr}\\left(\\mathbf{W}^T\\overline{\\mathbf{C}}\\vphantom{C}^1\\mathbf{W}\\right)}{\\mathrm{tr}\\left(\\mathbf{W}^T\\left(\\overline{\\mathbf{C}}\\vphantom{C}^1+\\overline{\\mathbf{C}}\\vphantom{C}^2\\right)\\mathbf{W}\\right)} \\end{split} \\end{equation} CC1W=(CC1+CC2)WΛ1 ⟺ Wargmax​tr(WT(CC1+CC2)W)tr(WTCC1W)​​​​ 其中应满足WT(C‾C1+C‾C2)W=I\\mathbf{W}^T\\left(\\overline{\\mathbf{C}}\\vphantom{C}^1+\\overline{\\mathbf{C}}\\vphantom{C}^2\\right)\\mathbf{W}=\\mathbf{I}WT(CC1+CC2)W=I的约束条件。 这就是CSP常见的第二种形式，它跟原始形式在数学上相互等价，由于分母在约束下是单位矩阵，也常写作如下优化问题： arg max⁡Wtr(WTC‾C1W)s.t.WT(C‾C1+C‾C2)W=I\\begin{equation} \\begin{split} \\argmax_{\\mathbf{W}}\\quad &amp;\\mathrm{tr}\\left(\\mathbf{W}^T\\overline{\\mathbf{C}}\\vphantom{C}^1\\mathbf{W}\\right)\\\\ \\textrm{s.t.}\\quad &amp;\\mathbf{W}^T\\left(\\overline{\\mathbf{C}}\\vphantom{C}^1+\\overline{\\mathbf{C}}\\vphantom{C}^2\\right)\\mathbf{W} = \\mathbf{I}\\\\ \\end{split} \\end{equation} Wargmax​s.t.​tr(WTCC1W)WT(CC1+CC2)W=I​​​ 第三种形式 CSP的第三种表述形式需要绕点弯路。首先还是从CSP的原始形式出发，即寻找正交矩阵W\\mathbf{W}W使得以下条件成立： WTC‾C1W=Λ1WTC‾C2W=Λ2Λ1+Λ2=I\\begin{equation} \\begin{split} \\mathbf{W}^T\\overline{\\mathbf{C}}\\vphantom{C}^1\\mathbf{W} &amp;= \\mathbf{\\Lambda}^1\\\\ \\mathbf{W}^T\\overline{\\mathbf{C}}\\vphantom{C}^2\\mathbf{W} &amp;= \\mathbf{\\Lambda}^2\\\\ \\mathbf{\\Lambda}^1 + \\mathbf{\\Lambda}^2 &amp;= \\mathbf{I}\\\\ \\end{split} \\end{equation} WTCC1WWTCC2WΛ1+Λ2​=Λ1=Λ2=I​​​ 在第二个公式的左右两边同时右乘矩阵W−1(C‾C2)−1\\mathbf{W}^{-1}\\left(\\overline{\\mathbf{C}}\\vphantom{C}^2\\right)^{-1}W−1(CC2)−1，可以得到： WT=Λ2W−1(C‾C2)−1\\begin{equation} \\mathbf{W}^T=\\mathbf{\\Lambda}^2\\mathbf{W}^{-1}\\left(\\overline{\\mathbf{C}}\\vphantom{C}^2\\right)^{-1} \\end{equation} WT=Λ2W−1(CC2)−1​​ 将该式带入WTC‾C1W=Λ1\\mathbf{W}^T\\overline{\\mathbf{C}}\\vphantom{C}^1\\mathbf{W} = \\mathbf{\\Lambda}^1WTCC1W=Λ1，替换掉WT\\mathbf{W}^TWT，可得： Λ2W−1(C‾C2)−1C‾C1W=Λ1\\begin{equation} \\mathbf{\\Lambda}^2\\mathbf{W}^{-1}\\left(\\overline{\\mathbf{C}}\\vphantom{C}^2\\right)^{-1}\\overline{\\mathbf{C}}\\vphantom{C}^1\\mathbf{W} = \\mathbf{\\Lambda}^1 \\end{equation} Λ2W−1(CC2)−1CC1W=Λ1​​ 左右两边左乘C‾C2W(Λ2)−1\\overline{\\mathbf{C}}\\vphantom{C}^2 \\mathbf{W} \\left(\\mathbf{\\Lambda}^2\\right)^{-1}CC2W(Λ2)−1，有： C‾C1W=C‾C2W(Λ2)−1Λ1=C‾C2WΛΛ=(Λ2)−1Λ1\\begin{equation} \\begin{split} \\overline{\\mathbf{C}}\\vphantom{C}^1\\mathbf{W} &amp;= \\overline{\\mathbf{C}}\\vphantom{C}^2 \\mathbf{W} \\left(\\mathbf{\\Lambda}^2\\right)^{-1} \\mathbf{\\Lambda}^1\\\\ &amp;= \\overline{\\mathbf{C}}\\vphantom{C}^2 \\mathbf{W} \\mathbf{\\Lambda}\\\\ \\mathbf{\\Lambda} &amp;= \\left(\\mathbf{\\Lambda}^2\\right)^{-1} \\mathbf{\\Lambda}^1\\\\ \\end{split} \\end{equation} CC1WΛ​=CC2W(Λ2)−1Λ1=CC2WΛ=(Λ2)−1Λ1​​​ 没错，我们又推出了熟悉的广义特征值问题，再考虑广义雷利商与之的联系，可以得到CSP的第三种形式： arg max⁡Wtr(WTC‾C1W)s.t.WTC‾C2W=I\\begin{equation} \\begin{split} \\argmax_{\\mathbf{W}}\\quad &amp;\\mathrm{tr}\\left(\\mathbf{W}^T\\overline{\\mathbf{C}}\\vphantom{C}^1\\mathbf{W}\\right)\\\\ \\textrm{s.t.}\\quad &amp;\\mathbf{W}^T\\overline{\\mathbf{C}}\\vphantom{C}^2\\mathbf{W} = \\mathbf{I}\\\\ \\end{split} \\end{equation} Wargmax​s.t.​tr(WTCC1W)WTCC2W=I​​​ 相比CSP的原始形式和第二种形式，第三种形式更适合从直观上解释CSP在运动想象上有效的原因。运动想象会产生事件相关同步（ERS）和事件相关去同步（ERD）的现象，简单来说就是从电信号上看，某些脑区能量升高，某些脑区能量降低，故能量变化才是运动想象分类的关键特征。而方差可以看作一导信号能量的高低（协方差矩阵则是多导信号的综合反应），因此CSP的第三种形式实质体现的是这样一个问题： 寻找一种变换方式w\\mathbf{w}w，使得变换后任务1的能量（wTC‾C1w\\mathbf{w}^T\\overline{\\mathbf{C}}\\vphantom{C}^1\\mathbf{w}wTCC1w）和任务2的能量（wTC‾C2w\\mathbf{w}^T\\overline{\\mathbf{C}}\\vphantom{C}^2\\mathbf{w}wTCC2w）差异最大化（其比值最大）。 CSP的这种特性恰好和运动想象产生的神经机制变化现象一致，CSP对能量特征做转换，从而强化了不同任务间能量的差异。 关于CSP的第三种形式，最后还需要注意的一点是其同CSP的第二种形式（或原始形式）并不完全等价，我们在推导第三种形式过程种始终没有用到这样一个约束条件Λ1+Λ2=I\\mathbf{\\Lambda}^1 + \\mathbf{\\Lambda}^2 = \\mathbf{I}Λ1+Λ2=I。 这表明，第三种形式是CSP的一种泛化形式，其和CSP原始形式和第二种表述的差异仅在于特征值Λ\\LambdaΛ不要求在0~1的范围内，具体来说，它们的特征值间存在这样一种关系： Λ=(Λ2)−1Λ1Λ1=(Λ+I)−1ΛΛ2=(Λ+I)−1\\begin{equation} \\begin{split} \\mathbf{\\Lambda} &amp;= \\left(\\mathbf{\\Lambda}^2\\right)^{-1} \\mathbf{\\Lambda}^1\\\\ \\mathbf{\\Lambda}^1 &amp;= (\\mathbf{\\Lambda} + I)^{-1}\\mathbf{\\Lambda}\\\\ \\mathbf{\\Lambda}^2 &amp;= (\\mathbf{\\Lambda} + I)^{-1}\\\\ \\end{split} \\end{equation} ΛΛ1Λ2​=(Λ2)−1Λ1=(Λ+I)−1Λ=(Λ+I)−1​​​ 实现分析 CSP作为经典算法有各种实现，这里主要分析MNE的CSP源码，看看有啥可以学习的地方。 空间滤波器的选择 基本上，目前CSP算法中m的选择方案大多是根据经验选择（通常选择2～4）个。MNE的CSP选择了第二种形式的CSP算法，最后求解的特征值范围在0～1之间，因此可以对∣λi1−0.5∣\\left| \\lambda_i^1 - 0.5 \\right|∣∣​λi1​−0.5∣∣​先排序再取前M个成分的特征向量组成空间滤波器组W~\\tilde{\\mathbf{W}}W~（与前后各m个的做法有些许差别，但实践中很难有显著性上的差异，这种做法相对方便一些）。 实际上，针对2分类CSP算法，特征值与类平均协方差矩阵间黎曼距离在各个特征向量分量上的投影长度密切相关，具体的证明细节可以看Alexandre Barachant的这篇会议，对于C‾C1\\overline{\\mathbf{C}}\\vphantom{C}^1CC1和C‾C2\\overline{\\mathbf{C}}\\vphantom{C}^2CC2，有如下关系： δR(C‾C1,C‾C2)=∑i=1Nclog2(λi11−λi1)\\begin{equation} \\mathrm{\\delta}_R\\left(\\overline{\\mathbf{C}}\\vphantom{C}^1,\\overline{\\mathbf{C}}\\vphantom{C}^2\\right) = \\sqrt{\\sum_{i=1}^{N_c} \\mathrm{log}^2\\left(\\frac{\\lambda_i^1}{1-\\lambda_i^1}\\right)} \\end{equation} δR​(CC1,CC2)=i=1∑Nc​​log2(1−λi1​λi1​​)​​​ 也就是说，我们可以选定一个阈值ϵ\\epsilonϵ，将特征值及特征向量按log2(λ1−λ)\\mathrm{log}^2\\left(\\frac{\\lambda}{1-\\lambda}\\right)log2(1−λλ​)从大到小排序，选取最小的M使下式成立： ∑i=1Mlog2(λi11−λi1)δR(C‾C1,C‾C2)≥ϵ\\begin{equation} \\frac{\\sqrt{\\sum_{i=1}^{M} \\mathrm{log}^2\\left(\\frac{\\lambda_i^1}{1-\\lambda_i^1}\\right)}}{\\mathrm{\\delta}_R\\left(\\overline{\\mathbf{C}}\\vphantom{C}^1,\\overline{\\mathbf{C}}\\vphantom{C}^2\\right)} \\ge \\epsilon \\end{equation} δR​(CC1,CC2)∑i=1M​log2(1−λi1​λi1​​)​​≥ϵ​​ 再将前M个特征向量组成空间滤波器组W~\\tilde{\\mathbf{W}}W~。当ϵ=0.9\\epsilon=0.9ϵ=0.9时，我们可以说选定的空间滤波器组可以贡献大约90%左右的2类之间的黎曼距离。 最后一种常用的空间滤波器选择方法是计算空间滤波后的特征同标签之间的互信息，再按互信息从大到小排列选择前M个空间滤波器。互信息常用于CSP的衍生算法FBCSP的特征筛选过程，也可以用于CSP（MNE中的CSP也提供了互信息的排序选项）。至于哪一种选择方法是最优的，目前似乎还没有定论（我感觉）。 协方差矩阵正则化 CSP中的正则化方法主要是对协方差矩阵做正则化处理，从十几年前开始，BCI研究者就在协方差矩阵的正则化处理上做了大量的工作，有些正则化方法与BCI的变异性问题也有着密切的联系，因此这一方面的正则化方法展开来讲就收不住啦。我们这里介绍的正则化方法的目的非常单纯，就是为了解决EEG中可能存在的协方差矩阵非正定的问题。 一般而言，本文的第一个公式C(i)\\mathbf{C}^{(i)}C(i)在大多数情况下都是正定的。所谓矩阵M\\mathbf{M}M是正定（definite-positive）的，是指对任意非0实向量z\\mathbf{z}z，zTMz\\mathbf{z}^T\\mathbf{M}\\mathbf{z}zTMz都是一个正数。不过在实践中，经常会遇到程序报类似b matrix is not definite positive这种的错误，这种情况来源于底层的特征值分解或广义特征值分解函数对于矩阵的正定性有较为严格的要求，但输入的协方差矩阵却不是正定的。 那么为啥协方差矩阵不是正定的呢？大概率可按照以下三种情况逐步排查： EEG采样信号X(i)\\mathbf{X}^{(i)}X(i)中的Nc&gt;NsN_c \\gt N_sNc​&gt;Ns​，即导联多于采样点 虽然导联多于采样点，但X(i)\\mathbf{X}^{(i)}X(i)的秩小于NcN_cNc​，即可能存在导联打串或做过共平均参考变换等导致矩阵不满秩的预处理操作 前两条都不满足，则可能是由于数值计算精度上出了问题，比如单个样本的协方差矩阵满足正定性，但平均协方差矩阵却不是正定的（我也不太懂数值计算方面的内容，这一条有待考证，但确实碰到过这样的现象） 总之，为了让计算进行下去，对协方差矩阵做正则化处理是很有必要的，协方差矩阵的正则化就是对协方差矩阵做以下变换： (1−λ)∗C+λ∗μ∗I\\begin{equation} (1-\\lambda) * \\mathbf{C} + \\lambda * \\mu * \\mathbf{I} \\end{equation} (1−λ)∗C+λ∗μ∗I​​ 其中λ\\lambdaλ是待估计的正则化系数，μ=tr(C)Nc\\mu=\\frac{\\mathrm{tr}(\\mathbf{C})}{N_c}μ=Nc​tr(C)​是为了对单位矩阵的数值范围做限定。sklearn的covariance模块中列出了多种正则化处理方法，比如著名的ledoit-wolf正则化、oas正则化等方法，选个顺眼的用就行。","categories":[{"name":"脑-机接口","slug":"脑-机接口","permalink":"https://mrswolf.github.io/categories/%E8%84%91-%E6%9C%BA%E6%8E%A5%E5%8F%A3/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://mrswolf.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"矩阵分解","slug":"矩阵分解","permalink":"https://mrswolf.github.io/tags/%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/"},{"name":"脑-机接口","slug":"脑-机接口","permalink":"https://mrswolf.github.io/tags/%E8%84%91-%E6%9C%BA%E6%8E%A5%E5%8F%A3/"}]}],"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://mrswolf.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"linux","slug":"linux","permalink":"https://mrswolf.github.io/categories/linux/"},{"name":"分布式计算","slug":"分布式计算","permalink":"https://mrswolf.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97/"},{"name":"脑-机接口","slug":"脑-机接口","permalink":"https://mrswolf.github.io/categories/%E8%84%91-%E6%9C%BA%E6%8E%A5%E5%8F%A3/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://mrswolf.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"矩阵分解","slug":"矩阵分解","permalink":"https://mrswolf.github.io/tags/%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/"},{"name":"深度学习","slug":"深度学习","permalink":"https://mrswolf.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"manjaro","slug":"manjaro","permalink":"https://mrswolf.github.io/tags/manjaro/"},{"name":"linux","slug":"linux","permalink":"https://mrswolf.github.io/tags/linux/"},{"name":"matlab","slug":"matlab","permalink":"https://mrswolf.github.io/tags/matlab/"},{"name":"脑-机接口","slug":"脑-机接口","permalink":"https://mrswolf.github.io/tags/%E8%84%91-%E6%9C%BA%E6%8E%A5%E5%8F%A3/"}]}