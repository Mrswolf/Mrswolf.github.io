{"meta":{"title":"swolf的博客","subtitle":"记录我的学习生活","description":"欢迎光临，这里是swolf的博客~","author":"swolf","url":"https://mrswolf.github.io","root":"/"},"pages":[{"title":"About","date":"2023-09-12T15:27:05.536Z","updated":"2022-09-15T10:43:50.123Z","comments":false,"path":"about/index.html","permalink":"https://mrswolf.github.io/about/index.html","excerpt":"","text":"I received the Ph.D. degree in biomedical engineering in 2022 from the Tianjin University. I currently works as an engineer in a company and also an independent researcher. My research interests include brain-computer interfaces, human-computer interaction, medical image processing, machine learning and blockchain. 我是一名生物医学工程博士，博士课题研究方向为脑-机接口。目前，为了生存在恰饭，空闲时间就做一些自己感兴趣的事。主要研究兴趣包括计算机科学、脑-机接口、Web3等等。博客主要记录我在学习过程中的心得体会，欢迎感兴趣的朋友与我讨论～。 文章转载只需注明出处即可～Enjoy! Contact Email: swolfforever@gmail.com"},{"title":"Categories","date":"2023-09-12T15:27:05.540Z","updated":"2022-06-04T06:41:45.199Z","comments":false,"path":"categories/index.html","permalink":"https://mrswolf.github.io/categories/index.html","excerpt":"","text":""},{"title":"Links","date":"2023-09-12T15:27:05.540Z","updated":"2022-03-20T06:41:20.079Z","comments":false,"path":"links/index.html","permalink":"https://mrswolf.github.io/links/index.html","excerpt":"","text":""},{"title":"Publications","date":"2023-09-12T15:27:05.550Z","updated":"2022-06-09T04:11:40.491Z","comments":false,"path":"publications/index.html","permalink":"https://mrswolf.github.io/publications/index.html","excerpt":"","text":"Enhancing transfer performance across datasets for brain-computer interfaces using a combination of alignment strategies and adaptive batch normalization L Xu, M Xu, Z Ma, K Wang, TP Jung, D Ming.(2021) [link] Improving Transfer Performance of Deep Learning with Adaptive Batch Normalization for Brain-computer Interfaces L Xu, Z Ma, J Meng, M Xu, TP Jung, D Ming.(2021) [link] Review of brain encoding and decoding mechanisms for EEG-based brain–computer interface L Xu, M Xu, TP Jung, D Ming.(2021) [link] Cross-dataset variability problem in EEG decoding with deep learning L Xu, M Xu, Y Ke, X An, S Liu, D Ming.(2020) [link] An source-based common spatial filter selection for improving mis-triggering problem in brain-computer interface based on motor imagery L Xu, J Xu, K Wang, Z Wang, M Xu, F He, D Ming, H Qi.(2017) [link]"},{"title":"Repositories","date":"2023-09-12T15:27:05.553Z","updated":"2022-03-20T06:39:10.873Z","comments":false,"path":"repository/index.html","permalink":"https://mrswolf.github.io/repository/index.html","excerpt":"","text":""},{"title":"Tags","date":"2023-09-12T15:27:05.553Z","updated":"2022-03-21T07:21:55.131Z","comments":false,"path":"tags/index.html","permalink":"https://mrswolf.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Robust Principle Component Analysis","slug":"rpca","date":"2023-09-12T15:30:49.000Z","updated":"2023-09-12T16:48:06.610Z","comments":false,"path":"rpca/","link":"rpca","permalink":"https://mrswolf.github.io/rpca/","excerpt":"","text":"How does RPCA work? Background of PCA Principle component analysis (PCA) is the most widely used dimension reduction technique. Given a matrix X∈Rm×n\\mathbf{X} \\in \\mathbb{R}^{m \\times n}X∈Rm×n in which each column of X\\mathbf{X}X represents a measurement and the rank of X\\mathbf{X}X is rrr, PCA solves the optimization problem as follows: arg min⁡L∈Rm×n ∥X−L∥F2s.t. rank(L)=l,l≪r \\begin{equation} \\begin{split} \\argmin_{\\mathbf{L} \\in \\mathbb{R}^{m \\times n}} &amp;\\ \\|\\mathbf{X} - \\mathbf{L}\\|_F^2\\\\ \\text{s.t.} &amp;\\ \\ rank(\\mathbf{L}) = l, l \\ll r \\end{split} \\end{equation} L∈Rm×nargmin​s.t.​ ∥X−L∥F2​ rank(L)=l,l≪r​​​ which is also known as low-rank approximation. Mathematically, it can be expressed as X=L+N\\mathbf{X}=\\mathbf{L}+\\mathbf{N}X=L+N, where each element of N\\mathbf{N}N follows an iid normal distribution. The Eckart-Young-Mirsky theorem states that the best rank-lll approximation in terms of the Frobenius norm is L=∑i=1lσiuiviT\\mathbf{L} = \\sum_{i=1}^{l} \\sigma_i u_i v_i^TL=∑i=1l​σi​ui​viT​, where X=∑i=1rσiuiviT\\mathbf{X} = \\sum_{i=1}^{r} \\sigma_i u_i v_i^TX=∑i=1r​σi​ui​viT​ represents the compact SVD of X\\mathbf{X}X. The proof of this theorem can be found in this note. I also have a note talking about PCA in decomposition formula. Introduction to RPCA One drawback of PCA is that it is highly sensitive to corrupted data. The “corrupted” here means significant changes occur on measurements, like specularities on human face images or missing records in database. That’s why robust principle component analysis (RPCA) comes. RPCA aims to recover a low-rank matrix Y\\mathbf{Y}Y from highly corrupted measurements X=L+S\\mathbf{X}=\\mathbf{L}+\\mathbf{S}X=L+S, where the elements in S\\mathbf{S}S can have arbitrarily large magnitude and they are supposed to be sparse. The optimization problem is: arg min⁡L,S∈Rm×n ∥L∥∗+λ∥vec(S)∥1s.t. L+S=X \\begin{equation} \\begin{split} \\argmin_{\\mathbf{L}, \\mathbf{S} \\in \\mathbb{R}^{m \\times n}} &amp;\\ \\|\\mathbf{L}\\|_* + \\lambda \\|vec(\\mathbf{S})\\|_1\\\\ \\text{s.t.} &amp;\\ \\mathbf{L} + \\mathbf{S} = \\mathbf{X} \\end{split} \\end{equation} L,S∈Rm×nargmin​s.t.​ ∥L∥∗​+λ∥vec(S)∥1​ L+S=X​​​ where vec(⋅)vec(\\cdot)vec(⋅) represents vectorize operator and ∥⋅∥∗\\|\\cdot\\|_*∥⋅∥∗​ and ∥⋅∥1\\|\\cdot\\|_1∥⋅∥1​ represent matrix nuclear norm and vector l1l_1l1​-norm, respectively. At first sight, the problem seems impossible to solve due to insufficient information. In fact, we need to assume that the low-rank component L\\mathbf{L}L is not sparse and the sparsity pattern of S\\mathbf{S}S is selected uniformly at random. More formally speaking, the incoherence condition should be made, see candes’ paper for more information. Two instances that RPCA is not capable of disentangling two matrices. Suppose that the matrix X\\mathbf{X}X has only a one at the top left corner and zeros everywhere else. How can we decide whether X\\mathbf{X}X is the low-rank or sparse? Suppose that the first column of S\\mathbf{S}S is the opposite of that of L\\mathbf{L}L and zeros everywhere else. We would not able to recover L\\mathbf{L}L and S\\mathbf{S}S since the column space of X\\mathbf{X}X falls into that of L\\mathbf{L}L. The main result of candes’ paper says that the RPCA problem with above assumptions, has high probability to obtain the solution, given λ=1/max(m,n)\\lambda = 1/\\sqrt{max(m, n)}λ=1/max(m,n)​. The choice of λ\\lambdaλ is a pure mathematical analysis and it works correctly. However, we may improve the performance of RPCA by choosing λ\\lambdaλ in accordance with prior knowledge about the solution. Solutions Proximal Gradient Descent This kind of technique is also called the Principal Component Pursuit (PCP). Before introduce proximal gradient descent (PGD), lets’ recall that how gradient descent works. Considering f(x)∈R,x∈Rmf(\\mathbf{x}) \\in \\mathbb{R}, \\mathbf{x} \\in \\mathbb{R}^{m}f(x)∈R,x∈Rm, f(x)f(\\mathbf{x})f(x) is convex and differentiable, we want to find the solution of the problem: arg min⁡x∈Rm f(x) \\begin{equation} \\begin{split} \\argmin_{\\mathbf{x} \\in \\mathbb{R}^m} &amp;\\ f(\\mathbf{x}) \\end{split} \\end{equation} x∈Rmargmin​​ f(x)​​​ Firstly,we expand the 2-order Taylor series of the function f(x)f(\\mathbf{x})f(x) and substitute the Hessian matrix f′′(x)f&#x27;&#x27;(\\mathbf{x})f′′(x) with an identity matrix 1tI\\frac{1}{t} \\mathbf{I}t1​I: f(z)≈f(x)+f′(x)(z−x)+12(z−x)Tf′′(x)(z−x)≈f(x)+f′(x)(z−x)+12(z−x)T1tI(z−x) \\begin{equation} \\begin{split} f(\\mathbf{z}) &amp;\\approx f(\\mathbf{x}) + f&#x27;(\\mathbf{x})(\\mathbf{z}-\\mathbf{x}) + \\frac{1}{2}(\\mathbf{z}-\\mathbf{x})^T f&#x27;&#x27;(\\mathbf{x}) (\\mathbf{z}-\\mathbf{x})\\\\ &amp;\\approx f(\\mathbf{x}) + f&#x27;(\\mathbf{x})(\\mathbf{z}-\\mathbf{x}) + \\frac{1}{2}(\\mathbf{z}-\\mathbf{x})^T \\frac{1}{t}\\mathbf{I} (\\mathbf{z}-\\mathbf{x}) \\end{split} \\end{equation} f(z)​≈f(x)+f′(x)(z−x)+21​(z−x)Tf′′(x)(z−x)≈f(x)+f′(x)(z−x)+21​(z−x)Tt1​I(z−x)​​​ and this is a quadratic approximation of f(x)f(\\mathbf{x})f(x) at point x\\mathbf{x}x (here we use the numerator layout). The original problem can be reduced to the following problem: arg min⁡z∈Rm f(x)+f′(x)(z−x)+12(z−x)T1tI(z−x) \\begin{equation} \\begin{split} \\argmin_{\\mathbf{z} \\in \\mathbb{R}^m} &amp;\\ f(\\mathbf{x}) + f&#x27;(\\mathbf{x})(\\mathbf{z}-\\mathbf{x}) + \\frac{1}{2}(\\mathbf{z}-\\mathbf{x})^T \\frac{1}{t}\\mathbf{I} (\\mathbf{z}-\\mathbf{x}) \\end{split} \\end{equation} z∈Rmargmin​​ f(x)+f′(x)(z−x)+21​(z−x)Tt1​I(z−x)​​​ which further reduces to the form: arg min⁡z∈Rm 12t∥z−(x−tf′(x)T)∥22 \\begin{equation} \\begin{split} \\argmin_{\\mathbf{z} \\in \\mathbb{R}^m} \\ \\frac{1}{2t}\\|\\mathbf{z}-(\\mathbf{x}-tf&#x27;(\\mathbf{x})^T)\\|_2^2 \\end{split} \\end{equation} z∈Rmargmin​ 2t1​∥z−(x−tf′(x)T)∥22​​​​ So the gradient descent update is: x+=x−tf′(x)T \\begin{equation} \\begin{split} \\mathbf{x}^+ = \\mathbf{x}-tf&#x27;(\\mathbf{x})^T \\end{split} \\end{equation} x+=x−tf′(x)T​​​ But what if f(x)f(\\mathbf{x})f(x) is not differentiable? Lets’ break f(x)f(\\mathbf{x})f(x) into two parts g(x)g(\\mathbf{x})g(x) and h(x)h(\\mathbf{x})h(x), f(x)=g(x)+h(x)f(\\mathbf{x}) = g(\\mathbf{x}) + h(\\mathbf{x})f(x)=g(x)+h(x), such that g(x)g(\\mathbf{x})g(x) is convex and differentiable and h((x))h(\\mathbf(x))h((x)) is convex but not differentiable. We can still approximate f(x)f(\\mathbf{x})f(x) at point x\\mathbf{x}x with Tayler series of g(x)g(\\mathbf{x})g(x): f(z)=g(z)+h(z)≈g(x)+g′(x)(z−x)+12(z−x)Tg′′(x)(z−x)+h(z)≈g(x)+g′(x)(z−x)+12(z−x)T1tI(z−x)+h(z) \\begin{equation} \\begin{split} f(\\mathbf{z}) &amp;= g(\\mathbf{z}) + h(\\mathbf{z})\\\\ &amp;\\approx g(\\mathbf{x}) + g&#x27;(\\mathbf{x})(\\mathbf{z}-\\mathbf{x}) + \\frac{1}{2}(\\mathbf{z}-\\mathbf{x})^T g&#x27;&#x27;(\\mathbf{x}) (\\mathbf{z}-\\mathbf{x}) + h(\\mathbf{z})\\\\ &amp;\\approx g(\\mathbf{x}) + g&#x27;(\\mathbf{x})(\\mathbf{z}-\\mathbf{x}) + \\frac{1}{2}(\\mathbf{z}-\\mathbf{x})^T \\frac{1}{t}\\mathbf{I} (\\mathbf{z}-\\mathbf{x}) + h(\\mathbf{z}) \\end{split} \\end{equation} f(z)​=g(z)+h(z)≈g(x)+g′(x)(z−x)+21​(z−x)Tg′′(x)(z−x)+h(z)≈g(x)+g′(x)(z−x)+21​(z−x)Tt1​I(z−x)+h(z)​​​ The optimization problem can be reduced to: arg min⁡z∈Rm 12t∥z−(x−tg′(x)T)∥22+h(z) \\begin{equation} \\begin{split} \\argmin_{\\mathbf{z} \\in \\mathbb{R}^m} \\ \\frac{1}{2t}\\|\\mathbf{z}-(\\mathbf{x}-tg&#x27;(\\mathbf{x})^T)\\|_2^2 + h(\\mathbf{z}) \\end{split} \\end{equation} z∈Rmargmin​ 2t1​∥z−(x−tg′(x)T)∥22​+h(z)​​​ Here we define proximal mapping proxt(x)prox_t(\\mathbf{x})proxt​(x) of h(x)h(\\mathbf{x})h(x) as: proxt(x)=arg min⁡z∈Rm 12t∥z−x∥22+h(z) \\begin{equation} \\begin{split} prox_t(\\mathbf{x}) = \\argmin_{\\mathbf{z} \\in \\mathbb{R}^m} \\ \\frac{1}{2t}\\|\\mathbf{z}-\\mathbf{x}\\|_2^2 + h(\\mathbf{z}) \\end{split} \\end{equation} proxt​(x)=z∈Rmargmin​ 2t1​∥z−x∥22​+h(z)​​​ If we know the proximal mapping of h(x)h(\\mathbf{x})h(x), we can easily get the solution of the above optimization problem also known as the PGD update: x+=proxt(x−tg′(x)T)=x−tx−proxt(x−tg′(x)T)t=x−tGt(x) \\begin{equation} \\begin{split} \\mathbf{x}^+ &amp;= prox_t(\\mathbf{x}-tg&#x27;(\\mathbf{x})^T)\\\\ &amp;= \\mathbf{x} - t\\frac{\\mathbf{x}-prox_t(\\mathbf{x}-tg&#x27;(\\mathbf{x})^T)}{t}\\\\ &amp;= \\mathbf{x} - t G_t(\\mathbf{x}) \\end{split} \\end{equation} x+​=proxt​(x−tg′(x)T)=x−ttx−proxt​(x−tg′(x)T)​=x−tGt​(x)​​​ where Gt(x)G_t(\\mathbf{x})Gt​(x) is the generalized gradient of f(x)f(\\mathbf{x})f(x). Here we list a few common proximal mappings. h(x)h(\\mathbf{x})h(x) proxt(x)prox_t(\\mathbf{x})proxt​(x) note ∣x∣1|\\mathbf{x}|_1∣x∣1​ St(x)=sign(x)max(∣x∣−t,0)S_t(\\mathbf{x}) = sign(\\mathbf{x}) max(\\lvert\\mathbf{x}\\rvert-t, 0)St​(x)=sign(x)max(∣x∣−t,0) l1l_1l1​-norm of vectors, soft-thresholding operator ∣X∣∗|\\mathbf{X}|_*∣X∣∗​ SVTt(X)=Udiag(St(diag(Σ)))VSVT_t(\\mathbf{X})=\\mathbf{U} diag(S_t(diag(\\mathbf{\\Sigma}))) \\mathbf{V}SVTt​(X)=Udiag(St​(diag(Σ)))V nuclear norm of matrices, singular value thresholding operator Back to RPCA, we transform the original problem into an unconstrained problem using a sightly relaxed version of the original problem: arg min⁡L,S∈Rm×n μ∥L∥∗+μλ∥vec(S)∥1+12∥X−L−S∥F2 \\begin{equation} \\begin{split} \\argmin_{\\mathbf{L}, \\mathbf{S} \\in \\mathbb{R}^{m \\times n}} &amp;\\ \\mu\\|\\mathbf{L}\\|_* + \\mu\\lambda \\|vec(\\mathbf{S})\\|_1 + \\frac{1}{2} \\|\\mathbf{X}-\\mathbf{L}-\\mathbf{S}\\|_F^2 \\end{split} \\end{equation} L,S∈Rm×nargmin​​ μ∥L∥∗​+μλ∥vec(S)∥1​+21​∥X−L−S∥F2​​​​ where as μ\\muμ approaches 0, any solution to the above problem approaches the solution set of the original problem. Here we let: h([LS])=μ∥L∥∗+μλ∥vec(S)∥1=μ∥[I,0][LS]∥∗+μλ∥vec([0,I][LS])∥1g([LS])=12∥X−L−S∥F2=12∥X−[I,I][LS]∥F2arg min⁡L,S∈Rm×n h([LS])+g([LS]) \\begin{equation} \\begin{split} h(\\begin{bmatrix} \\mathbf{L} \\\\ \\mathbf{S} \\end{bmatrix}) &amp;= \\mu\\|\\mathbf{L}\\|_* + \\mu\\lambda \\|vec(\\mathbf{S})\\|_1\\\\ &amp;= \\mu\\|\\begin{bmatrix} \\mathbf{I} , \\mathbf{0} \\end{bmatrix} \\begin{bmatrix} \\mathbf{L} \\\\ \\mathbf{S} \\end{bmatrix}\\|_* + \\mu\\lambda \\|vec\\left(\\begin{bmatrix} \\mathbf{0} , \\mathbf{I} \\end{bmatrix} \\begin{bmatrix} \\mathbf{L} \\\\ \\mathbf{S} \\end{bmatrix}\\right)\\|_1\\\\ g(\\begin{bmatrix} \\mathbf{L} \\\\ \\mathbf{S} \\end{bmatrix}) &amp;= \\frac{1}{2} \\|\\mathbf{X}-\\mathbf{L}-\\mathbf{S}\\|_F^2\\\\ &amp;= \\frac{1}{2} \\|\\mathbf{X}-\\begin{bmatrix} \\mathbf{I} , \\mathbf{I} \\end{bmatrix} \\begin{bmatrix} \\mathbf{L} \\\\ \\mathbf{S} \\end{bmatrix}\\|_F^2\\\\ \\argmin_{\\mathbf{L}, \\mathbf{S} \\in \\mathbb{R}^{m \\times n}} &amp;\\ h(\\begin{bmatrix} \\mathbf{L} \\\\ \\mathbf{S} \\end{bmatrix}) + g(\\begin{bmatrix} \\mathbf{L} \\\\ \\mathbf{S} \\end{bmatrix})\\\\ \\end{split} \\end{equation} h([LS​])g([LS​])L,S∈Rm×nargmin​​=μ∥L∥∗​+μλ∥vec(S)∥1​=μ∥[I,0​][LS​]∥∗​+μλ∥vec([0,I​][LS​])∥1​=21​∥X−L−S∥F2​=21​∥X−[I,I​][LS​]∥F2​ h([LS​])+g([LS​])​​​ The gradient of g([LS])g(\\begin{bmatrix} \\mathbf{L} \\\\ \\mathbf{S} \\end{bmatrix})g([LS​]) in numerator layout is: g′([LS])=−[X−L−SX−L−S]T \\begin{equation} \\begin{split} g&#x27;(\\begin{bmatrix} \\mathbf{L} \\\\ \\mathbf{S} \\end{bmatrix}) &amp;= -\\begin{bmatrix} \\mathbf{X} - \\mathbf{L} - \\mathbf{S} \\\\ \\mathbf{X} - \\mathbf{L} - \\mathbf{S} \\end{bmatrix}^T \\end{split} \\end{equation} g′([LS​])​=−[X−L−SX−L−S​]T​​​ The proximal mapping of h([LS])h(\\begin{bmatrix} \\mathbf{L} \\\\ \\mathbf{S} \\end{bmatrix})h([LS​]) can be decomposed into two separate proximal mappings: proxt([LS])=arg min⁡ZL,ZS∈Rm×n 12t∥[ZLZS]−[LS]∥F2+h([ZLZS])=arg min⁡ZL∈Rm×n 12t∥ZL−L∥F2+μ∥L∥∗+arg min⁡ZS∈Rm×n 12t∥ZS−S∥F2+μλ∥vec(S)∥1 \\begin{equation} \\begin{split} prox_t(\\begin{bmatrix} \\mathbf{L} \\\\ \\mathbf{S} \\end{bmatrix}) &amp;= \\argmin_{\\mathbf{Z_L}, \\mathbf{Z_S} \\in \\mathbb{R}^{m \\times n}} \\ \\frac{1}{2t}\\|\\begin{bmatrix} \\mathbf{Z_L} \\\\ \\mathbf{Z_S} \\end{bmatrix}-\\begin{bmatrix} \\mathbf{L} \\\\ \\mathbf{S} \\end{bmatrix}\\|_F^2 + h(\\begin{bmatrix} \\mathbf{Z_L} \\\\ \\mathbf{Z_S}\\end{bmatrix})\\\\ &amp;= \\argmin_{\\mathbf{Z_L} \\in \\mathbb{R}^{m \\times n}} \\ \\frac{1}{2t}\\|\\mathbf{Z_L} -\\mathbf{L}\\|_F^2 + \\mu\\|\\mathbf{L}\\|_* + \\argmin_{\\mathbf{Z_S} \\in \\mathbb{R}^{m \\times n}} \\ \\frac{1}{2t}\\|\\mathbf{Z_S} -\\mathbf{S}\\|_F^2 + \\mu\\lambda\\|vec(\\mathbf{S})\\|_1 \\\\ \\end{split} \\end{equation} proxt​([LS​])​=ZL​,ZS​∈Rm×nargmin​ 2t1​∥[ZL​ZS​​]−[LS​]∥F2​+h([ZL​ZS​​])=ZL​∈Rm×nargmin​ 2t1​∥ZL​−L∥F2​+μ∥L∥∗​+ZS​∈Rm×nargmin​ 2t1​∥ZS​−S∥F2​+μλ∥vec(S)∥1​​​​ so the solution would be: proxt([LS])=[SVTμt(L)uvec(Sμλt(vec(S)))] \\begin{equation} \\begin{split} prox_t(\\begin{bmatrix} \\mathbf{L} \\\\ \\mathbf{S} \\end{bmatrix}) &amp;= \\begin{bmatrix} SVT_{\\mu t}\\left(\\mathbf{L}\\right) \\\\ uvec\\left(S_{\\mu \\lambda t}\\left(vec(\\mathbf{S})\\right)\\right) \\end{bmatrix} \\end{split} \\end{equation} proxt​([LS​])​=[SVTμt​(L)uvec(Sμλt​(vec(S)))​]​​​ where uvec(⋅)uvec(\\cdot)uvec(⋅) represents unvectorize operator. Finally, we get our proximal gradient descent update rules of the RPCA problem: [L(k)S(k)]=proxt([L(k−1)+t(X−L(k−1)−S(k−1))S(k−1)+t(X−L(k−1)−S(k−1))])=[SVTμt(L(k−1)+t(X−L(k−1)−S(k−1)))uvec(Sμλt(vec(S(k−1)+t(X−L(k−1)−S(k−1)))))] \\begin{equation} \\begin{split} \\begin{bmatrix} \\mathbf{L}^{(k)} \\\\ \\mathbf{S}^{(k)} \\end{bmatrix} &amp;= prox_t\\left(\\begin{bmatrix} \\mathbf{L}^{(k-1)}+t(\\mathbf{X}-\\mathbf{L}^{(k-1)} - \\mathbf{S}^{(k-1)}) \\\\ \\mathbf{S}^{(k-1)}+t(\\mathbf{X}-\\mathbf{L}^{(k-1)} - \\mathbf{S}^{(k-1)}) \\end{bmatrix} \\right)\\\\ &amp;= \\begin{bmatrix} SVT_{\\mu t}\\left(\\mathbf{L}^{(k-1)}+t(\\mathbf{X}-\\mathbf{L}^{(k-1)} - \\mathbf{S}^{(k-1)})\\right) \\\\ uvec\\left(S_{\\mu \\lambda t}\\left(vec(\\mathbf{S}^{(k-1)}+t(\\mathbf{X}-\\mathbf{L}^{(k-1)} - \\mathbf{S}^{(k-1)}))\\right)\\right) \\end{bmatrix} \\end{split} \\end{equation} [L(k)S(k)​]​=proxt​([L(k−1)+t(X−L(k−1)−S(k−1))S(k−1)+t(X−L(k−1)−S(k−1))​])=[SVTμt​(L(k−1)+t(X−L(k−1)−S(k−1)))uvec(Sμλt​(vec(S(k−1)+t(X−L(k−1)−S(k−1)))))​]​​​ Practicalities of PGD Convergence For f(x)=g(x)+h(x)f(\\mathbf{x}) = g(\\mathbf{x}) + h(\\mathbf{x})f(x)=g(x)+h(x), we assume: ggg is convex , differentiable, and g′g&#x27;g′ is Lipschitz continuous with constant L&gt;0L &gt; 0L&gt;0 hhh is convex and the proximal mapping of hhh can be evaluated then proximal gradient descent with fixed step size t≤1Lt \\le \\frac{1}{L}t≤L1​ satisfies: f(x(k))−f∗≤∥x(0)−x∗∥222tk \\begin{equation} \\begin{split} f(\\mathbf{x}^{(k)}) - f^{\\ast} \\le \\frac{\\|\\mathbf{x}^{(0)} - \\mathbf{x}^{\\ast}\\|_2^2}{2tk} \\end{split} \\end{equation} f(x(k))−f∗≤2tk∥x(0)−x∗∥22​​​​​ The above theorem suggests that PGD has convergence rate O(1/ϵ)O(1/\\epsilon)O(1/ϵ). By the way, the Lipschitz constant of 12∥X−L−S∥F2\\frac{1}{2} \\|\\mathbf{X}-\\mathbf{L}-\\mathbf{S}\\|_F^221​∥X−L−S∥F2​ is 2? Stopping Rules Tricks Alternating Direction Method of Multipliers Practicalities of ADMM Convergence Stopping Rules Tricks Applications MRI Reconstruction Appendix Proof of SVT For each τ≥0\\tau \\ge 0τ≥0 and Y∈Rm×n\\mathbf{Y} \\in \\mathbf{R}^{m \\times n}Y∈Rm×n, the singular value shrinkage operator obeys Dτ(Y)=arg min⁡X12∥X−Y∥F2+τ∥X∥∗ \\begin{equation} \\begin{split} \\mathcal{D}_\\tau(\\mathbf{Y}) = \\argmin_{\\mathbf{X}} \\frac{1}{2}\\| \\mathbf{X} - \\mathbf{Y} \\|_F^2 + \\tau \\| \\mathbf{X} \\|_* \\end{split} \\end{equation} Dτ​(Y)=Xargmin​21​∥X−Y∥F2​+τ∥X∥∗​​​​ Proof. Since the function h(X):=12∥X−Y∥F2+τ∥X∥∗h(\\mathbf{X}) := \\frac{1}{2}\\| \\mathbf{X} - \\mathbf{Y} \\|_F^2 + \\tau \\| \\mathbf{X} \\|_*h(X):=21​∥X−Y∥F2​+τ∥X∥∗​ is strictly convex, it is easy to see that there exists a unique minimizer, and we thus need to prove that it is equal to Dτ(Y)\\mathcal{D}_\\tau(\\mathbf{Y})Dτ​(Y). To do this, recall the definition of a subgradient of a convex function f:Rm×n→Rf: \\mathbb{R}^{m \\times n} \\rightarrow \\mathbb{R}f:Rm×n→R. We say that Z\\mathbf{Z}Z is a subgradient of fff at X0\\mathbf{X}_0X0​, denoted Z∈∂f(X0)Z \\in \\partial f(\\mathbf{X}_0)Z∈∂f(X0​), if f(X)≥f(X0)+⟨Z,X−X0⟩f(\\mathbf{X}) \\ge f(\\mathbf{X}_0) + \\langle \\mathbf{Z}, \\mathbf{X} - \\mathbf{X}_0 \\ranglef(X)≥f(X0​)+⟨Z,X−X0​⟩ for all X\\mathbf{X}X. Now X^\\hat{\\mathbf{X}}X^ minimizes hhh if and only if 0 is a subgradient of the functional hhh at the point X^\\hat{\\mathbf{X}}X^, i.e. 0∈X^−Y+τ∂∥X^∥∗0 \\in \\hat{\\mathbf{X}} − \\mathbf{Y} + \\tau \\partial \\|\\hat{\\mathbf{X}}\\|_*0∈X^−Y+τ∂∥X^∥∗​, where ∂∥X^∥∗\\partial \\|\\hat{\\mathbf{X}}\\|_*∂∥X^∥∗​ is the set of subgradients of the nuclear norm. Let X∈Rm×n\\mathbf{X} \\in R^{m \\times n}X∈Rm×n be an arbitrary matrix and UΣV∗\\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^*UΣV∗ be its SVD. It is known that ∂∥X∥∗={UV∗+W:W∈Rm×n,U∗W=0WV=0,∥W∥2≤1}\\partial \\|\\mathbf{X}\\|_* = \\{ \\mathbf{U} \\mathbf{V}^* + \\mathbf{W}: \\mathbf{W} \\in \\mathbb{R}^{m \\times n}, \\mathbf{U}*\\mathbf{W}=0 \\mathbf{W}\\mathbf{V}=0, \\|\\mathbf{W}\\|_2 \\le 1\\}∂∥X∥∗​={UV∗+W:W∈Rm×n,U∗W=0WV=0,∥W∥2​≤1}. Set X^=Dτ(Y)\\hat{\\mathbf{X}} = \\mathcal{D}_\\tau(\\mathbf{Y})X^=Dτ​(Y) for short. In order to show that X^\\hat{\\mathbf{X}}X^ obeys the optimal condition, decompose the SVD of Y\\mathbf{Y}Y as Y=U0Σ0V0∗+U1Σ1V1∗\\mathbf{Y} = \\mathbf{U}_0 \\mathbf{\\Sigma}_0 \\mathbf{V}_0^* + \\mathbf{U}_1 \\mathbf{\\Sigma}_1 \\mathbf{V}_1^*Y=U0​Σ0​V0∗​+U1​Σ1​V1∗​ , where U0,V0\\mathbf{U}_0,\\mathbf{V}_0U0​,V0​ (resp. U1,V1\\mathbf{U}_1,\\mathbf{V}_1U1​,V1​) are the singular vectors associated with singular values greater than τ\\tauτ (resp. smaller than or equal to τ\\tauτ). With these notations, we have X^=U0(Σ0−τI)V0\\hat{\\mathbf{X}} = \\mathbf{U}_0 \\left(\\mathbf{\\Sigma}_0 - \\tau \\mathbf{I} \\right)\\mathbf{V}_0X^=U0​(Σ0​−τI)V0​ and, therefore, Y−X^=τ(U0V0∗+W),W=τ−1U1Σ1V1∗\\mathbf{Y} - \\hat{\\mathbf{X}} = \\tau \\left( \\mathbf{U}_0 \\mathbf{V}_0^* + \\mathbf{W}\\right), \\mathbf{W} = \\tau^{-1} \\mathbf{U}_1 \\mathbf{\\Sigma}_1 \\mathbf{V}_1^*Y−X^=τ(U0​V0∗​+W),W=τ−1U1​Σ1​V1∗​. By definition, U0∗W=0WV0=0\\mathbf{U}_0*\\mathbf{W}=0 \\mathbf{W}\\mathbf{V}_0=0U0​∗W=0WV0​=0 and since the diagonal elements of Σ1\\mathbf{\\Sigma}_1Σ1​ have magnitudes bounded by τ\\tauτ , we also have ∥W∥2≤1\\|\\mathbf{W}\\|_2 \\le 1∥W∥2​≤1. Hence Y−X^∈∂∥X^∥∗\\mathbf{Y} - \\hat{\\mathbf{X}} \\in \\partial \\|\\hat{\\mathbf{X}}\\|_*Y−X^∈∂∥X^∥∗​, which concludes the proof. References","categories":[{"name":"machine learning","slug":"machine-learning","permalink":"https://mrswolf.github.io/categories/machine-learning/"}],"tags":[{"name":"machine learning","slug":"machine-learning","permalink":"https://mrswolf.github.io/tags/machine-learning/"},{"name":"matrix decomposition","slug":"matrix-decomposition","permalink":"https://mrswolf.github.io/tags/matrix-decomposition/"}]},{"title":"Scientific Computing in VS Code","slug":"vscode-python","date":"2023-06-11T12:23:45.000Z","updated":"2023-06-11T12:40:14.012Z","comments":false,"path":"vscode-python/","link":"vscode-python","permalink":"https://mrswolf.github.io/vscode-python/","excerpt":"I used to do all scientific computing work on Jupyter notebooks. My most common way of debugging was print, which is definitely not the best way to do so.","text":"I used to do all scientific computing work on Jupyter notebooks. My most common way of debugging was print, which is definitely not the best way to do so. Things got complex when I sometimes had to debug other packages installed in the Python environment. The debug mode provided in Jupyter is not very convenient. But I do like the freedom of running cells in Jupyter, which is important for testing new algorithms, so I don’t want to move my workflow to other IDEs like Pycharm. My final choice is VS Code with its gorgeous Python extensions. Extensions Here are my installed extensions: Python Jupyter Python Indent VSCode Great Icons Remote-SSH GitLens Settings not found definition For some reasons, the default Jedi language server failed to parse non-standard packages. Open Settings by Ctrl+, and search python.languageServer, change the language server to Pylance. You may need restart the VS Code. debug library code The default Jupyter debug only steps through user-written code. To debug library code, open Settings by Ctrl+, and search justmycode, uncheck the option and restart the VS Code. Running Jupyter Server I’m using Jupyter Lab instead of Jupyter Notebook. Open the console and change to your Python virtual environment. Firstly, reset the password you used to connect the Jupyter server: jupyter server password Then start a Jupyter server jupyter lab --no-browser. This command would start a Jupyter server without taking you to the default page. And that’s it. It’s time to code. Running Jupyter notebook in VSCode There are many ways to create a Jupyer notebook in VS Code (shortcuts, commands, etc). The most easy way is to create a new file named xxx.ipynb and you can directly open it in the VS Code. The first thing is to make sure you select the right Python kernel or environment to run the notebook. You may have multiple choices. Then toggle the Jupyter server selection menu and select Existing server. You will be prompt to enter your Jupyter password. Running cells in VS Code is almost the same as in the Jupyter browser app. Triggering debug mode requires clicking the debug cell button alongside each cell. Just like MATLAB, you can set multiple breakpoints and watch how variables change with the program.","categories":[{"name":"tools","slug":"tools","permalink":"https://mrswolf.github.io/categories/tools/"}],"tags":[{"name":"vscode","slug":"vscode","permalink":"https://mrswolf.github.io/tags/vscode/"},{"name":"python","slug":"python","permalink":"https://mrswolf.github.io/tags/python/"},{"name":"jupyter","slug":"jupyter","permalink":"https://mrswolf.github.io/tags/jupyter/"}]},{"title":"Fourier Transform","slug":"fourier-transform","date":"2023-05-15T16:00:00.000Z","updated":"2023-09-12T15:45:56.083Z","comments":false,"path":"fourier-transform/","link":"fourier-transform","permalink":"https://mrswolf.github.io/fourier-transform/","excerpt":"The Fourier transform decomposes a function into different frequency components, which can be summed to represent the original function.","text":"The Fourier transform decomposes a function into different frequency components, which can be summed to represent the original function. Fourier Series TL;DR Any continuous periodic function can be represented by Fourier series, where Fourier coefficients are the weighted integrals of the periodic function over one period. Definition A periodic function fT(x)f_{T}(x)fT​(x) with a period TTT can be decomposed into the following exponential form: fT(x)=∑n=−∞∞cnei2πnxT\\begin{equation} f_{T}(x) = \\sum_{n=-\\infty}^{\\infty} c_n e^{i 2\\pi n \\frac{x}{T}} \\end{equation} fT​(x)=n=−∞∑∞​cn​ei2πnTx​​​ where cnc_ncn​ are Fourier coefficients: cn=1T∫TfT(x)e−i2πnxTdx\\begin{equation} c_n = \\frac{1}{T} \\int_{T} f_{T}(x) e^{-i 2\\pi n \\frac{x}{T}} dx \\end{equation} cn​=T1​∫T​fT​(x)e−i2πnTx​dx​​ Continuous Fourier Transform TL;DR The Fourier transform works for both periodic and non-periodic continuous functions. The Fourier transform is a special case of the Fourier series when the period goes to infinity. The Fourier transform of a periodic function is a summation of weighted delta functions at specific frequencies (harmonics of 1/T1/T1/T), where the weights are the Fourier coefficients. Definition For any integrable real-valued function f:R→Cf: \\mathbb{R} \\rightarrow \\mathbb{C}f:R→C, the Fourier transform is defined as: f^(k)=∫−∞∞f(x)e−i2πkxdx\\begin{equation} \\hat{f}(k) = \\int_{-\\infty}^{\\infty} f(x) e^{-i 2\\pi k x} dx \\end{equation} f^​(k)=∫−∞∞​f(x)e−i2πkxdx​​ where k∈Rk \\in \\mathbb{R}k∈R represents continuous frequencies. For example, if xxx is measured in seconds, then frequency is in hertz. The Fourier transform is able to represent periodic and non-periodic functions, whereas the Fourier series only works for periodic functions. The inverse Fourier transform is defined as: f(x)=∫−∞∞f^(k)ei2πxkdk\\begin{equation} f(x) = \\int_{-\\infty}^{\\infty} \\hat{f}(k) e^{i 2\\pi x k} dk \\end{equation} f(x)=∫−∞∞​f^​(k)ei2πxkdk​​ f(x)f(x)f(x) and f^(k)\\hat{f}(k)f^​(k) are often referred to as a Fourier transform pair. Here, we use F\\mathcal{F}F and F−1\\mathcal{F}^{-1}F−1 to denote the Fourier transform (FT) and inverse Fourier transform (iFT), respectively. Sign of Fourier transform Remember the sign used in Fourier transform and inverse Fourier transform is just a convention. Mathematicians usually choose a negative sign for the inverse Fourier transform while engineers stuck to a positive sign for it. It is not that one is better than the other. The consistency is the key, otherwise errors and confusions may arise. Connection to Fourier Series Only periodic signals could be decomposed with Fourier series, what about non-periodic signals? We would see that Fourier transform is actually Fourier series when TTT goes to the infinity, meaning there is non-periodic signals. f(x)=lim⁡T→∞fT(x)=lim⁡T→∞∑n=−∞∞cnei2πnxT=lim⁡T→∞∑n=−∞∞[1T∫TfT(x)e−i2πnxTdx]ei2πnxT=lim⁡Δfn→0∑n=−∞∞[∫−T/2T/2fT(x)e−i2πknxdx]ei2πxknΔkn, kn=nT Δkn=1T=∫−∞∞[∫−∞∞f(x)e−i2πkxdx]ei2πxkdk=∫−∞∞f^(k)ei2πxkdk\\begin{equation} \\begin{split} f(x) &amp;= \\lim_{T \\rightarrow \\infty} f_T(x)\\\\ &amp;= \\lim_{T \\rightarrow \\infty} \\sum_{n=-\\infty}^{\\infty} c_n e^{i 2\\pi n \\frac{x}{T}}\\\\ &amp;= \\lim_{T \\rightarrow \\infty} \\sum_{n=-\\infty}^{\\infty} \\left[ \\frac{1}{T} \\int_{T} f_{T}(x) e^{-i 2\\pi n \\frac{x}{T}} dx \\right] e^{i 2\\pi n \\frac{x}{T}}\\\\ &amp;= \\lim_{\\Delta{f_n} \\rightarrow 0} \\sum_{n=-\\infty}^{\\infty} \\left[ \\int_{-T/2}^{T/2} f_{T}(x) e^{-i 2\\pi k_n x} dx \\right] e^{i 2\\pi x k_n} \\Delta{k_n},\\ k_n = \\frac{n}{T}\\ \\Delta{k_n}=\\frac{1}{T}\\\\ &amp;= \\int_{-\\infty}^{\\infty} \\left[ \\int_{-\\infty}^{\\infty} f(x) e^{-i 2\\pi k x} dx \\right] e^{i 2\\pi x k} dk\\\\ &amp;= \\int_{-\\infty}^{\\infty} \\hat{f}(k) e^{i 2\\pi x k} dk \\end{split} \\end{equation} f(x)​=T→∞lim​fT​(x)=T→∞lim​n=−∞∑∞​cn​ei2πnTx​=T→∞lim​n=−∞∑∞​[T1​∫T​fT​(x)e−i2πnTx​dx]ei2πnTx​=Δfn​→0lim​n=−∞∑∞​[∫−T/2T/2​fT​(x)e−i2πkn​xdx]ei2πxkn​Δkn​, kn​=Tn​ Δkn​=T1​=∫−∞∞​[∫−∞∞​f(x)e−i2πkxdx]ei2πxkdk=∫−∞∞​f^​(k)ei2πxkdk​​​ and that is exactly Fourier transform (note the definition of integral above). Properties Linearity For any complex numbers aaa and bbb, if h(x)=af(x)+bg(x)h(x)=af(x)+bg(x)h(x)=af(x)+bg(x), then h^(k)=f^(k)+g^(k)\\hat{h}(k) = \\hat{f}(k) + \\hat{g}(k)h^(k)=f^​(k)+g^​(k). Time Shifting For any real number x0x_0x0​, if h(x)=f(x−x0)h(x)=f(x-x_0)h(x)=f(x−x0​), then h^(k)=e−i2πx0kf^(k)\\hat{h}(k)=e^{-i 2\\pi x_0 k} \\hat{f}(k)h^(k)=e−i2πx0​kf^​(k). F[h(x)]=∫−∞∞f(x−x0)e−i2πkxdx=∫−∞∞f(x^)e−i2πk(x^+x0)d(x^+x0)=e−i2πx0k∫−∞∞f(x^)e−i2πkx^dx^=e−i2πx0kf^(k)\\begin{equation} \\begin{split} \\mathcal{F}\\left[h(x)\\right] &amp;= \\int_{-\\infty}^{\\infty} f(x-x_0) e^{-i 2\\pi k x} dx\\\\ &amp;= \\int_{-\\infty}^{\\infty} f(\\hat{x}) e^{-i 2\\pi k (\\hat{x}+x_0)} d(\\hat{x}+x_0)\\\\ &amp;= e^{-i 2\\pi x_0 k} \\int_{-\\infty}^{\\infty} f(\\hat{x}) e^{-i 2\\pi k \\hat{x}} d\\hat{x}\\\\ &amp;= e^{-i 2\\pi x_0 k} \\hat{f}(k) \\end{split} \\end{equation} F[h(x)]​=∫−∞∞​f(x−x0​)e−i2πkxdx=∫−∞∞​f(x^)e−i2πk(x^+x0​)d(x^+x0​)=e−i2πx0​k∫−∞∞​f(x^)e−i2πkx^dx^=e−i2πx0​kf^​(k)​​​ Frequency Shifting For any real number k0k_0k0​, if h^(k)=f^(k−k0)\\hat{h}(k) = \\hat{f}(k-k_0)h^(k)=f^​(k−k0​), then h(x)=ei2πk0xf(x)h(x)=e^{i 2 \\pi k_0 x} f(x)h(x)=ei2πk0​xf(x). F−1[h^(k)]=∫−∞∞f^(k−k0)ei2πxkdk=∫−∞∞f^(k^)ei2πx(k^+k0)d(k^+k0)=ei2πk0x∫−∞∞f^(k^)ei2πxk^dk^=ei2πk0xf(x)\\begin{equation} \\begin{split} \\mathcal{F}^{-1}\\left[\\hat{h}(k)\\right] &amp;= \\int_{-\\infty}^{\\infty} \\hat{f}(k-k_0) e^{i 2\\pi x k} dk\\\\ &amp;= \\int_{-\\infty}^{\\infty} \\hat{f}(\\hat{k}) e^{i 2\\pi x(\\hat{k}+k_0)} d(\\hat{k}+k_0)\\\\ &amp;= e^{i 2\\pi k_0 x} \\int_{-\\infty}^{\\infty} \\hat{f}(\\hat{k}) e^{i 2\\pi x\\hat{k}} d\\hat{k}\\\\ &amp;= e^{i 2\\pi k_0 x} f(x) \\end{split} \\end{equation} F−1[h^(k)]​=∫−∞∞​f^​(k−k0​)ei2πxkdk=∫−∞∞​f^​(k^)ei2πx(k^+k0​)d(k^+k0​)=ei2πk0​x∫−∞∞​f^​(k^)ei2πxk^dk^=ei2πk0​xf(x)​​​ Scale Property For any real number aaa, if h(x)=f(ax)h(x)=f(ax)h(x)=f(ax), then h^(k)=1∣a∣f^(ka)\\hat{h}(k)=\\frac{1}{|a|}\\hat{f}(\\frac{k}{a})h^(k)=∣a∣1​f^​(ak​). Let’s assuming a&gt;0a \\gt 0a&gt;0: F[h(x)]=∫−∞∞f(ax)e−i2πkxdx=∫−∞∞f(x^)e−i2πk(x^/a)d(x^/a)=1a∫−∞∞f(x^)e−i2π(k/a)x^dx^=1af^(ka)\\begin{equation} \\begin{split} \\mathcal{F}\\left[h(x)\\right] &amp;= \\int_{-\\infty}^{\\infty} f(ax) e^{-i 2\\pi k x} dx\\\\ &amp;= \\int_{-\\infty}^{\\infty} f(\\hat{x}) e^{-i 2\\pi k (\\hat{x}/a)} d(\\hat{x}/a)\\\\ &amp;= \\frac{1}{a} \\int_{-\\infty}^{\\infty} f(\\hat{x}) e^{-i 2\\pi (k / a) \\hat{x}} d\\hat{x}\\\\ &amp;= \\frac{1}{a} \\hat{f}(\\frac{k}{a}) \\end{split} \\end{equation} F[h(x)]​=∫−∞∞​f(ax)e−i2πkxdx=∫−∞∞​f(x^)e−i2πk(x^/a)d(x^/a)=a1​∫−∞∞​f(x^)e−i2π(k/a)x^dx^=a1​f^​(ak​)​​​ and if a&lt;0a \\lt 0a&lt;0: F[h(x)]=∫−∞∞f(ax)e−i2πkxdx=∫∞−∞f(x^)e−i2πk(x^/a)d(x^/a)=1−a∫−∞∞f(x^)e−i2π(k/a)x^dx^=1−aF(ka)\\begin{equation} \\begin{split} \\mathcal{F}\\left[h(x)\\right] &amp;= \\int_{-\\infty}^{\\infty} f(ax) e^{-i 2\\pi k x} dx\\\\ &amp;= \\int_{\\infty}^{-\\infty} f(\\hat{x}) e^{-i 2\\pi k (\\hat{x}/a)} d(\\hat{x}/a)\\\\ &amp;= \\frac{1}{-a} \\int_{-\\infty}^{\\infty} f(\\hat{x}) e^{-i 2\\pi (k/ a) \\hat{x}} d\\hat{x}\\\\ &amp;= \\frac{1}{-a} F(\\frac{k}{a}) \\end{split} \\end{equation} F[h(x)]​=∫−∞∞​f(ax)e−i2πkxdx=∫∞−∞​f(x^)e−i2πk(x^/a)d(x^/a)=−a1​∫−∞∞​f(x^)e−i2π(k/a)x^dx^=−a1​F(ak​)​​​ Time Convolution Theorem For Fourier transform pairs f(x)↔f^(k)f(x) \\leftrightarrow \\hat{f}(k)f(x)↔f^​(k) and g(x)↔g^(k)g(x) \\leftrightarrow \\hat{g}(k)g(x)↔g^​(k), we have: F[f(x)∗g(x)]=∫−∞∞∫−∞∞f(τ)g(x−τ)dτe−i2πkxdx=∫−∞∞∫−∞∞g(x−τ)e−i2πkxdxf(τ)dτ=∫−∞∞∫−∞∞g(x^)e−i2πk(x^+τ)d(x^+τ)f(τ)dτ=∫−∞∞(∫−∞∞g(x^)e−i2πkx^dx^)f(τ)e−i2πkτdτ=g^(k)∫−∞∞f(τ)e−i2πkτdτ=g^(k)f^(k)\\begin{equation} \\begin{split} \\mathcal{F}\\left[f(x) \\ast g(x)\\right] &amp;= \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} f(\\tau)g(x-\\tau) d\\tau e^{-i 2\\pi k x} dx \\\\ &amp;= \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} g(x-\\tau) e^{-i 2\\pi k x} dx f(\\tau) d\\tau\\\\ &amp;= \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} g(\\hat{x}) e^{-i 2\\pi k (\\hat{x}+\\tau)} d(\\hat{x}+\\tau) f(\\tau) d\\tau\\\\ &amp;= \\int_{-\\infty}^{\\infty} \\left(\\int_{-\\infty}^{\\infty} g(\\hat{x}) e^{-i 2\\pi k \\hat{x}} d\\hat{x}\\right) f(\\tau) e^{-i 2\\pi k \\tau} d\\tau\\\\ &amp;= \\hat{g}(k) \\int_{-\\infty}^{\\infty} f(\\tau) e^{-i 2\\pi k \\tau} d\\tau\\\\ &amp;= \\hat{g}(k) \\hat{f}(k) \\end{split} \\end{equation} F[f(x)∗g(x)]​=∫−∞∞​∫−∞∞​f(τ)g(x−τ)dτe−i2πkxdx=∫−∞∞​∫−∞∞​g(x−τ)e−i2πkxdxf(τ)dτ=∫−∞∞​∫−∞∞​g(x^)e−i2πk(x^+τ)d(x^+τ)f(τ)dτ=∫−∞∞​(∫−∞∞​g(x^)e−i2πkx^dx^)f(τ)e−i2πkτdτ=g^​(k)∫−∞∞​f(τ)e−i2πkτdτ=g^​(k)f^​(k)​​​ Frequency Convolution Theorem For Fourier transform pairs f(x)↔f^(k)f(x) \\leftrightarrow \\hat{f}(k)f(x)↔f^​(k) and g(x)↔g^(k)g(x) \\leftrightarrow \\hat{g}(k)g(x)↔g^​(k), we have: F−1[f^(k)∗g^(k)]=∫−∞∞∫−∞∞f^(τ)g^(k−τ)dτei2πxkdk=∫−∞∞∫−∞∞g^(k−τ)ei2πxkdkf^(τ)dτ=∫−∞∞∫−∞∞g^(k^)ei2πx(k^+τ)d(k^+τ)f^(τ)dτ=∫−∞∞(∫−∞∞g^(k^)ei2πxk^dk^)f^(τ)ei2πxτdτ=g(x)∫−∞∞f^(τ)ei2πxτdτ=g(x)f(x)\\begin{equation} \\begin{split} \\mathcal{F}^{-1}\\left[\\hat{f}(k) \\ast \\hat{g}(k)\\right] &amp;= \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} \\hat{f}(\\tau) \\hat{g}(k-\\tau) d\\tau e^{i 2\\pi x k} dk\\\\ &amp;= \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} \\hat{g}(k-\\tau) e^{i 2\\pi x k} dk \\hat{f}(\\tau) d\\tau\\\\ &amp;= \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} \\hat{g}(\\hat{k}) e^{i 2\\pi x (\\hat{k}+\\tau)} d(\\hat{k}+\\tau) \\hat{f}(\\tau) d\\tau\\\\ &amp;= \\int_{-\\infty}^{\\infty} \\left(\\int_{-\\infty}^{\\infty} \\hat{g}(\\hat{k}) e^{i 2\\pi x \\hat{k}} d\\hat{k}\\right) \\hat{f}(\\tau) e^{i 2\\pi x \\tau} d\\tau\\\\ &amp;= g(x) \\int_{-\\infty}^{\\infty} \\hat{f}(\\tau) e^{i 2\\pi x \\tau} d\\tau\\\\ &amp;= g(x) f(x) \\end{split} \\end{equation} F−1[f^​(k)∗g^​(k)]​=∫−∞∞​∫−∞∞​f^​(τ)g^​(k−τ)dτei2πxkdk=∫−∞∞​∫−∞∞​g^​(k−τ)ei2πxkdkf^​(τ)dτ=∫−∞∞​∫−∞∞​g^​(k^)ei2πx(k^+τ)d(k^+τ)f^​(τ)dτ=∫−∞∞​(∫−∞∞​g^​(k^)ei2πxk^dk^)f^​(τ)ei2πxτdτ=g(x)∫−∞∞​f^​(τ)ei2πxτdτ=g(x)f(x)​​​ Conguation F[f(x)‾]=∫−∞∞f(x)‾e−i2πkxdx=∫−∞∞f(x)ei2πkxdx‾=∫−∞∞f(x)e−i2π(−k)xdx‾=f^(−k)‾\\begin{equation} \\begin{split} \\mathcal{F}\\left[\\overline{f(x)}\\right] &amp;= \\int_{-\\infty}^{\\infty} \\overline{f(x)} e^{-i 2\\pi k x} dx \\\\ &amp;= \\overline{ \\int_{-\\infty}^{\\infty} f(x) e^{i 2\\pi k x} dx}\\\\ &amp;= \\overline{ \\int_{-\\infty}^{\\infty} f(x) e^{-i 2\\pi (-k) x} dx}\\\\ &amp;= \\overline{\\hat{f}(-k)} \\end{split} \\end{equation} F[f(x)​]​=∫−∞∞​f(x)​e−i2πkxdx=∫−∞∞​f(x)ei2πkxdx​=∫−∞∞​f(x)e−i2π(−k)xdx​=f^​(−k)​​​​ If f(x)f(x)f(x) is a real-valued function, then f^(−k)=f^(k)‾\\hat{f}(-k)=\\overline{\\hat{f}(k)}f^​(−k)=f^​(k)​, which is referred to as conjugate symmetric property. If f(x)f(x)f(x) is an imaginary-valued function, then f^(−k)=−f^(k)‾\\hat{f}(-k)=- \\overline{\\hat{f}(k)}f^​(−k)=−f^​(k)​, which is referred to as conjugate anti-symmetric property. Same properties occur in the inverse Fourier transform. Common FT Pairs Time Domain Frequency Domain Description 111 δ(k)\\delta(k)δ(k) δ(k)=∫−∞∞e−i2πkxdx\\delta(k) = \\int_{-\\infty}^{\\infty} e^{-i 2\\pi k x} dxδ(k)=∫−∞∞​e−i2πkxdx δ(x)\\delta(x)δ(x) 111 1=∫−∞∞δ(x)e−i2πkxdx1 = \\int_{-\\infty}^{\\infty} \\delta(x) e^{-i 2\\pi k x} dx1=∫−∞∞​δ(x)e−i2πkxdx sgn(x)={1,x&gt;00,x=0−1,x&lt;0\\mathrm{sgn}(x) = \\left\\{\\begin{aligned} 1,x \\gt 0\\\\ 0,x = 0\\\\-1,x \\lt 0\\\\ \\end{aligned}\\right.sgn(x)=⎩⎨⎧​1,x&gt;00,x=0−1,x&lt;0​ 1/(iπk)1/(i\\pi k)1/(iπk) sgn(x)\\mathrm{sgn}(x)sgn(x) is the sign function u(x)={1,x&gt;01/2,x=00,x&lt;0u(x) = \\left\\{\\begin{aligned} 1,x \\gt 0\\\\ 1/2,x = 0\\\\0,x \\lt 0\\\\ \\end{aligned}\\right.u(x)=⎩⎨⎧​1,x&gt;01/2,x=00,x&lt;0​ 12(δ(k)+1/(iπk))\\frac{1}{2}\\left(\\delta(k) + 1/(i\\pi k)\\right)21​(δ(k)+1/(iπk)) u(x)u(x)u(x) is the unit step function.u(x)=12(1+sgn(x))u(x)=\\frac{1}{2}(1+\\mathrm{sgn}(x))u(x)=21​(1+sgn(x)) eiaxe^{i a x}eiax δ(k−a/(2π))\\delta(k-a/(2\\pi))δ(k−a/(2π)) Frequency Shifting cos(ax)\\mathrm{cos}(ax)cos(ax) 12(δ(k−a/(2π))+δ(k+a/(2π)))\\frac{1}{2}\\left(\\delta(k-a/(2\\pi)) + \\delta(k+a/(2\\pi))\\right)21​(δ(k−a/(2π))+δ(k+a/(2π))) cos(ax)=12(eiax+e−iax)\\mathrm{cos}(ax) =\\\\ \\frac{1}{2}(e^{i a x} + e^{-i a x})cos(ax)=21​(eiax+e−iax) sin(ax)\\mathrm{sin}(ax)sin(ax) 12(δ(k−a/(2π))−δ(k+a/(2π)))\\frac{1}{2}\\left(\\delta(k-a/(2\\pi)) - \\delta(k+a/(2\\pi))\\right)21​(δ(k−a/(2π))−δ(k+a/(2π))) sin(ax)=12(eiax−e−iax)\\mathrm{sin}(ax) =\\\\ \\frac{1}{2}(e^{i a x} - e^{-i a x})sin(ax)=21​(eiax−e−iax) rect(x)={1,∣x∣&lt;1/21/2,∣x∣=1/20,∣x∣&gt;1/2\\mathrm{rect}(x)=\\left\\{\\begin{aligned}1,\\lvert x \\rvert \\lt 1/2\\\\ 1/2,\\lvert x \\rvert = 1/2\\\\ 0,\\lvert x \\rvert \\gt 1/2\\end{aligned}\\right.rect(x)=⎩⎨⎧​1,∣x∣&lt;1/21/2,∣x∣=1/20,∣x∣&gt;1/2​ sinc(k)=sin(πk)/(πk)\\mathrm{sinc}(k) = \\mathrm{sin}(\\pi k)/(\\pi k)sinc(k)=sin(πk)/(πk) rect(x)=u(x+1/2)−u(x−1/2)\\mathrm{rect}(x) =\\\\ u(x+1/2)-u(x-1/2)rect(x)=u(x+1/2)−u(x−1/2) sinc(x)\\mathrm{sinc}(x)sinc(x) rect(k)={1,∣k∣&lt;1/21/2,∣k∣=1/20,∣k∣&gt;1/2\\mathrm{rect}(k)=\\left\\{\\begin{aligned}1,\\lvert k \\rvert \\lt 1/2\\\\ 1/2,\\lvert k \\rvert = 1/2\\\\ 0,\\lvert k \\rvert \\gt 1/2\\end{aligned}\\right.rect(k)=⎩⎨⎧​1,∣k∣&lt;1/21/2,∣k∣=1/20,∣k∣&gt;1/2​ ∑n=−∞∞δ(x−nΔx)\\sum_{n=-\\infty}^{\\infty} \\delta(x-n \\Delta x)∑n=−∞∞​δ(x−nΔx) 1Δx∑m=−∞∞δ(k−m/Δx)\\frac{1}{\\Delta x} \\sum_{m=-\\infty}^{\\infty} \\delta(k-m/\\Delta x)Δx1​∑m=−∞∞​δ(k−m/Δx) Comb function Comb Function A comb function (a.k.a sampling function, sometimes referred to as impulse sampling) is a periodic function with the formula: SΔx(x)=∑n=−∞∞δ(x−nΔx)S_{\\Delta{x}}(x) = \\sum_{n=-\\infty}^{\\infty} \\delta(x-n\\Delta{x}) SΔx​(x)=n=−∞∑∞​δ(x−nΔx) where Δx\\Delta{x}Δx is the given period. Fourier transform could be extended to [[generalized functions]] like δ(x)\\delta(x)δ(x), which makes it possible to bypass the limitation of absolute integrable property on f(x)f(x)f(x). The key is to decompose periodic functions into Fourier series and use additive property of Fourier transform . The Fourier transform of SΔx(x)S_{\\Delta{x}}(x)SΔx​(x) is: F(SΔx(x))=∫−∞∞∑n=−∞∞δ(x−nΔx)e−i2πkxdx=∑n=−∞∞∫−∞∞δ(x−nΔx)e−i2πkxdx=∑n=−∞∞e−i2πknΔx\\begin{equation} \\begin{split} \\mathcal{F}\\left(S_{\\Delta{x}}(x)\\right) &amp;= \\int_{-\\infty}^{\\infty} \\sum_{n=-\\infty}^{\\infty} \\delta(x-n\\Delta{x}) e^{-i 2\\pi k x} dx\\\\ &amp;= \\sum_{n=-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} \\delta(x-n\\Delta{x}) e^{-i 2\\pi k x} dx\\\\ &amp;= \\sum_{n=-\\infty}^{\\infty} e^{-i 2\\pi k n \\Delta{x}}\\\\ \\end{split} \\end{equation} F(SΔx​(x))​=∫−∞∞​n=−∞∑∞​δ(x−nΔx)e−i2πkxdx=n=−∞∑∞​∫−∞∞​δ(x−nΔx)e−i2πkxdx=n=−∞∑∞​e−i2πknΔx​​​ It is not obvious to see what the transform is, but we can prove it with Fourier series. Note that SΔx(x)S_{\\Delta{x}}(x)SΔx​(x) is a periodic function, its Fourier series is represented as: cn=1Δx∫ΔxSΔx(x)e−i2πnx/Δxdx=1Δx∫−Δx2Δx2∑m=−∞∞δ(x−mΔx)e−i2πnx/Δxdx=1Δx∫−Δx2Δx2δ(x)e−i2πnx/Δxdx=1Δx∫−∞∞δ(x)e−i2πnx/Δxdx=1ΔxSΔx(x)=∑n=−∞∞cnei2πnx/Δx=1Δx∑n=−∞∞ei2πnx/Δx\\begin{equation} \\begin{split} c_n &amp;= \\frac{1}{\\Delta{x}} \\int_{\\Delta{x}} S_{\\Delta{x}}(x) e^{-i 2\\pi n x / \\Delta{x}} dx\\\\ &amp;= \\frac{1}{\\Delta{x}} \\int_{-\\frac{\\Delta{x}}{2}}^{\\frac{\\Delta{x}}{2}} \\sum_{m=-\\infty}^{\\infty} \\delta(x-m\\Delta{x}) e^{-i 2\\pi n x / \\Delta{x}} dx\\\\ &amp;= \\frac{1}{\\Delta{x}} \\int_{-\\frac{\\Delta{x}}{2}}^{\\frac{\\Delta{x}}{2}} \\delta(x) e^{-i 2\\pi n x / \\Delta{x}} dx\\\\ &amp;= \\frac{1}{\\Delta{x}} \\int_{-\\infty}^{\\infty} \\delta(x) e^{-i 2\\pi n x / \\Delta{x}} dx\\\\ &amp;= \\frac{1}{\\Delta{x}}\\\\ S_{\\Delta{x}}(x) &amp;= \\sum_{n=-\\infty}^{\\infty} c_n e^{i 2\\pi n x / \\Delta{x}}\\\\ &amp;= \\frac{1}{\\Delta{x}} \\sum_{n=-\\infty}^{\\infty} e^{i 2\\pi n x / \\Delta{x}} \\end{split} \\end{equation} cn​SΔx​(x)​=Δx1​∫Δx​SΔx​(x)e−i2πnx/Δxdx=Δx1​∫−2Δx​2Δx​​m=−∞∑∞​δ(x−mΔx)e−i2πnx/Δxdx=Δx1​∫−2Δx​2Δx​​δ(x)e−i2πnx/Δxdx=Δx1​∫−∞∞​δ(x)e−i2πnx/Δxdx=Δx1​=n=−∞∑∞​cn​ei2πnx/Δx=Δx1​n=−∞∑∞​ei2πnx/Δx​​​ and Dirac delta function is expressed as: δ(x)=∫−∞∞ei2πxkdk=∫−∞∞e−i2πx(−k)dk=∫∞−∞e−i2πxk^d(−k^)=∫−∞∞e−i2πxk^d(k^)\\begin{equation} \\begin{split} \\delta(x) &amp;= \\int_{-\\infty}^{\\infty} e^{i 2\\pi x k} dk\\\\ &amp;= \\int_{-\\infty}^{\\infty} e^{-i 2\\pi x (-k)} dk\\\\ &amp;= \\int_{\\infty}^{-\\infty} e^{-i 2\\pi x \\hat{k}} d(-\\hat{k})\\\\ &amp;= \\int_{-\\infty}^{\\infty} e^{-i 2\\pi x \\hat{k}} d(\\hat{k}) \\end{split} \\end{equation} δ(x)​=∫−∞∞​ei2πxkdk=∫−∞∞​e−i2πx(−k)dk=∫∞−∞​e−i2πxk^d(−k^)=∫−∞∞​e−i2πxk^d(k^)​​​ Now we can apply Fourier transform to SΔx(x)S_{\\Delta{x}}(x)SΔx​(x): F(SΔx(x))=∫−∞∞∑n=−∞∞cnei2πnx/Δxe−i2πkxdx=1Δx∑n=−∞∞∫−∞∞ei2π(n/Δx−k)xdx=1Δx∑n=−∞∞δ(k−n/Δx)\\begin{equation} \\begin{split} \\mathcal{F}\\left(S_{\\Delta{x}}(x)\\right) &amp;= \\int_{-\\infty}^{\\infty} \\sum_{n=-\\infty}^{\\infty} c_n e^{i 2\\pi n x / \\Delta{x}} e^{-i 2\\pi k x} dx\\\\ &amp;= \\frac{1}{\\Delta{x}} \\sum_{n=-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} e^{i 2\\pi (n/\\Delta{x} - k) x} dx\\\\ &amp;= \\frac{1}{\\Delta{x}} \\sum_{n=-\\infty}^{\\infty} \\delta({k - n/\\Delta{x}}) \\end{split} \\end{equation} F(SΔx​(x))​=∫−∞∞​n=−∞∑∞​cn​ei2πnx/Δxe−i2πkxdx=Δx1​n=−∞∑∞​∫−∞∞​ei2π(n/Δx−k)xdx=Δx1​n=−∞∑∞​δ(k−n/Δx)​​​ so F(SΔx(x))\\mathcal{F}\\left(S_{\\Delta{x}}(x)\\right)F(SΔx​(x)) is also a periodic function with the period as 1/Δx1/\\Delta{x}1/Δx. Hence, we again apply Fourier series: cn=Δx∫1ΔxF(SΔx(x))e−i2πnkΔxdk=∫−12Δx12Δx∑m=−∞∞δ(k−m/Δx)e−i2πnkΔxdk=∫−12Δx12Δxδ(k)e−i2πnkΔxdk=∫−∞∞δ(k)e−i2πnkΔxdk=1F(SΔx(x))=∑n=−∞∞cnei2πnkΔx=∑n=−∞∞ei2πnkΔx=∑n=−∞∞e−i2πnkΔx\\begin{equation} \\begin{split} c_n &amp;= \\Delta{x} \\int_{\\frac{1}{\\Delta{x}}} \\mathcal{F}\\left(S_{\\Delta{x}}(x)\\right) e^{-i 2\\pi n k \\Delta{x}} dk\\\\ &amp;= \\int_{-\\frac{1}{2\\Delta{x}}}^{\\frac{1}{2\\Delta{x}}} \\sum_{m=-\\infty}^{\\infty} \\delta(k-m/\\Delta{x}) e^{-i 2\\pi n k \\Delta{x}} dk\\\\ &amp;= \\int_{-\\frac{1}{2\\Delta{x}}}^{\\frac{1}{2\\Delta{x}}} \\delta(k) e^{-i 2\\pi n k \\Delta{x}} dk\\\\ &amp;= \\int_{-\\infty}^{\\infty} \\delta(k) e^{-i 2\\pi n k \\Delta{x}} dk\\\\ &amp;= 1\\\\ \\mathcal{F}\\left(S_{\\Delta{x}}(x)\\right) &amp;= \\sum_{n=-\\infty}^{\\infty} c_n e^{i 2\\pi n k \\Delta{x}}\\\\ &amp;= \\sum_{n=-\\infty}^{\\infty} e^{i 2\\pi n k \\Delta{x}}\\\\ &amp;= \\sum_{n=-\\infty}^{\\infty} e^{-i 2\\pi n k \\Delta{x}} \\end{split} \\end{equation} cn​F(SΔx​(x))​=Δx∫Δx1​​F(SΔx​(x))e−i2πnkΔxdk=∫−2Δx1​2Δx1​​m=−∞∑∞​δ(k−m/Δx)e−i2πnkΔxdk=∫−2Δx1​2Δx1​​δ(k)e−i2πnkΔxdk=∫−∞∞​δ(k)e−i2πnkΔxdk=1=n=−∞∑∞​cn​ei2πnkΔx=n=−∞∑∞​ei2πnkΔx=n=−∞∑∞​e−i2πnkΔx​​​ and we can see that why Fourier transform of a comb function is still a comb function. FT of Periodic Functions With the help of Dirac delta function, Fourier transform could also be used on periodic functions. Considering a periodic function fT(x)f_{T}(x)fT​(x) with period TTT, we can write it as a Fourier series: fT(x)=∑n=−∞∞cnei2πnx/T\\begin{equation} f_{T}(x) = \\sum_{n=-\\infty}^{\\infty} c_n e^{i 2\\pi n x/T} \\end{equation} fT​(x)=n=−∞∑∞​cn​ei2πnx/T​​ Let’s compute the Fourier transform: F(fT(x))=∫−∞∞fT(x)e−i2πkxdx=∑n=−∞∞cn∫−∞∞ei2π(n/T−k)xdx=∑n=−∞∞cnδ(n/T−k)=∑n=−∞∞cnδ(k−n/T)\\begin{equation} \\begin{split} \\mathcal{F}(f_T(x)) &amp;= \\int_{-\\infty}^{\\infty} f_T(x) e^{-i 2\\pi k x} dx\\\\ &amp;= \\sum_{n=-\\infty}^{\\infty} c_n \\int_{-\\infty}^{\\infty} e^{i 2\\pi (n/T-k) x} dx\\\\ &amp;= \\sum_{n=-\\infty}^{\\infty} c_n \\delta(n/T - k)\\\\ &amp;= \\sum_{n=-\\infty}^{\\infty} c_n \\delta(k - n/T) \\end{split} \\end{equation} F(fT​(x))​=∫−∞∞​fT​(x)e−i2πkxdx=n=−∞∑∞​cn​∫−∞∞​ei2π(n/T−k)xdx=n=−∞∑∞​cn​δ(n/T−k)=n=−∞∑∞​cn​δ(k−n/T)​​​ so the Fourier transform of a periodic function is a sum of delta functions at the Fourier series frequencies and the weight of each delta function is the Fourier series coefficient, as we proved in the Fourier transform of the comb function. The inverse Fourier transform is: F(∑n=−∞∞cnδ(k−n/T))=∫−∞∞∑n=−∞∞cnδ(k−n/T)ei2πxkdk=∑n=−∞∞cn∫−∞∞δ(k−n/T)ei2πxkdk=∑n=−∞∞cnei2πxn/T=fT(x)\\begin{equation} \\begin{split} \\mathcal{F}(\\sum_{n=-\\infty}^{\\infty} c_n \\delta(k - n/T)) &amp;= \\int_{-\\infty}^{\\infty} \\sum_{n=-\\infty}^{\\infty} c_n \\delta(k - n/T) e^{i 2\\pi x k} dk\\\\ &amp;= \\sum_{n=-\\infty}^{\\infty} c_n \\int_{-\\infty}^{\\infty} \\delta(k - n/T) e^{i 2\\pi x k} dk\\\\ &amp;= \\sum_{n=-\\infty}^{\\infty} c_n e^{i 2\\pi x n/T}\\\\ &amp;= f_T(x) \\end{split} \\end{equation} F(n=−∞∑∞​cn​δ(k−n/T))​=∫−∞∞​n=−∞∑∞​cn​δ(k−n/T)ei2πxkdk=n=−∞∑∞​cn​∫−∞∞​δ(k−n/T)ei2πxkdk=n=−∞∑∞​cn​ei2πxn/T=fT​(x)​​​ Discrete-Time Fourier Transform TL;DR The discrete-time Fourier transform (DTFT) is a special case of the Fourier transform when the original function is sampled. The frequency domain of DTFT is continuous but periodic, with a period of 1/Δx1/\\Delta{x}1/Δx, where Δx\\Delta{x}Δx is the sampling interval. The DTFT of periodic sequence is a summation of weighted delta functions. Definition For a discrete sequence of real or complex values f[n]f[n]f[n] with all integers nnn, the discrete-time Fourier transform is defined as: f^dtft(k)=∑n=−∞∞f[n]e−i2πnkΔx\\begin{equation} \\hat{f}_{dtft}(k) = \\sum_{n=-\\infty}^{\\infty} f[n] e^{-i2\\pi n k \\Delta{x}} \\end{equation} f^​dtft​(k)=n=−∞∑∞​f[n]e−i2πnkΔx​​ where 1Δx\\frac{1}{\\Delta{x}}Δx1​ is the sampling frequency in the time domain. This formula can be seen as a Fourier series (−-− and +++ signs are the same thing) and f^dtft(k)\\hat{f}_{dtft}(k)f^​dtft​(k) is actually a periodic function with period 1/Δx1/\\Delta{x}1/Δx. The inverse discrete-time Fourier transform is defined as: f[n]=Δx∫1/Δxf^dtft(k)ei2πnkΔxdk\\begin{equation} f[n] = \\Delta{x} \\int_{1/\\Delta{x}} \\hat{f}_{dtft}(k) e^{i 2\\pi n k \\Delta{x}} dk \\end{equation} f[n]=Δx∫1/Δx​f^​dtft​(k)ei2πnkΔxdk​​ note the integral is only evaluated in a period. Here we use Fdtft\\mathcal{F}_{dtft}Fdtft​ to represent the discrete-time Fourier transform (DTFT) and Fdtft−1\\mathcal{F}_{dtft}^{-1}Fdtft−1​ to represent the inverse discrete-time Fourier transform (iDTFT). Connection to the FT Modern computers can only handle discrete values instead of continuous signals. The most basic discretization technique is sampling. Considering a continuous function f(x)f(x)f(x) and an uniform sampling pattern SΔx(x)=∑n=−∞∞δ(x−nΔx)S_{\\Delta{x}}(x) = \\sum_{n=-\\infty}^{\\infty} \\delta(x-n\\Delta{x})SΔx​(x)=∑n=−∞∞​δ(x−nΔx), which is the [[#Comb Function]] we described above, the sampling process can be simulated as: fS(x)=f(x)SΔx(x)=∑n=−∞∞f(x)δ(x−nΔx)\\begin{equation} \\begin{split} f_S(x) &amp;= f(x) S_{\\Delta{x}}(x)\\\\ &amp;= \\sum_{n=-\\infty}^{\\infty} f(x)\\delta(x-n\\Delta{x}) \\end{split} \\end{equation} fS​(x)​=f(x)SΔx​(x)=n=−∞∑∞​f(x)δ(x−nΔx)​​​ The Fourier transform (using the definition) of the above function is: f^S(k)=∫−∞∞fS(x)e−i2πkxdx=∫−∞∞(∑n=−∞∞f(x)δ(x−nΔx))e−i2πkxdx=∑n=−∞∞(∫−∞∞f(x)e−i2πkxδ(x−nΔx)dx)=∑n=−∞∞f(nΔx)e−i2πknΔx, f[n]=f(nΔx)=∑n=−∞∞f[n]e−i2πnkΔx=f^dtft(k)\\begin{equation} \\begin{split} \\hat{f}_S(k) &amp;= \\int_{-\\infty}^{\\infty} f_S(x) e^{-i 2\\pi k x} dx\\\\ &amp;= \\int_{-\\infty}^{\\infty} \\left( \\sum_{n=-\\infty}^{\\infty} f(x)\\delta(x-n\\Delta{x}) \\right) e^{-i 2\\pi k x} dx\\\\ &amp;= \\sum_{n=-\\infty}^{\\infty} \\left( \\int_{-\\infty}^{\\infty} f(x) e^{-i 2\\pi k x} \\delta(x-n\\Delta{x}) dx\\right)\\\\ &amp;= \\sum_{n=-\\infty}^{\\infty} f(n\\Delta{x})e^{-i 2\\pi k n\\Delta{x}},\\ f[n]=f(n\\Delta{x})\\\\ &amp;=\\sum_{n=-\\infty}^{\\infty} f[n] e^{-i 2\\pi n k \\Delta{x}}\\\\ &amp;= \\hat{f}_{dtft}(k) \\end{split} \\end{equation} f^​S​(k)​=∫−∞∞​fS​(x)e−i2πkxdx=∫−∞∞​(n=−∞∑∞​f(x)δ(x−nΔx))e−i2πkxdx=n=−∞∑∞​(∫−∞∞​f(x)e−i2πkxδ(x−nΔx)dx)=n=−∞∑∞​f(nΔx)e−i2πknΔx, f[n]=f(nΔx)=n=−∞∑∞​f[n]e−i2πnkΔx=f^​dtft​(k)​​​ which is the definition of discrete-time Fourier transform. The next step is to prove the correctness of the inverse discrete-time Fourier transform: Δx∫1/Δxf^dtft(k)ei2πnkΔxdk=Δx∫−12Δx12Δxf^dtft(k)ei2πnkΔxdk=Δx∫−12Δx12Δxf^S(k)ei2πnkΔxdk=Δx∫−12Δx12Δx[∑m=−∞∞f[m]e−i2πmkΔx]ei2πnkΔxdk=Δx∑m=−∞∞f[m][∫−12Δx12Δxei2π(n−m)kΔxdk]=Δx∑m=−∞∞f[m][1i2π(n−m)Δxei2π(n−m)kΔx∣−12Δx12Δx]=Δx∑m=−∞∞f[m][1Δxsin⁡(π(n−m))π(n−m)]=f[n]\\begin{equation} \\begin{split} \\Delta{x} \\int_{1/\\Delta{x}} \\hat{f}_{dtft}(k) e^{i 2\\pi n k \\Delta{x}} dk &amp;= \\Delta{x} \\int_{-\\frac{1}{2\\Delta{x}}}^{\\frac{1}{2\\Delta{x}}} \\hat{f}_{dtft}(k) e^{i 2\\pi n k \\Delta{x}} dk\\\\ &amp;= \\Delta{x} \\int_{-\\frac{1}{2\\Delta{x}}}^{\\frac{1}{2\\Delta{x}}} \\hat{f}_{S}(k) e^{i 2\\pi n k \\Delta{x}} dk\\\\ &amp;=\\Delta{x} \\int_{-\\frac{1}{2\\Delta{x}}}^{\\frac{1}{2\\Delta{x}}} \\left[\\sum_{m=-\\infty}^{\\infty} f[m] e^{-i 2\\pi m k \\Delta{x}}\\right] e^{i 2\\pi n k \\Delta{x}} dk\\\\ &amp;=\\Delta{x} \\sum_{m=-\\infty}^{\\infty} f[m] \\left[ \\int_{-\\frac{1}{2\\Delta{x}}}^{\\frac{1}{2\\Delta{x}}} e^{i 2\\pi (n-m) k \\Delta{x}} dk \\right]\\\\ &amp;= \\Delta{x} \\sum_{m=-\\infty}^{\\infty} f[m] \\left[ \\left. \\frac{1}{i 2\\pi (n-m) \\Delta{x}} e^{i 2\\pi (n-m) k \\Delta{x}} \\right|_{-\\frac{1}{2\\Delta{x}}}^{\\frac{1}{2\\Delta{x}}} \\right]\\\\ &amp;= \\Delta{x} \\sum_{m=-\\infty}^{\\infty} f[m] \\left[\\frac{1}{\\Delta{x}} \\frac{\\sin(\\pi(n-m))}{\\pi (n-m)}\\right]\\\\ &amp;= f[n] \\end{split} \\end{equation} Δx∫1/Δx​f^​dtft​(k)ei2πnkΔxdk​=Δx∫−2Δx1​2Δx1​​f^​dtft​(k)ei2πnkΔxdk=Δx∫−2Δx1​2Δx1​​f^​S​(k)ei2πnkΔxdk=Δx∫−2Δx1​2Δx1​​[m=−∞∑∞​f[m]e−i2πmkΔx]ei2πnkΔxdk=Δxm=−∞∑∞​f[m][∫−2Δx1​2Δx1​​ei2π(n−m)kΔxdk]=Δxm=−∞∑∞​f[m][i2π(n−m)Δx1​ei2π(n−m)kΔx∣∣​−2Δx1​2Δx1​​]=Δxm=−∞∑∞​f[m][Δx1​π(n−m)sin(π(n−m))​]=f[n]​​​ note that sinc(x)=sin⁡(πx)πx\\mathrm{sinc}(x)=\\frac{\\sin(\\pi x)}{\\pi x}sinc(x)=πxsin(πx)​ only has a nonzero value 1 at x=0x=0x=0 and 0 for other integers. It is clear that f^S(k)\\hat{f}_S(k)f^​S​(k) is a periodic function with period 1/Δx1/\\Delta{x}1/Δx, we can see that from the convolution theorem of the Fourier transform: f^S(k)=F[f(x)SΔx(x)]=F[f(x)]∗F[SΔx(x)]=f^(k)∗S^Δx(k)=f^(k)∗1Δx∑n=−∞∞δ(k−n/Δx)=1Δx∑n=−∞∞f^(k)∗δ(k−n/Δx)=1Δx∑n=−∞∞∫−∞∞f^(τ)δ(k−τ−n/Δx)dτ=1Δx∑n=−∞∞∫−∞∞f^(τ)δ(−(τ−k+n/Δx))dτ=1Δx∑n=−∞∞∫−∞∞f^(τ)δ(τ−(k−n/Δx))dτ=1Δx∑n=−∞∞f^(k−n/Δx)\\begin{equation} \\begin{split} \\hat{f}_S(k) &amp;= \\mathcal{F}\\left[f(x)S_{\\Delta{x}}(x)\\right]\\\\ &amp;= \\mathcal{F}\\left[f(x)\\right] \\ast \\mathcal{F}\\left[S_{\\Delta{x}}(x)\\right]\\\\ &amp;= \\hat{f}(k) \\ast \\hat{S}_{\\Delta{x}}(k)\\\\ &amp;= \\hat{f}(k) \\ast \\frac{1}{\\Delta{x}} \\sum_{n=-\\infty}^{\\infty} \\delta(k - n/\\Delta{x})\\\\ &amp;= \\frac{1}{\\Delta{x}} \\sum_{n=-\\infty}^{\\infty} \\hat{f}(k) \\ast \\delta(k - n/\\Delta{x})\\\\ &amp;= \\frac{1}{\\Delta{x}} \\sum_{n=-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} \\hat{f}(\\tau) \\delta(k - \\tau - n/\\Delta{x}) d\\tau\\\\ &amp;= \\frac{1}{\\Delta{x}} \\sum_{n=-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} \\hat{f}(\\tau) \\delta(-(\\tau - k + n/\\Delta{x})) d\\tau\\\\ &amp;= \\frac{1}{\\Delta{x}} \\sum_{n=-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} \\hat{f}(\\tau) \\delta(\\tau - (k - n/\\Delta{x})) d\\tau\\\\ &amp;= \\frac{1}{\\Delta{x}} \\sum_{n=-\\infty}^{\\infty} \\hat{f}(k-n/\\Delta{x})\\\\ \\end{split} \\end{equation} f^​S​(k)​=F[f(x)SΔx​(x)]=F[f(x)]∗F[SΔx​(x)]=f^​(k)∗S^Δx​(k)=f^​(k)∗Δx1​n=−∞∑∞​δ(k−n/Δx)=Δx1​n=−∞∑∞​f^​(k)∗δ(k−n/Δx)=Δx1​n=−∞∑∞​∫−∞∞​f^​(τ)δ(k−τ−n/Δx)dτ=Δx1​n=−∞∑∞​∫−∞∞​f^​(τ)δ(−(τ−k+n/Δx))dτ=Δx1​n=−∞∑∞​∫−∞∞​f^​(τ)δ(τ−(k−n/Δx))dτ=Δx1​n=−∞∑∞​f^​(k−n/Δx)​​​ so the discrete-time Fourier transform of f[n]f[n]f[n] is a summation of shifted replicates of f^(k)\\hat{f}(k)f^​(k) in terms of a frequency period 1/Δx1/\\Delta{x}1/Δx. Properties Common DTFT Pairs DTFT of Periodic Sequences Considering f[n]f[n]f[n] is an NNN-periodic sequence, we can write f[n]f[n]f[n] as fS(x)=fT(x)SΔx(x)f_S(x) = f_T(x) S_{\\Delta{x}}(x)fS​(x)=fT​(x)SΔx​(x), where NΔx=TN\\Delta{x}=TNΔx=T. fS(x)f_S(x)fS​(x) is also a periodic function with a period TTT: fS(x+T)=fT(x+T)SΔx(x+T)=fT(x)SΔx(x+NΔx)=fT(x)SΔx(x)=fS(x)f[n+N]=fS((n+N)Δx)=fS(nΔx+T)=fS(nΔx)=f[n]\\begin{equation} \\begin{split} f_{S}(x+T) &amp;= f_{T}(x+T) S_{\\Delta{x}}(x+T)\\\\ &amp;= f_T(x) S_{\\Delta{x}}(x+N\\Delta{x})\\\\ &amp;= f_T(x) S_{\\Delta{x}}(x)\\\\ &amp;= f_S(x)\\\\ f[n+N] &amp;= f_S((n+N)\\Delta{x})\\\\ &amp;= f_S(n\\Delta{x}+T)\\\\ &amp;= f_S(n\\Delta{x})\\\\ &amp;= f[n] \\end{split} \\end{equation} fS​(x+T)f[n+N]​=fT​(x+T)SΔx​(x+T)=fT​(x)SΔx​(x+NΔx)=fT​(x)SΔx​(x)=fS​(x)=fS​((n+N)Δx)=fS​(nΔx+T)=fS​(nΔx)=f[n]​​​ For delta function, we also know that: ∫abf(x)δ(x−x0)dx==∫−∞∞f(x)(u(x−a)−u(x−b))δ(x−x0)dx=f(x0)(u(x0−a)−u(x0−b))={f(x0)x0∈(a,b)f(x0)/2x0=a,b0else\\begin{equation} \\begin{split} \\int_{a}^{b} f(x) \\delta(x-x_0) dx &amp;=\\\\ &amp;= \\int_{-\\infty}^{\\infty} f(x) \\left(u(x-a) - u(x-b)\\right) \\delta(x-x_0) dx\\\\ &amp;= f(x_0)\\left(u(x_0-a) - u(x_0-b)\\right)\\\\ &amp;= \\begin{cases} f(x_0) &amp; x_0 \\in (a, b)\\\\ f(x_0)/2 &amp; x_0=a,b\\\\ 0 &amp; \\text{else} \\end{cases} \\end{split} \\end{equation} ∫ab​f(x)δ(x−x0​)dx​==∫−∞∞​f(x)(u(x−a)−u(x−b))δ(x−x0​)dx=f(x0​)(u(x0​−a)−u(x0​−b))=⎩⎨⎧​f(x0​)f(x0​)/20​x0​∈(a,b)x0​=a,belse​​​​ where u(x)u(x)u(x) is the unit step function. Remember that Fourier transform of a periodic function is a summation of delta functions weighted by the Fourier coefficients: cm=1T∫TfS(x)e−i2πmx/Tdx=1T∫−−Δx2T−−Δx2fT(x)SΔx(x)e−i2πmx/Tdx=1T∫−−Δx2T−−Δx2fT(x)∑n=−∞∞δ(x−nΔx)e−i2πmx/Tdx=1T∫−−Δx2T−−Δx2fT(x)∑n=0N−1δ(x−nΔx)e−i2πmx/Tdx=1T∑n=0N−1∫−−Δx2T−−Δx2fT(x)e−i2πmx/Tδ(x−nΔx)dx=1T∑n=0N−1fT(nΔx)e−i2πnmΔx/T=1T∑n=0N−1fT(nΔx)e−i2πnmΔx/(NΔx))=1T∑n=0N−1fT(nΔx)e−i2πnm/N)=1T∑n=0N−1f[n]e−i2πnm/Nf^dtft(k)=F(fS(x))=∑m=−∞∞cmδ(k−m/T)=1T∑m=−∞∞(∑n=0N−1f[n]e−i2πnm/N)δ(k−m/T)=1T∑m=−∞∞f^[m]δ(k−m/T)=1NΔx∑m=−∞∞f^[m]δ(k−mNΔx)\\begin{equation} \\begin{split} c_m &amp;= \\frac{1}{T} \\int_{T} f_{S}(x) e^{-i 2\\pi m x / T} dx\\\\ &amp;= \\frac{1}{T} \\int_{-\\frac{-\\Delta{x}}{2}}^{T-\\frac{-\\Delta{x}}{2}} f_T(x) S_{\\Delta{x}}(x) e^{-i 2\\pi m x / T} dx\\\\ &amp;= \\frac{1}{T} \\int_{-\\frac{-\\Delta{x}}{2}}^{T-\\frac{-\\Delta{x}}{2}} f_T(x) \\sum_{n=-\\infty}^{\\infty} \\delta(x-n\\Delta{x}) e^{-i 2\\pi m x / T} dx\\\\ &amp;= \\frac{1}{T} \\int_{-\\frac{-\\Delta{x}}{2}}^{T-\\frac{-\\Delta{x}}{2}} f_T(x) \\sum_{n=0}^{N-1} \\delta(x-n\\Delta{x}) e^{-i 2\\pi m x / T} dx\\\\ &amp;= \\frac{1}{T} \\sum_{n=0}^{N-1} \\int_{-\\frac{-\\Delta{x}}{2}}^{T-\\frac{-\\Delta{x}}{2}} f_T(x) e^{-i 2\\pi m x / T} \\delta(x-n\\Delta{x}) dx\\\\ &amp;= \\frac{1}{T} \\sum_{n=0}^{N-1} f_T(n\\Delta{x}) e^{-i 2\\pi n m \\Delta{x} / T}\\\\ &amp;= \\frac{1}{T} \\sum_{n=0}^{N-1} f_T(n\\Delta{x}) e^{-i 2\\pi n m \\Delta{x} / (N\\Delta{x}))}\\\\ &amp;= \\frac{1}{T} \\sum_{n=0}^{N-1} f_T(n\\Delta{x}) e^{-i 2\\pi n m / N)}\\\\ &amp;= \\frac{1}{T} \\sum_{n=0}^{N-1} f[n] e^{-i 2\\pi n m / N}\\\\ \\hat{f}_{dtft}(k) &amp;= \\mathcal{F}(f_S(x))\\\\ &amp;= \\sum_{m=-\\infty}^{\\infty} c_m \\delta(k - m/T)\\\\ &amp;= \\frac{1}{T} \\sum_{m=-\\infty}^{\\infty} \\left(\\sum_{n=0}^{N-1} f[n] e^{-i 2\\pi n m / N}\\right) \\delta(k - m/T)\\\\ &amp;= \\frac{1}{T} \\sum_{m=-\\infty}^{\\infty} \\hat{f}[m] \\delta(k - m/T)\\\\ &amp;= \\frac{1}{N\\Delta{x}} \\sum_{m=-\\infty}^{\\infty} \\hat{f}[m] \\delta(k - \\frac{m}{N\\Delta{x}}) \\end{split} \\end{equation} cm​f^​dtft​(k)​=T1​∫T​fS​(x)e−i2πmx/Tdx=T1​∫−2−Δx​T−2−Δx​​fT​(x)SΔx​(x)e−i2πmx/Tdx=T1​∫−2−Δx​T−2−Δx​​fT​(x)n=−∞∑∞​δ(x−nΔx)e−i2πmx/Tdx=T1​∫−2−Δx​T−2−Δx​​fT​(x)n=0∑N−1​δ(x−nΔx)e−i2πmx/Tdx=T1​n=0∑N−1​∫−2−Δx​T−2−Δx​​fT​(x)e−i2πmx/Tδ(x−nΔx)dx=T1​n=0∑N−1​fT​(nΔx)e−i2πnmΔx/T=T1​n=0∑N−1​fT​(nΔx)e−i2πnmΔx/(NΔx))=T1​n=0∑N−1​fT​(nΔx)e−i2πnm/N)=T1​n=0∑N−1​f[n]e−i2πnm/N=F(fS​(x))=m=−∞∑∞​cm​δ(k−m/T)=T1​m=−∞∑∞​(n=0∑N−1​f[n]e−i2πnm/N)δ(k−m/T)=T1​m=−∞∑∞​f^​[m]δ(k−m/T)=NΔx1​m=−∞∑∞​f^​[m]δ(k−NΔxm​)​​​ Note we use a trick with a range between −−Δx2-\\frac{-\\Delta{x}}{2}−2−Δx​ and T−−Δx2T-\\frac{-\\Delta{x}}{2}T−2−Δx​to make sure only N points are available for δ(x−mΔx)\\delta(x-m\\Delta{x})δ(x−mΔx) in the integral equation, and introduce a new symbol f^[m]=∑n=0N−1f[n]e−i2πnm/N\\hat{f}[m] = \\sum_{n=0}^{N-1} f[n] e^{-i 2\\pi n m / N}f^​[m]=∑n=0N−1​f[n]e−i2πnm/N, which is referred to as the discrete Fourier transform (explained latter). So the discrete-time Fourier transform of a periodic sequence can be seen as the Fourier transform of a periodic function with exact sampling interval (Δx=TN\\Delta x=\\frac{T}{N}Δx=NT​). The result is a summation of delta functions, where the weights are the discrete Fourier transform values. f^dtft(k)\\hat{f}_{dtft}(k)f^​dtft​(k) converges to zero everywhere except at integer multiples of 1T\\frac{1}{T}T1​, known as harmonic frequencies. And the period 1/Δx1/\\Delta{x}1/Δx still holds for f^dtft(k)\\hat{f}_{dtft}(k)f^​dtft​(k): f^dtft(k+1/Δx)=1NΔx∑m=−∞∞f^[m]δ(k+1/Δx−m/(NΔx))=1NΔx∑m=−∞∞f^[m]δ(k−(m−N)/(NΔx)), n=m−N=1NΔx∑n=−∞∞f^[n+N]δ(k−n/(NΔx))=1NΔx∑n=−∞∞f^[n]δ(k−n/(NΔx))=f^dtft(k)\\begin{equation} \\begin{split} \\hat{f}_{dtft}(k + 1/\\Delta{x}) &amp;= \\frac{1}{N\\Delta{x}} \\sum_{m=-\\infty}^{\\infty} \\hat{f}[m] \\delta(k + 1/\\Delta{x} - m/(N\\Delta{x}))\\\\ &amp;= \\frac{1}{N\\Delta{x}} \\sum_{m=-\\infty}^{\\infty} \\hat{f}[m] \\delta(k - (m-N)/(N\\Delta{x})),\\ n=m-N\\\\ &amp;= \\frac{1}{N\\Delta{x}} \\sum_{n=-\\infty}^{\\infty} \\hat{f}[n+N] \\delta(k - n/(N\\Delta{x}))\\\\ &amp;= \\frac{1}{N\\Delta{x}} \\sum_{n=-\\infty}^{\\infty} \\hat{f}[n] \\delta(k - n/(N\\Delta{x}))\\\\ &amp;= \\hat{f}_{dtft}(k) \\end{split} \\end{equation} f^​dtft​(k+1/Δx)​=NΔx1​m=−∞∑∞​f^​[m]δ(k+1/Δx−m/(NΔx))=NΔx1​m=−∞∑∞​f^​[m]δ(k−(m−N)/(NΔx)), n=m−N=NΔx1​n=−∞∑∞​f^​[n+N]δ(k−n/(NΔx))=NΔx1​n=−∞∑∞​f^​[n]δ(k−n/(NΔx))=f^​dtft​(k)​​​ Substituting f^dtft(k)\\hat{f}_{dtft}(k)f^​dtft​(k) into the inverse discrete-time Fourier transform formula (note that ks=1Δxk_s=\\frac{1}{\\Delta{x}}ks​=Δx1​ is the sampling frequency), we can verify that: 1ks∫ksf^dtft(k)ei2πnkksdk=1ks∫−ks2Nks−ks2NksN∑m=−∞∞f^dft[m]δ(k−mks/N)ei2πnkksdk=1N∫−ks2Nks−ks2N∑m=0N−1f^dft[m]δ(k−mks/N)ei2πnkksdk=1N∑m=0N−1∫−ks2Nks−ks2Nf^dft[m]ei2πnkksδ(k−mks/N)dk=1N∑m=0N−1f^dft[m]ei2πnmN=1N∑m=0N−1∑l=0N−1f[l]e−i2πmlNei2πnmN=1N∑l=0N−1f[l]∑m=0N−1ei2π(n−l)mN=1N∑l=0N−1f[l][eiπN−1N(n−l)sin⁡(π(n−l))sin⁡(πn−lN)]=1N∑l=0N−1f[l]g[l]=f[n]\\begin{equation} \\begin{split} \\frac{1}{k_s} \\int_{k_s} \\hat{f}_{dtft}(k) e^{i 2\\pi n \\frac{k}{k_s}} dk &amp;= \\frac{1}{k_s} \\int_{-\\frac{k_s}{2N}}^{k_s - \\frac{k_s}{2N}} \\frac{k_s}{N} \\sum_{m=-\\infty}^{\\infty} \\hat{f}_{dft}[m] \\delta(k - m k_s / N) e^{i 2\\pi n \\frac{k}{k_s}} dk\\\\ &amp;= \\frac{1}{N} \\int_{-\\frac{k_s}{2N}}^{k_s - \\frac{k_s}{2N}} \\sum_{m=0}^{N-1} \\hat{f}_{dft}[m] \\delta(k - m k_s / N) e^{i 2\\pi n \\frac{k}{k_s}} dk\\\\ &amp;= \\frac{1}{N} \\sum_{m=0}^{N-1} \\int_{-\\frac{k_s}{2N}}^{k_s - \\frac{k_s}{2N}} \\hat{f}_{dft}[m] e^{i 2\\pi n \\frac{k}{k_s}} \\delta(k - m k_s / N) dk\\\\ &amp;= \\frac{1}{N} \\sum_{m=0}^{N-1} \\hat{f}_{dft}[m] e^{i 2\\pi n \\frac{m}{N}} \\\\ &amp;= \\frac{1}{N} \\sum_{m=0}^{N-1} \\sum_{l=0}^{N-1} f[l] e^{-i 2\\pi m \\frac{l}{N}} e^{i 2\\pi n \\frac{m}{N}}\\\\ &amp;= \\frac{1}{N} \\sum_{l=0}^{N-1} f[l] \\sum_{m=0}^{N-1} e^{i 2\\pi (n-l) \\frac{m}{N}}\\\\ &amp;= \\frac{1}{N} \\sum_{l=0}^{N-1} f[l] \\left[e^{i \\pi \\frac{N-1}{N} (n-l)} \\frac{\\sin(\\pi (n-l))}{\\sin(\\pi \\frac{n-l}{N})}\\right]\\\\ &amp;= \\frac{1}{N} \\sum_{l=0}^{N-1} f[l] g[l]\\\\ &amp;= f[n] \\end{split} \\end{equation} ks​1​∫ks​​f^​dtft​(k)ei2πnks​k​dk​=ks​1​∫−2Nks​​ks​−2Nks​​​Nks​​m=−∞∑∞​f^​dft​[m]δ(k−mks​/N)ei2πnks​k​dk=N1​∫−2Nks​​ks​−2Nks​​​m=0∑N−1​f^​dft​[m]δ(k−mks​/N)ei2πnks​k​dk=N1​m=0∑N−1​∫−2Nks​​ks​−2Nks​​​f^​dft​[m]ei2πnks​k​δ(k−mks​/N)dk=N1​m=0∑N−1​f^​dft​[m]ei2πnNm​=N1​m=0∑N−1​l=0∑N−1​f[l]e−i2πmNl​ei2πnNm​=N1​l=0∑N−1​f[l]m=0∑N−1​ei2π(n−l)Nm​=N1​l=0∑N−1​f[l][eiπNN−1​(n−l)sin(πNn−l​)sin(π(n−l))​]=N1​l=0∑N−1​f[l]g[l]=f[n]​​​ where g[l]=eiπN−1N(n−l)sin⁡(π(n−l))sin⁡(πn−lN)g[l]=e^{i \\pi \\frac{N-1}{N} (n-l)} \\frac{\\sin(\\pi (n-l))}{\\sin(\\pi \\frac{n-l}{N})}g[l]=eiπNN−1​(n−l)sin(πNn−l​)sin(π(n−l))​ only has a value NNN when l=nl=nl=n otherwise 0. Discrete-Time Fourier Series Definition For a N-periodic sequence f[n]f[n]f[n], it has the following series representation: cm=∑n=0N−1f[n]e−i2πmnNf[n]=1N∑m=0N−1cmei2πnmN\\begin{equation} \\begin{split} c_m &amp;= \\sum_{n=0}^{N-1} f[n] e^{-i2\\pi\\frac{mn}{N}}\\\\ f[n] &amp;= \\frac{1}{N} \\sum_{m=0}^{N-1} c_m e^{i2\\pi \\frac{nm}{N}} \\end{split} \\end{equation} cm​f[n]​=n=0∑N−1​f[n]e−i2πNmn​=N1​m=0∑N−1​cm​ei2πNnm​​​​ Here cmc_mcm​’s are the DTFS coefficients and they are periodic with period NNN. The series representation of f[n]f[n]f[n] is called the inverse DTFS. Connection to the DTFT As we proved in DTFT of Periodic Sequences, DTFS is actually a discretized version of DTFT of periodic sequences. Given a signal f(x)f(x)f(x) with period TTT and its N-periodic sequence f[n]f[n]f[n], we first compute its DTFS coefficients cmc_mcm​, then the DTFT f^dtft(k)\\hat{f}_{dtft}(k)f^​dtft​(k) of f[n]f[n]f[n] is represented by: f^dtft(k)=1T∑m=−∞∞cmδ(k−m/T)\\begin{equation} \\begin{split} \\hat{f}_{dtft}(k) &amp;= \\frac{1}{T} \\sum_{m=-\\infty}^{\\infty} c_m \\delta(k - m/T)\\\\ \\end{split} \\end{equation} f^​dtft​(k)​=T1​m=−∞∑∞​cm​δ(k−m/T)​​​ The original sequence f[n]f[n]f[n] can be recovered from the inverse DTFT, which is also the inverse DTFS. Discrete Fourier Transform TL;DR I found that Professor Jeffery Fessler’s note gives a clear picture of relations of different transforms. In general, the DFT is a simpified way to transform sampled time-domain sequences to sampled frequency-domain sequences. Definition For a sequence of NNN complex numbers {f[n]}n=0N−1{\\{f[n]\\}}_{n=0}^{N-1}{f[n]}n=0N−1​, the discrete Fourier Transform transforms the sequence into another sequence of complex numbers {f^[m]}m=0N−1{\\{\\hat{f}[m]\\}}_{m=0}^{N-1}{f^​[m]}m=0N−1​: f^[m]=∑n=0N−1f[n]e−i2πknN\\begin{equation} \\hat{f}[m] = \\sum_{n=0}^{N-1} f[n] e^{-i 2\\pi k \\frac{n}{N}} \\end{equation} f^​[m]=n=0∑N−1​f[n]e−i2πkNn​​​ The inverse discrete Fourier transform is given by: f[n]=1N∑m=0N−1f^[m]ei2πnmN\\begin{equation} f[n] = \\frac{1}{N} \\sum_{m=0}^{N-1} \\hat{f}[m] e^{i 2\\pi n \\frac{m}{N}} \\end{equation} f[n]=N1​m=0∑N−1​f^​[m]ei2πnNm​​​ Connection to the DTFT and DTFS The DFT can be motivated by the sampling of the DTFT. Consider sampling the DTFT with Δk\\Delta{k}Δk sampling interval such that one period was sampled with exactly NNN points (NΔk=1/ΔxN \\Delta{k} = 1/\\Delta{x}NΔk=1/Δx): f^dtft[m]=f^dtft(mΔk)=∑n=−∞∞f[n]e−i2πnΔxmΔk=∑n=−∞∞f[n]e−i2πnmN\\begin{equation} \\begin{split} \\hat{f}_{dtft}[m] &amp;= \\hat{f}_{dtft}(m\\Delta{k})\\\\ &amp;= \\sum_{n=-\\infty}^{\\infty} f[n] e^{-i2\\pi n\\Delta{x} m\\Delta{k}}\\\\ &amp;= \\sum_{n=-\\infty}^{\\infty} f[n] e^{-i2\\pi \\frac{nm}{N}} \\end{split} \\end{equation} f^​dtft​[m]​=f^​dtft​(mΔk)=n=−∞∑∞​f[n]e−i2πnΔxmΔk=n=−∞∑∞​f[n]e−i2πNnm​​​​ Let’s break f[n]f[n]f[n] into NNN-length segments such as f[0]...f[N−1]f[0] ... f[N-1]f[0]...f[N−1], f[N]...f[2N−1]f[N] ... f[2N-1]f[N]...f[2N−1], let n=l−jNn=l-jNn=l−jN where l∈[0,N−1]l \\in [0, N-1]l∈[0,N−1] and j∈[−∞,∞]j \\in [-\\infty, \\infty]j∈[−∞,∞], then f^dtft[m]\\hat{f}_{dtft}[m]f^​dtft​[m] can be redefined with NNN-point periodic superposition fps[l]=∑j=−∞∞f[l−jN]f_{ps}[l] = \\sum_{j=-\\infty}^{\\infty} f[l-jN]fps​[l]=∑j=−∞∞​f[l−jN]: f^dtft[m]=∑n=−∞∞f[n]e−i2πnmN=∑j=−∞∞∑l=0N−1f[l−jN]e−i2π(l−jN)mN=∑j=−∞∞∑l=0N−1f[l−jN]e−i2πlmN=∑l=0N−1(∑j=−∞∞f[l−jN])e−i2πlmN=∑l=0N−1fps[l]e−i2πlmN\\begin{equation} \\begin{split} \\hat{f}_{dtft}[m] &amp;= \\sum_{n=-\\infty}^{\\infty} f[n] e^{-i2\\pi \\frac{nm}{N}}\\\\ &amp;= \\sum_{j=-\\infty}^{\\infty} \\sum_{l=0}^{N-1} f[l-jN] e^{-i2\\pi (l-jN) \\frac{m}{N}}\\\\ &amp;= \\sum_{j=-\\infty}^{\\infty} \\sum_{l=0}^{N-1} f[l-jN] e^{-i2\\pi l \\frac{m}{N}}\\\\ &amp;= \\sum_{l=0}^{N-1} \\left(\\sum_{j=-\\infty}^{\\infty} f[l-jN]\\right) e^{-i2\\pi l \\frac{m}{N}}\\\\ &amp;= \\sum_{l=0}^{N-1} f_{ps}[l] e^{-i2\\pi l \\frac{m}{N}} \\end{split} \\end{equation} f^​dtft​[m]​=n=−∞∑∞​f[n]e−i2πNnm​=j=−∞∑∞​l=0∑N−1​f[l−jN]e−i2π(l−jN)Nm​=j=−∞∑∞​l=0∑N−1​f[l−jN]e−i2πlNm​=l=0∑N−1​(j=−∞∑∞​f[l−jN])e−i2πlNm​=l=0∑N−1​fps​[l]e−i2πlNm​​​​ Obviously, fps[l]f_{ps}[l]fps​[l] is a NNN-periodic sequence. Since fps[l]f_{ps}[l]fps​[l] is NNN-periodic, it can be represented with the DTFS: cm=∑l=0N−1fps[l]e−i2πlmNfps[l]=1N∑m=0N−1cmei2πmlN\\begin{equation} \\begin{split} c_m &amp;= \\sum_{l=0}^{N-1} f_{ps}[l] e^{-i 2\\pi \\frac{lm}{N}}\\\\ f_{ps}[l] &amp;= \\frac{1}{N} \\sum_{m=0}^{N-1} c_m e^{i 2\\pi \\frac{ml}{N}} \\end{split} \\end{equation} cm​fps​[l]​=l=0∑N−1​fps​[l]e−i2πNlm​=N1​m=0∑N−1​cm​ei2πNml​​​​ Comparing the DTFS coefficients and the above DTFT samples, we see that: cm=f^dtft[m]\\begin{equation} \\begin{split} c_m &amp;= \\hat{f}_{dtft}[m] \\end{split} \\end{equation} cm​​=f^​dtft​[m]​​​ Thus, we can recover the periodic sequence fps[l]f_{ps}[l]fps​[l] from those DTFT samples with the inverse DTFS: fps[l]=1N∑m=0N−1f^dtft[m]ei2πmlN\\begin{equation} \\begin{split} f_{ps}[l] &amp;= \\frac{1}{N} \\sum_{m=0}^{N-1} \\hat{f}_{dtft}[m] e^{i 2\\pi \\frac{ml}{N}} \\end{split} \\end{equation} fps​[l]​=N1​m=0∑N−1​f^​dtft​[m]ei2πNml​​​​ However, this recovery does not ensure that we can recover the original sequence f[n]f[n]f[n] with those DTFT samples. fps[l]f_{ps}[l]fps​[l] is a sum of shifted replicates of f[n]f[n]f[n]. Thus there is no perfect reconstruction for non time-limited sequences since time-domain replicates overlap and aliasing occurs. There is a special case where time-domain replicates do not overlap. Considering a time-limited sequence f[n]f[n]f[n] with duration LLL which it has nonzero values only in the interval 0,...,L−10,..., L-10,...,L−1. If N≥LN \\ge LN≥L, then there is no overlap in the replicates. The original sequence f[n]f[n]f[n] can be recovered from fps[l]f_{ps}[l]fps​[l]: f[n]={fps[n]n∈[0,L−1)0otherwise\\begin{equation} f[n] = \\begin{cases} f_{ps}[n] &amp; n \\in [0, L-1)\\\\ 0 &amp; \\text{otherwise} \\end{cases} \\end{equation} f[n]={fps​[n]0​n∈[0,L−1)otherwise​​​ In fact, if f[n]f[n]f[n] is time-limited, the DTFT samples simplifies to a limited summation: f^[m]=f^dtft[m]=∑n=0L−1f[n]e−i2πnmM=∑n=0N−1f[n]e−i2πnmM\\begin{equation} \\begin{split} \\hat{f}[m] &amp;= \\hat{f}_{dtft}[m]\\\\ &amp;= \\sum_{n=0}^{L-1} f[n] e^{-i2\\pi n \\frac{m}{M}}\\\\ &amp;= \\sum_{n=0}^{N-1} f[n] e^{-i2\\pi n \\frac{m}{M}} \\end{split} \\end{equation} f^​[m]​=f^​dtft​[m]=n=0∑L−1​f[n]e−i2πnMm​=n=0∑N−1​f[n]e−i2πnMm​​​​ where f[n]=0f[n]=0f[n]=0 for n≥Ln \\ge Ln≥L and the above formula is called DFT and f[n]f[n]f[n] can be recovered from the inverse DFT formula: f[n]={1N∑m=0N−1f^[m]ei2πmnNn=0,...,N−10otherwise\\begin{equation} \\begin{split} f[n] = \\begin{cases} \\frac{1}{N} \\sum_{m=0}^{N-1} \\hat{f}[m] e^{i 2\\pi \\frac{mn}{N}} &amp; n=0, ..., N-1\\\\ 0 &amp; \\text{otherwise} \\end{cases} \\end{split} \\end{equation} f[n]={N1​∑m=0N−1​f^​[m]ei2πNmn​0​n=0,...,N−1otherwise​​​​ Properties TODO Common DFT Pairs TODO Non-uniform Fast Fourier Transform TODO","categories":[{"name":"signal processing","slug":"signal-processing","permalink":"https://mrswolf.github.io/categories/signal-processing/"}],"tags":[{"name":"machine learning","slug":"machine-learning","permalink":"https://mrswolf.github.io/tags/machine-learning/"},{"name":"signal processing","slug":"signal-processing","permalink":"https://mrswolf.github.io/tags/signal-processing/"}]},{"title":"Makefile使用规则","slug":"learn-makefile","date":"2022-06-06T16:00:00.000Z","updated":"2023-09-12T15:45:36.009Z","comments":false,"path":"learn-makefile/","link":"learn-makefile","permalink":"https://mrswolf.github.io/learn-makefile/","excerpt":"出于工作需要，我要开始系统学习c++了。目前我的主力台式机是Linux系统，最常用的编辑器是VS Code，所以想要得到一个比较完整的C/C++工程方案，似乎学习Makefile的相关规则是必不可少的。","text":"出于工作需要，我要开始系统学习c++了。目前我的主力台式机是Linux系统，最常用的编辑器是VS Code，所以想要得到一个比较完整的C/C++工程方案，似乎学习Makefile的相关规则是必不可少的。本文的内容主要源于makefile tutorial by example（感谢作者大大）。 为什么要使用makefile 对于大型C/C++项目而言，完整的程序功能往往是很多子模块组合得到的。试想开发者改变或添加了几个新的功能，如果手动重新编译整个项目（比如在shell里输入一长串的g++命令以及一堆的lib），不仅费时费力、还容易出错。make工具正是用来处理这种某些模型源代码文件需要重新编译的情况，它所依赖的配置文件就是Makefile。简而言之，make通过预先配置好的Makefile，按一定的规则负责处理C/C++项目的编译，从而得到最终的可执行程序。目前，大多数开源项目都会提供Makefile，开发者可以很方便的调用make命令编译整个开源项目。 当然，make并不是唯一用于处理C/C++项目编译的解决方案，一些替代的解决方案包括CMake、Bazel等等。在Windows平台上，Visual Studio也有其内置的build工具。 本篇所使用的make是指GNU make，在Linux和MacOS上是默认安装的make实现。 基本语法 现在我们来创建一个最简单的Makefile。首先在任何目录下创建一个名为Makefile的文件（注意文件名就是Makefile，大些的M，也没有类似.txt的后缀），然后在其中输入这些文本： hello: echo &quot;hello make&quot; 注意，Makefile的缩进只能是Tab，不可以用空格，不然make会报缺失分隔符错误，要小心你的编辑器是不是自动将Tab缩进转换成了4个空格。接下来在该目录下的terminal里运行make命令，输出如下： $ makeecho &quot;hello make&quot;hello make 表明需要执行echo &quot;hello make&quot;命令，执行结果是hello make。 Makefile文件是由一系列rule组成的，一个rule定义如下： targets: prerequisties command command command targets是本次操作希望达成的目标，它实质是一系列文件名，相互之间用空格隔开，通常只有一个文件名，比如例子中的hello文件，而每一行的command则是用来达成targets的手段，通过执行command，我们希望在所有命令执行完成后能够产生targets所规定的文件。prerequisties同样也是一系列文件名，相互之间用空格隔开，它表示执行命令前这些文件应当已经存在了，有些类似程序的依赖概念。一个rule可以简单理解为在prerequisties存在的情况下，运行一系列command，希望最终能够得到targets中的文件。 基本执行关系 blah: blah.o gcc blah.o -o blah # Runs thirdblah.o: blah.c gcc -c blah.c -o blah.o # Runs secondblah.c: echo &quot;int main() &#123; return 0; &#125;&quot; &gt; blah.c # Runs firstclean: rm -f blah blah.o blah.c 上面是一个有四个rule的Makefile文件，不指定target情况下，运行make会默认以第一个rule中的targets为目标，从而进行如下操作： 第一个rule想产生名为blah的文件，但是需要文件blah.o，所以跳转到第二个rule 第二个rule又需要文件blah.c，所以跳转到第三个rule 第三个rule为了产生blah.c，执行命令产生一个blah.c文件，接着返回第二个rule 现在有了blah.c，第二个rule编译生成blah.o二进制文件，接着返回第一个rule 现在有了blah.o，第一个rule链接生成blah可执行文件，任务完成 现在我们的目录下已经有blah文件了，所以再运行一次make不会产生任何效果。注意到blah目标的产生与clean无关，所以clean所代表的rule不会执行，不过可以显式运行make clean命令来调用该rule规定的清理文件操作。 最后需要注意的是，make似乎只会检查一次targets和prerequisties是否存在，至于调用之后是不是成功产生了目标文件，make是不会去检查的，而是默认成功了。 TODO","categories":[{"name":"program","slug":"program","permalink":"https://mrswolf.github.io/categories/program/"}],"tags":[{"name":"c++","slug":"c","permalink":"https://mrswolf.github.io/tags/c/"},{"name":"linux","slug":"linux","permalink":"https://mrswolf.github.io/tags/linux/"}]},{"title":"为什么要少用逆矩阵","slug":"why-not-invert-that-matrix","date":"2022-05-29T16:00:00.000Z","updated":"2022-06-01T16:00:00.000Z","comments":false,"path":"why-not-invert-that-matrix/","link":"why-not-invert-that-matrix","permalink":"https://mrswolf.github.io/why-not-invert-that-matrix/","excerpt":"我经常会在线性代数教材以及论坛讨论中看到不建议使用逆矩阵A−1\\mathbf{A}^{-1}A−1来求解线性方程Ax=b\\mathbf{A}\\mathbf{x}=\\mathbf{b}Ax=b，尽管我一直遵循这样的原则（实践中逆矩阵确实不够稳定），但仍然不明白不使用逆矩阵的理由。本文总结了我在网上看到的一些关于逆矩阵的讨论，希望能解释为什么要少用逆矩阵来求解线性方程。","text":"我经常会在线性代数教材以及论坛讨论中看到不建议使用逆矩阵A−1\\mathbf{A}^{-1}A−1来求解线性方程Ax=b\\mathbf{A}\\mathbf{x}=\\mathbf{b}Ax=b，尽管我一直遵循这样的原则（实践中逆矩阵确实不够稳定），但仍然不明白不使用逆矩阵的理由。本文总结了我在网上看到的一些关于逆矩阵的讨论，希望能解释为什么要少用逆矩阵来求解线性方程。 数学原理 解线性方程组（逆矩阵） 考虑求解线性方程组中的x∈Rn\\mathbf{x} \\in \\mathbb{R}^nx∈Rn： Ax=b\\begin{equation} \\mathbf{A} \\mathbf{x} = \\mathbf{b} \\end{equation} Ax=b​​ 其中A∈Rn×n,b∈Rn\\mathbf{A} \\in \\mathbb{R}^{n \\times n},\\mathbf{b} \\in \\mathbb{R}^nA∈Rn×n,b∈Rn。 显然一种简单直观的求解方法是计算A\\mathbf{A}A的逆矩阵A−1\\mathbf{A}^{-1}A−1： x=A−1b\\begin{equation} \\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{b} \\end{equation} x=A−1b​​ 求解逆矩阵通常需要2n32n^32n3次浮点数操作（floating-point operations，flops），此外计算A−1b\\mathbf{A}^{-1}\\mathbf{b}A−1b需要2n22n^22n2 flops，总共2n3+2n22n^3+2n^22n3+2n2 flops。 不过实践中不推荐逆矩阵求解，因为可能在准确性上产生问题。求解线性方程更常用的是分解方法，比如接下来的LU分解。 解线性方程组（LU分解） LU分解（lower–upper decomposition）是一种常用的矩阵分解方法，基本思路是利用高斯消元法将A\\mathbf{A}A转化为上三角矩阵U\\mathbf{U}U，主要过程如下所示： [×××××××××]⏞A→[×××0××0××]⏞L1A→[×××0××00×]⏞L2L1A\\begin{equation}\\nonumber \\overbrace{\\begin{bmatrix} \\times &amp; \\times &amp; \\times\\\\ \\times &amp; \\times &amp; \\times\\\\ \\times &amp; \\times &amp; \\times\\\\ \\end{bmatrix}}^{\\mathbf{A}} \\rightarrow \\overbrace{\\begin{bmatrix} \\times &amp; \\times &amp; \\times\\\\ 0 &amp; \\times &amp; \\times\\\\ 0 &amp; \\times &amp; \\times\\\\ \\end{bmatrix}}^{\\mathbf{L}_1\\mathbf{A}} \\rightarrow \\overbrace{\\begin{bmatrix} \\times &amp; \\times &amp; \\times\\\\ 0 &amp; \\times &amp; \\times\\\\ 0 &amp; 0 &amp; \\times\\\\ \\end{bmatrix}}^{\\mathbf{L}_2\\mathbf{L}_1\\mathbf{A}} \\end{equation} ⎣⎡​×××​×××​×××​⎦⎤​​A​→⎣⎡​×00​×××​×××​⎦⎤​​L1​A​→⎣⎡​×00​××0​×××​⎦⎤​​L2​L1​A​​ 其中L1,L2\\mathbf{L}_1,\\mathbf{L}_2L1​,L2​是一系列下三角矩阵，用来表示第n行加上第n-1行乘以某个数的消元操作，所以矩阵A\\mathbf{A}A可以消元成为一个上三角矩阵U\\mathbf{U}U： Ln−1Ln−2⋯L1A=U\\begin{equation} \\mathbf{L}_{n-1}\\mathbf{L}_{n-2} \\cdots \\mathbf{L}_{1}\\mathbf{A} = \\mathbf{U} \\end{equation} Ln−1​Ln−2​⋯L1​A=U​​ 矩阵A\\mathbf{A}A的LU分解为： A=(L1−1⋯Ln−2−1Ln−1−1)U=LU\\begin{equation} \\begin{split} \\mathbf{A} &amp;= \\left(\\mathbf{L}_{1}^{-1} \\cdots \\mathbf{L}_{n-2}^{-1} \\mathbf{L}_{n-1}^{-1}\\right) \\mathbf{U}\\\\ &amp;= \\mathbf{L} \\mathbf{U} \\end{split} \\end{equation} A​=(L1−1​⋯Ln−2−1​Ln−1−1​)U=LU​​​ 下三角矩阵的逆依然是下三角矩阵，并且一系列下三角矩阵的乘积也是一个三角矩阵，因此L\\mathbf{L}L是下三角矩阵。此外，虽然涉及到求Li\\mathbf{L}_iLi​的逆，但是这一步骤实施起来是非常简单的，Li\\mathbf{L}_iLi​主对角线元素不变、下三角元素全部乘以-1即可得到Li−1\\mathbf{L}_i^{-1}Li−1​。因为Li\\mathbf{L}_iLi​每次都只对第iii列上的主元做消元操作，所以下三角部分只会在第iii列存在不为0的元素。以3×33 \\times 33×3矩阵为例，L1\\mathbf{L}_1L1​可以写成如下的形式： [100a10b01]\\begin{equation}\\nonumber \\begin{bmatrix} 1 &amp; 0 &amp; 0\\\\ a &amp; 1 &amp; 0\\\\ b &amp; 0 &amp; 1\\\\ \\end{bmatrix} \\end{equation} ⎣⎡​1ab​010​001​⎦⎤​​ 很自然的可以证明L1−1\\mathbf{L}_1^{-1}L1−1​为： [100−a10−b01]\\begin{equation}\\nonumber \\begin{bmatrix} 1 &amp; 0 &amp; 0\\\\ -a &amp; 1 &amp; 0\\\\ -b &amp; 0 &amp; 1\\\\ \\end{bmatrix} \\end{equation} ⎣⎡​1−a−b​010​001​⎦⎤​​ 上述步骤的LU分解需要的浮点数操作近似为23n3\\frac{2}{3}n^332​n3 flops。 接下来我们看看如何利用LU分解解线性方程组。约定A\\b\\mathbf{A} \\backslash \\mathbf{b}A\\b代表Ax=b\\mathbf{A}\\mathbf{x}=\\mathbf{b}Ax=b的解x\\mathbf{x}x（这也是MATLAB求解线性方程组的代码），按照以下步骤求解出x\\mathbf{x}x： LU=AL\\b=yU\\y=x\\begin{equation} \\begin{split} \\mathbf{L} \\mathbf{U} &amp;= \\mathbf{A}\\\\ \\mathbf{L} \\backslash \\mathbf{b} &amp;= \\mathbf{y}\\\\ \\mathbf{U} \\backslash \\mathbf{y} &amp;= \\mathbf{x}\\\\ \\end{split} \\end{equation} LUL\\bU\\y​=A=y=x​​​ 所以先求解Ly=b\\mathbf{L}\\mathbf{y}=\\mathbf{b}Ly=b得到中间变量y\\mathbf{y}y，再求解Ux=y\\mathbf{U}\\mathbf{x}=\\mathbf{y}Ux=y得到x\\mathbf{x}x，看上去比直接求解Ax=b\\mathbf{A}\\mathbf{x} = \\mathbf{b}Ax=b复杂了不少，为什么要这么做呢？ 原因在于L\\mathbf{L}L和U\\mathbf{U}U作为三角矩阵能显著简化线性方程组求解，比如Ly=b\\mathbf{L}\\mathbf{y}=\\mathbf{b}Ly=b，我们有： [1⋮⋱li,1⋯1⋮⋮⋮⋱ln,1⋯⋯⋯1][y1⋮yi⋮yn]=[b1⋮bi⋮bn]\\begin{equation} \\begin{bmatrix} 1 &amp; &amp; &amp; &amp; \\\\ \\vdots&amp;\\ddots&amp; &amp; &amp;\\\\ l_{i,1}&amp;\\cdots&amp;1&amp; &amp;\\\\ \\vdots&amp;\\vdots&amp;\\vdots&amp;\\ddots&amp;\\\\ l_{n,1}&amp;\\cdots&amp;\\cdots&amp;\\cdots&amp;1\\\\ \\end{bmatrix} \\begin{bmatrix} y_1\\\\ \\vdots\\\\ y_i\\\\ \\vdots\\\\ y_n\\\\ \\end{bmatrix} = \\begin{bmatrix} b_1\\\\ \\vdots\\\\ b_i\\\\ \\vdots\\\\ b_n\\\\ \\end{bmatrix} \\end{equation} ⎣⎡​1⋮li,1​⋮ln,1​​⋱⋯⋮⋯​1⋮⋯​⋱⋯​1​⎦⎤​⎣⎡​y1​⋮yi​⋮yn​​⎦⎤​=⎣⎡​b1​⋮bi​⋮bn​​⎦⎤​​​ 可以用递归方法求解y\\mathbf{y}y： y1=b1yi=bi−li,1y1−⋯−li,i−1yi−1\\begin{equation} \\begin{split} y_1 &amp;= b_1\\\\ y_i &amp;= b_i - l_{i,1}y_1 - \\cdots - l_{i, i-1}y_{i-1} \\end{split} \\end{equation} y1​yi​​=b1​=bi​−li,1​y1​−⋯−li,i−1​yi−1​​​​ 这种思路叫forward substitution，即从上往下求解y\\mathbf{y}y。之后可以用相似的方式从下往上求解x\\mathbf{x}x，叫backward substitution。forward substitution的成本为（每一个元素各需要i−1i-1i−1次乘法和减法）n2−nn^2-nn2−n flops，同理可得backward substitution的计算成本为n2+nn^2+nn2+n flops，所以用LU分解求解线性方程组的总计算成本为23n3+2n2\\frac{2}{3}n^3+2n^232​n3+2n2 flops。 解线性方程组（QR分解） QR分解（QR decomposition）也常用于求解线性方程组。QR分解将A∈Rn×n\\mathbf{A} \\in \\mathbb{R}^{n \\times n}A∈Rn×n分解为正交矩阵Q∈Rn×n\\mathbf{Q} \\in \\mathbb{R}^{n \\times n}Q∈Rn×n和上三角矩阵R∈Rn×n\\mathbf{R} \\in \\mathbb{R}^{n \\times n}R∈Rn×n： A=QR\\begin{equation} \\mathbf{A} = \\mathbf{Q} \\mathbf{R} \\end{equation} A=QR​​ 对于正交矩阵Q\\mathbf{Q}Q，它的逆矩阵Q−1\\mathbf{Q}^{-1}Q−1实际就是QT\\mathbf{Q}^TQT本身，所以对于线性方程组可以简化为： Ax=bQRx=bRx=QTb\\begin{equation} \\begin{split} \\mathbf{A} \\mathbf{x} &amp;= \\mathbf{b}\\\\ \\mathbf{Q}\\mathbf{R}\\mathbf{x} &amp;= \\mathbf{b}\\\\ \\mathbf{R}\\mathbf{x} &amp;= \\mathbf{Q}^T\\mathbf{b}\\\\ \\end{split} \\end{equation} AxQRxRx​=b=b=QTb​​​ 因为R\\mathbf{R}R是上三角矩阵，对于x\\mathbf{x}x的求解可以采用LU分解中介绍的backward substitution方式求解，从而避免了求逆的操作。 QR分解可以采用Gram-Schmidt正交化过程计算得到，这一过程与正交的几何表示密切相关，因此易于理解，不过这种正交化过程容易数值不稳定，因此很少直接在实践中使用。实践中更常用的是使用Givens rotations方法来实现QR分解。 Givens rotations的步骤与LU分解相似，对于矩阵A\\mathbf{A}A，采用一系列旋转矩阵Gij\\mathbf{G}_i^jGij​将A\\mathbf{A}A逐步转化为上三角矩阵： [×××××××××]⏞A→[××××××0××]⏞G31A→[×××0××0××]⏞G21G31A→[×××0××00×]⏞G32G21G31A\\begin{equation}\\nonumber \\overbrace{\\begin{bmatrix} \\times &amp; \\times &amp; \\times\\\\ \\times &amp; \\times &amp; \\times\\\\ \\times &amp; \\times &amp; \\times\\\\ \\end{bmatrix}}^{\\mathbf{A}} \\rightarrow \\overbrace{\\begin{bmatrix} \\times &amp; \\times &amp; \\times\\\\ \\times &amp; \\times &amp; \\times\\\\ 0 &amp; \\times &amp; \\times\\\\ \\end{bmatrix}}^{\\mathbf{G}_3^1\\mathbf{A}} \\rightarrow \\overbrace{\\begin{bmatrix} \\times &amp; \\times &amp; \\times\\\\ 0 &amp; \\times &amp; \\times\\\\ 0 &amp; \\times &amp; \\times\\\\ \\end{bmatrix}}^{\\mathbf{G}_2^1\\mathbf{G}_3^1\\mathbf{A}} \\rightarrow \\overbrace{\\begin{bmatrix} \\times &amp; \\times &amp; \\times\\\\ 0 &amp; \\times &amp; \\times\\\\ 0 &amp; 0 &amp; \\times\\\\ \\end{bmatrix}}^{\\mathbf{G}_3^2\\mathbf{G}_2^1\\mathbf{G}_3^1\\mathbf{A}} \\end{equation} ⎣⎡​×××​×××​×××​⎦⎤​​A​→⎣⎡​××0​×××​×××​⎦⎤​​G31​A​→⎣⎡​×00​×××​×××​⎦⎤​​G21​G31​A​→⎣⎡​×00​××0​×××​⎦⎤​​G32​G21​G31​A​​ 对于需要置0的第aija_{ij}aij​个元素(i&gt;j)(i \\gt j)(i&gt;j)，Gij\\mathbf{G}_i^jGij​构造为： Gij=[1⋯0⋯0⋯0⋮⋱⋮⋮⋮0⋯cjj⋯−sji⋯0⋮⋮⋱⋮⋮0⋯sij⋯cii⋯0⋮⋮⋮⋱⋮0⋯0⋯0⋯1]\\begin{equation} \\mathbf{G}_i^j = \\begin{bmatrix} 1 &amp; \\cdots &amp; 0 &amp; \\cdots &amp; 0 &amp; \\cdots &amp; 0\\\\ \\vdots &amp; \\ddots &amp; \\vdots &amp; &amp; \\vdots &amp; &amp; \\vdots\\\\ 0 &amp; \\cdots &amp; c_{jj} &amp; \\cdots &amp; -s_{ji} &amp; \\cdots &amp; 0\\\\ \\vdots &amp; &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; &amp; \\vdots\\\\ 0 &amp; \\cdots &amp; s_{ij} &amp; \\cdots &amp; c_{ii} &amp; \\cdots &amp; 0\\\\ \\vdots &amp; &amp; \\vdots &amp; &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ 0 &amp; \\cdots &amp; 0 &amp; \\cdots &amp; 0 &amp; \\cdots &amp; 1\\\\ \\end{bmatrix} \\end{equation} Gij​=⎣⎡​1⋮0⋮0⋮0​⋯⋱⋯⋯⋯​0⋮cjj​⋮sij​⋮0​⋯⋯⋱⋯⋯​0⋮−sji​⋮cii​⋮0​⋯⋯⋯⋱⋯​0⋮0⋮0⋮1​⎦⎤​​​ 其中cjj=cii=cos⁡(θ)c_{jj} = c_{ii} = \\cos(\\theta)cjj​=cii​=cos(θ)，sij=sji=sin⁡(θ)s_{ij} = s_{ji} = \\sin(\\theta)sij​=sji​=sin(θ)。显而易见这是一个对ijijij平面上的向量进行旋转的矩阵，对于任意向量，若想将其在第iii个轴上的分量置0，则需要将其按θ\\thetaθ角旋转至对应的坐标轴上，旋转变化不会改变除第iii、jjj行外的其他元素，因此通过反复运用旋转变换，可以将A\\mathbf{A}A的下三角区域置0，有： Gnn−1⋯Gn−11Gn1A=R\\begin{equation} \\mathbf{G}_n^{n-1} \\cdots \\mathbf{G}_{n-1}^1 \\mathbf{G}_n^1 \\mathbf{A} = \\mathbf{R} \\end{equation} Gnn−1​⋯Gn−11​Gn1​A=R​​ 又因为旋转矩阵本身就是正交矩阵，所以其逆就是矩阵转置本身，所以有： A=(Gnn−1⋯Gn−11Gn1)TR=QR\\begin{equation} \\begin{split} \\mathbf{A} &amp;= \\left(\\mathbf{G}_n^{n-1} \\cdots \\mathbf{G}_{n-1}^1 \\mathbf{G}_n^1\\right)^T \\mathbf{R}\\\\ &amp;= \\mathbf{Q}\\mathbf{R} \\end{split} \\end{equation} A​=(Gnn−1​⋯Gn−11​Gn1​)TR=QR​​​ Givens rotations实现的QR分解的复杂度大约是73n3\\frac{7}{3}n^337​n3 flops（我也不确定），优势在于可以较为容易的实现并行化操作。 复杂度分析 从上面的分析中可以看出，LU分解大概需要23n3\\frac{2}{3}n^332​n3 flops，逆矩阵大概需要2n32n^32n3 flops，是LU分解的3倍，因此线性方程组求解采用逆矩阵方法会更慢一些。尽管逆矩阵求解存在更快的算法，但是通常来说LU分解还是更有效率，所以一般而言还是应当直接求解线性方程组而不是计算逆矩阵。 此外，逆矩阵计算往往需要更大的内存。如果A\\mathbf{A}A是稀疏矩阵（大多数元素是0），则可以使用更加有效率的稀疏结构存储。但是A−1\\mathbf{A}^{-1}A−1通常是稠密的，因此存储逆矩阵需要更多的内存。 准确性分析 逆矩阵的另一个缺点在于求解的误差更大，特别在A\\mathbf{A}A是病态矩阵的情况下尤为明显。 数值分析领域常用前向误差（forward error）和反向误差（backward error）来分析误差。比如对于函数y=f(x)y=f(x)y=f(x)，用y^\\hat{y}y^​来近似输出yyy，那么前向误差被定义为∣y^−y∣|\\hat{y}-y|∣y^​−y∣（这似乎就是我们常说的误差），而反向误差则衡量问题的敏感性，即为了产生近似解，输入数据同真实数据的偏移情况。y^\\hat{y}y^​的反向误差是满足y^=f(x+Δx)\\hat{y}=f(x+\\Delta x)y^​=f(x+Δx)的最小Δx\\Delta xΔx。 前向误差和反向误差联合起来可以分析一个问题是良态（well-conditioned）还是病态（ill-conditioned）的。对于良态问题，输入较小的改变会导致输出产生较小的改变；而病态问题，输入较小的改变会导致输出较大的改变。条件数（condition number）就是用来判断良态和病态的工具，条件数越大，输入很小的差异就会产生较大的输出误差，更倾向于病态问题。矩阵的条件数κ(A)\\kappa(\\mathbf{A})κ(A)定义为： κ(A)=∥A∥∥A−1∥\\begin{equation} \\kappa(\\mathbf{A}) = \\|\\mathbf{A}\\| \\|\\mathbf{A}^{-1}\\| \\end{equation} κ(A)=∥A∥∥A−1∥​​ 如果采用矩阵的2范数，上式可以进一步简化为最大特征值λ1\\lambda_1λ1​与最小特征值λn\\lambda_nλn​之比： κ(A)=λ1λn\\begin{equation} \\kappa(\\mathbf{A}) = \\frac{\\lambda_1}{\\lambda_n} \\end{equation} κ(A)=λn​λ1​​​​ stackexchange和这篇blog详细分析了前向误差∥x−x^∥\\|\\mathbf{x}-\\hat{\\mathbf{x}}\\|∥x−x^∥和反向误差∥b−Ax^∥\\|\\mathbf{b}-\\mathbf{A}\\hat{\\mathbf{x}}\\|∥b−Ax^∥（注意这里x\\mathbf{x}x是我们求解问题的输出），有如下结论： 对于良态矩阵A\\mathbf{A}A，LU分解的前向误差和逆矩阵的前向误差是很接近的 对于病态矩阵A\\mathbf{A}A，逆矩阵的反向误差会远大于LU分解的反向误差 即使对于良态矩阵A\\mathbf{A}A，逆矩阵的反向误差也比LU分解的反向误差更大 此外，前向误差和反向误差有如下不等式关系： ∥x−x^∥≤κ(A)∥b−Ax^∥\\begin{equation} \\|\\mathbf{x}-\\hat{\\mathbf{x}}\\| \\le \\kappa(\\mathbf{A}) \\|\\mathbf{b}-\\mathbf{A}\\hat{\\mathbf{x}}\\| \\end{equation} ∥x−x^∥≤κ(A)∥b−Ax^∥​​ 综合来看的话，利用LU分解、QR分解等方法直接求解线性方程组比逆矩阵求解要更加准确。 源码分析 TODO: compare python and matlab code","categories":[{"name":"machine learning","slug":"machine-learning","permalink":"https://mrswolf.github.io/categories/machine-learning/"}],"tags":[{"name":"machine learning","slug":"machine-learning","permalink":"https://mrswolf.github.io/tags/machine-learning/"},{"name":"matrix decomposition","slug":"matrix-decomposition","permalink":"https://mrswolf.github.io/tags/matrix-decomposition/"}]},{"title":"矩阵微积分","slug":"matrix-calculus","date":"2022-03-27T16:00:00.000Z","updated":"2022-03-30T16:00:00.000Z","comments":false,"path":"matrix-calculus/","link":"matrix-calculus","permalink":"https://mrswolf.github.io/matrix-calculus/","excerpt":"矩阵微分和矩阵求导几乎是求解优化问题不可避免的必学内容，这一方面的内容老实说我很难完全掌握。这里记录一下一些常用的矩阵微分求导的规范和技巧。","text":"矩阵微分和矩阵求导几乎是求解优化问题不可避免的必学内容，这一方面的内容老实说我很难完全掌握。这里记录一下一些常用的矩阵微分求导的规范和技巧。 符号约定和布局规范 首先微分（differential）是在自变量微小变化下造成的因变量的微小变化，而导数（derivative）则是这种变化的速率。联系导数和微分的方程式叫做微分方程（differential equation）。比如函数y=f(x)y=\\mathrm{f}(x)y=f(x)，xxx的微小变化用符号dxdxdx表示，导致的yyy的微小变化用dydydy表示，xxx的变化引起的yyy的变化的速率用∂y∂x\\frac{\\partial y}{\\partial x}∂x∂y​表示，函数f(x)\\mathrm{f}(x)f(x)的微分方程就是： dy=∂y∂xdx\\begin{equation} dy=\\frac{\\partial y}{\\partial x} dx \\end{equation} dy=∂x∂y​dx​​ 矩阵微分/导数同函数微分/导数基本一致，只不过现在输入输出都是矩阵（向量也是矩阵的一种）的表现形式，比如y=f(x)\\mathbf{y} = \\mathrm{f}(\\mathbf{x})y=f(x)。这里用加粗大写字母表示矩阵，例如A\\mathbf{A}A；用加粗小写字母表示向量，例如a\\mathbf{a}a；用不加粗小写字母表示标量，例如aaa。 矩阵微分求导的难点在于没有固定的布局规范，导致有些文章和教材看起来互相冲突。比如对于∂y∂x\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}∂x∂y​，其中y∈Rm×1\\mathbf{y} \\in \\mathbb{R}^{m \\times 1}y∈Rm×1，x∈Rn×1\\mathbf{x} \\in \\mathbb{R}^{n \\times 1}x∈Rn×1，向量对向量的导数用矩阵形式可以有两种表示布局方案： 分子布局（numerator layout），这种布局要求y\\mathbf{y}y是按列排的，x\\mathbf{x}x是按行排列的（即xT\\mathbf{x}^TxT），最终输出∂y∂x∈Rm×n\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} \\in \\mathbb{R}^{m \\times n}∂x∂y​∈Rm×n，我觉得可以简单理解为与分子上矩阵元素相关的输出排列规则不变（跟原来一致），而与分母上矩阵元素相关输出排列规则应当是原来元素的转置。 分母布局（denominator layout），这种布局要求y\\mathbf{y}y是按行排的（即yT\\mathbf{y}^TyT），x\\mathbf{x}x是按列排列的，最终输出∂y∂x∈Rn×m\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} \\in \\mathbb{R}^{n \\times m}∂x∂y​∈Rn×m，正好跟分子布局相反。 为了方便推导和记忆，我选择采用分子布局，那么矩阵间导数的形式应当是这样的： yyy y∈Rm×1\\mathbf{y} \\in \\mathbb{R}^{m \\times 1}y∈Rm×1 Y∈Rm×n\\mathbf{Y} \\in \\mathbb{R}^{m \\times n}Y∈Rm×n xxx ∂y∂x\\frac{\\partial y}{\\partial x}∂x∂y​ ∂y∂x∈Rm×1\\frac{\\partial \\mathbf{y}}{\\partial x} \\in \\mathbb{R}^{m \\times 1}∂x∂y​∈Rm×1 ∂Y∂x∈Rm×n\\frac{\\partial \\mathbf{Y}}{\\partial x} \\in \\mathbb{R}^{m \\times n}∂x∂Y​∈Rm×n x∈Rp×1\\mathbf{x} \\in \\mathbb{R}^{p \\times 1}x∈Rp×1 ∂y∂x∈R1×p\\frac{\\partial y}{\\partial \\mathbf{x}} \\in \\mathbb{R}^{1 \\times p}∂x∂y​∈R1×p ∂y∂x∈Rm×p\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} \\in \\mathbb{R}^{m \\times p}∂x∂y​∈Rm×p X∈Rp×q\\mathbf{X} \\in \\mathbb{R}^{p \\times q}X∈Rp×q ∂y∂X∈Rq×p\\frac{\\partial y}{\\partial \\mathbf{X}} \\in \\mathbb{R}^{q \\times p}∂X∂y​∈Rq×p 微分规则 矩阵求导常需要用到sum rule、product rule和chain rule，其中链式法则不适用于matrix-by-scalar或scalar-by-matrix的形式，所以直接复合函数求导蛮麻烦的。wiki上说可以先用微分的规则求微分，然后再转换成导数的形式。 sum rules： h(x)=f(x)+g(x)dh(x)=df(x)+dg(x)\\begin{equation} \\begin{split} h(x) &amp;= f(x) + g(x)\\\\ dh(x) &amp;= df(x) + dg(x)\\\\ \\end{split} \\end{equation} h(x)dh(x)​=f(x)+g(x)=df(x)+dg(x)​​​ product rules： h(x)=f(x)g(x)dh(x)=df(x)g(x)+f(x)dg(x)\\begin{equation} \\begin{split} h(x) &amp;= f(x)g(x)\\\\ dh(x) &amp;= df(x)g(x) + f(x)dg(x)\\\\ \\end{split} \\end{equation} h(x)dh(x)​=f(x)g(x)=df(x)g(x)+f(x)dg(x)​​​ chain rules： h(x)=f(g(x))dh(x)=f(g(x+dx))−f(g(x))=f(g(x)+dg(x))−f(g(x))=df(y)∣y=g(x),dy=dg(x)\\begin{equation} \\begin{split} h(x) &amp;= f(g(x))\\\\ dh(x) &amp;= f(g(x+dx)) - f(g(x))\\\\ &amp;= f(g(x)+dg(x)) - f(g(x))\\\\ &amp;= df(y)|_{y=g(x),dy=dg(x)}\\\\ \\end{split} \\end{equation} h(x)dh(x)​=f(g(x))=f(g(x+dx))−f(g(x))=f(g(x)+dg(x))−f(g(x))=df(y)∣y=g(x),dy=dg(x)​​​​ trace tricks： a=tr(a)tr(A)=tr(AT)tr(A+B)=tr(A)+tr(B)tr(ABC)=tr(BCA)=tr(CAB)\\begin{equation} \\begin{split} a &amp;= \\mathrm{tr}(a)\\\\ \\mathrm{tr}(\\mathbf{A}) &amp;= \\mathrm{tr}(\\mathbf{A}^T)\\\\ \\mathrm{tr}(\\mathbf{A}+\\mathbf{B}) &amp;= \\mathrm{tr}(\\mathbf{A}) + \\mathrm{tr}(\\mathbf{B})\\\\ \\mathrm{tr}(\\mathbf{A}\\mathbf{B}\\mathbf{C}) &amp;= \\mathrm{tr}(\\mathbf{B}\\mathbf{C}\\mathbf{A})\\\\ &amp;= \\mathrm{tr}(\\mathbf{C}\\mathbf{A}\\mathbf{B})\\\\ \\end{split} \\end{equation} atr(A)tr(A+B)tr(ABC)​=tr(a)=tr(AT)=tr(A)+tr(B)=tr(BCA)=tr(CAB)​​​ kronecker product rules: A⊗(B+C)=A⊗B+A⊗C(kA)⊗B=A⊗(kB)=k(A⊗B)(A⊗B)⊗C=A⊗(B⊗C)(A⊗B)(C⊗D)=(AC)⊗(BD)(A⊗B)∘(C⊗D)=(A∘C)⊗(B∘D)(A⊗B)−1=A−1⊗B−1(A⊗B)T=AT⊗BTtr(A⊗B)=tr(A)tr(B)\\begin{equation} \\begin{split} \\mathbf{A}\\otimes(\\mathbf{B}+\\mathbf{C}) &amp;= \\mathbf{A}\\otimes\\mathbf{B} + \\mathbf{A}\\otimes\\mathbf{C}\\\\ (k\\mathbf{A})\\otimes\\mathbf{B} &amp;= \\mathbf{A}\\otimes(k\\mathbf{B})=k(\\mathbf{A}\\otimes\\mathbf{B})\\\\ (\\mathbf{A}\\otimes\\mathbf{B})\\otimes\\mathbf{C} &amp;= \\mathbf{A}\\otimes(\\mathbf{B}\\otimes\\mathbf{C})\\\\ (\\mathbf{A}\\otimes\\mathbf{B})(\\mathbf{C}\\otimes\\mathbf{D}) &amp;= (\\mathbf{A}\\mathbf{C})\\otimes(\\mathbf{B}\\mathbf{D})\\\\ (\\mathbf{A}\\otimes\\mathbf{B})\\circ(\\mathbf{C}\\otimes\\mathbf{D}) &amp;= (\\mathbf{A}\\circ\\mathbf{C})\\otimes(\\mathbf{B}\\circ\\mathbf{D})\\\\ (\\mathbf{A}\\otimes\\mathbf{B})^{-1} &amp;= \\mathbf{A}^{-1} \\otimes \\mathbf{B}^{-1}\\\\ (\\mathbf{A}\\otimes\\mathbf{B})^{T} &amp;= \\mathbf{A}^{T} \\otimes \\mathbf{B}^{T}\\\\ \\mathrm{tr}(\\mathbf{A}\\otimes\\mathbf{B}) &amp;= \\mathrm{tr}(\\mathbf{A})\\mathrm{tr}(\\mathbf{B})\\\\ \\end{split} \\end{equation} A⊗(B+C)(kA)⊗B(A⊗B)⊗C(A⊗B)(C⊗D)(A⊗B)∘(C⊗D)(A⊗B)−1(A⊗B)Ttr(A⊗B)​=A⊗B+A⊗C=A⊗(kB)=k(A⊗B)=A⊗(B⊗C)=(AC)⊗(BD)=(A∘C)⊗(B∘D)=A−1⊗B−1=AT⊗BT=tr(A)tr(B)​​​ hadamard product rules: A∘B=B∘AA∘(B+C)=A∘B+A∘C(kA)∘B=A∘(kB)=k(A∘B)(A∘B)∘C=A∘(B∘C)\\begin{equation} \\begin{split} \\mathbf{A}\\circ\\mathbf{B} &amp;= \\mathbf{B}\\circ\\mathbf{A}\\\\ \\mathbf{A}\\circ(\\mathbf{B}+\\mathbf{C}) &amp;= \\mathbf{A}\\circ\\mathbf{B} + \\mathbf{A}\\circ\\mathbf{C}\\\\ (k\\mathbf{A})\\circ\\mathbf{B} &amp;= \\mathbf{A}\\circ(k\\mathbf{B})=k(\\mathbf{A}\\circ\\mathbf{B})\\\\ (\\mathbf{A}\\circ\\mathbf{B})\\circ\\mathbf{C} &amp;= \\mathbf{A}\\circ(\\mathbf{B}\\circ\\mathbf{C})\\\\ \\end{split} \\end{equation} A∘BA∘(B+C)(kA)∘B(A∘B)∘C​=B∘A=A∘B+A∘C=A∘(kB)=k(A∘B)=A∘(B∘C)​​​ 总结矩阵常用的微分规则如下： 说明 表达式 微分结果 A\\mathbf{A}A不是X\\mathbf{X}X的函数 d(A)d\\left(\\mathbf{A}\\right)d(A) 0\\mathbf{0}0 aaa不是X\\mathbf{X}X的函数 d(aX)d(a\\mathbf{X})d(aX) adXad\\mathbf{X}adX d(X⊗Y)d(\\mathbf{X} \\otimes \\mathbf{Y})d(X⊗Y) (dX)⊗Y+X⊗(dY)(d\\mathbf{X}) \\otimes \\mathbf{Y} + \\mathbf{X} \\otimes (d\\mathbf{Y})(dX)⊗Y+X⊗(dY) d(X∘Y)d(\\mathbf{X} \\circ \\mathbf{Y})d(X∘Y) (dX)∘Y+X∘(dY)(d\\mathbf{X}) \\circ \\mathbf{Y} + \\mathbf{X} \\circ (d\\mathbf{Y})(dX)∘Y+X∘(dY) d(XT)d(\\mathbf{X}^T)d(XT) (dX)T(d\\mathbf{X})^T(dX)T 共轭转置 d(XH)d(\\mathbf{X}^H)d(XH) (dX)H(d\\mathbf{X})^H(dX)H d(X−1)d(\\mathbf{X}^{-1})d(X−1) −X−1(dX)X−1-\\mathbf{X}^{-1}(d\\mathbf{X})\\mathbf{X}^{-1}−X−1(dX)X−1 nnn是正整数 d(Xn)d(\\mathbf{X}^n)d(Xn) ∑i=0n−1Xi(dX)Xn−1−i\\sum_{i=0}^{n-1} \\mathbf{X}^i(d\\mathbf{X})\\mathbf{X}^{n-1-i}∑i=0n−1​Xi(dX)Xn−1−i d(eX)d(e^{\\mathbf{X}})d(eX) ∫01eaX(dX)e(1−a)Xda\\int_0^1 e^{a\\mathbf{X}}(d\\mathbf{X}) e^{(1-a)\\mathbf{X}}da∫01​eaX(dX)e(1−a)Xda d(log(X))d(\\mathrm{log}(\\mathbf{X}))d(log(X)) ∫0∞(X+zI)−1(dX)(X+zI)−1dz\\int_0^{\\infty} (\\mathbf{X}+z\\mathbf{I})^{-1}(d\\mathbf{X})(\\mathbf{X}+z\\mathbf{I})^{-1}dz∫0∞​(X+zI)−1(dX)(X+zI)−1dz d(tr(X))d(\\mathrm{tr}(\\mathbf{X}))d(tr(X)) tr(dX)\\mathrm{tr}(d\\mathbf{X})tr(dX) d(det⁡(X))d(\\det(\\mathbf{X}))d(det(X)) det⁡(X)tr(X−1dX)\\det(\\mathbf{X})\\mathrm{tr}(\\mathbf{X}^{-1}d\\mathbf{X})det(X)tr(X−1dX) d(log⁡(det⁡(X)))d(\\log(\\det(\\mathbf{X})))d(log(det(X))) tr(X−1dX)\\mathrm{tr}(\\mathbf{X}^{-1}d\\mathbf{X})tr(X−1dX) 微分-导数转换 在获得表达式的微分形式后，可以按如下规则进行导数形式的转化： 微分形式 导数形式 dy=adxdy=adxdy=adx ∂y∂x=a\\frac{\\partial y}{\\partial x}=a∂x∂y​=a dy=aTdxdy=\\mathbf{a}^Td\\mathbf{x}dy=aTdx ∂y∂x=aT\\frac{\\partial y}{\\partial \\mathbf{x}}=\\mathbf{a}^T∂x∂y​=aT dy=tr(AdX)dy=\\mathrm{tr}(\\mathbf{A}d\\mathbf{X})dy=tr(AdX) ∂y∂X=A\\frac{\\partial y}{\\partial \\mathbf{X}}=\\mathbf{A}∂X∂y​=A dy=adxd\\mathbf{y}=\\mathbf{a}dxdy=adx ∂y∂x=a\\frac{\\partial \\mathbf{y}}{\\partial x}=\\mathbf{a}∂x∂y​=a dy=Adxd\\mathbf{y}=\\mathbf{A} d\\mathbf{x}dy=Adx ∂y∂x=A\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}=\\mathbf{A}∂x∂y​=A dY=Adxd\\mathbf{Y}=\\mathbf{A}dxdY=Adx ∂Y∂x=A\\frac{\\partial \\mathbf{Y}}{\\partial x}=\\mathbf{A}∂x∂Y​=A","categories":[{"name":"machine learning","slug":"machine-learning","permalink":"https://mrswolf.github.io/categories/machine-learning/"}],"tags":[{"name":"machine learning","slug":"machine-learning","permalink":"https://mrswolf.github.io/tags/machine-learning/"},{"name":"matrix decomposition","slug":"matrix-decomposition","permalink":"https://mrswolf.github.io/tags/matrix-decomposition/"},{"name":"deep learning","slug":"deep-learning","permalink":"https://mrswolf.github.io/tags/deep-learning/"}]},{"title":"主成分分析PCA","slug":"pca","date":"2019-12-10T16:00:00.000Z","updated":"2022-03-20T16:00:00.000Z","comments":false,"path":"principle-component-analysis/","link":"principle-component-analysis","permalink":"https://mrswolf.github.io/principle-component-analysis/","excerpt":"主成分分析（Principle Component Analysis，PCA）是常用的一种矩阵分解算法，PCA通过旋转原始空间来使得数据在各个正交轴上的投影最大，通过选择前几个正交轴可以实现数据降维的目的。","text":"主成分分析（Principle Component Analysis，PCA）是常用的一种矩阵分解算法，PCA通过旋转原始空间来使得数据在各个正交轴上的投影最大，通过选择前几个正交轴可以实现数据降维的目的。 PCA数学原理 优化问题 PCA的优化问题如下： arg max⁡Wtrace(WTXXTW)s.t.WTW=I\\begin{equation} \\begin{split} \\argmax_{\\mathbf{W}}\\quad &amp;\\mathrm{trace}\\left(\\mathbf{W}^T\\mathbf{X}\\mathbf{X}^T\\mathbf{W}\\right)\\\\ \\textrm{s.t.}\\quad &amp;\\mathbf{W}^T\\mathbf{W} = \\mathbf{I} \\end{split} \\end{equation} Wargmax​s.t.​trace(WTXXTW)WTW=I​​​ 其中X∈RM×N\\mathbf{X} \\in \\mathbb{R}^{M \\times N}X∈RM×N是数据，W∈RM×M\\mathbf{W} \\in \\mathbb{R}^{M \\times M}W∈RM×M是投影矩阵，NNN是样本点个数，MMM是特征个数。PCA要求数据X\\mathbf{X}X做零均值处理，优化问题的解可以转化为如下特征值分解问题的解： (XXT)W=WΛ\\begin{equation} \\left(\\mathbf{X}\\mathbf{X}^T\\right)\\mathbf{W} = \\mathbf{W}\\mathbf{\\Lambda} \\end{equation} (XXT)W=WΛ​​ 这里假设W\\mathbf{W}W的列向量按相应特征值的大小从大到小排列，保留W\\mathbf{W}W前K列即前K个成分的列向量W^\\hat{\\mathbf{W}}W^，降维后的数据特征为： X^=W^TX\\begin{equation} \\hat{\\mathbf{X}} = \\hat{\\mathbf{W}}^T\\mathbf{X} \\end{equation} X^=W^TX​​ 其中X^∈RK×N\\hat{\\mathbf{X}} \\in \\mathbb{R}^{K \\times N}X^∈RK×N。 实现分析 svd替代eig sklearn中的PCA实现并未使用eig而是使用svd，主要原因是svd比eig具有更好的数值稳定性（当然代价是其计算时间要比eig更长）。使用svd代替eig也是很多学者如Andrew Ng建议的策略，在StackExchange上也有关于svd和eig的相关讨论讨论1、讨论2。sklearn中直接对数据矩阵X\\mathbf{X}X而不是协方差矩阵XXT\\mathbf{X}\\mathbf{X}^TXXT做svd，其等价关系如下： X=UΣVTXXT=UΣ2UTW=UΛ=Σ2\\begin{equation} \\begin{split} \\mathbf{X} &amp;= \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T\\\\ \\mathbf{X}\\mathbf{X}^T &amp;= \\mathbf{U} \\mathbf{\\Sigma}^2 \\mathbf{U}^T\\\\ \\mathbf{W} &amp;= \\mathbf{U}\\\\ \\mathbf{\\Lambda} &amp;= \\mathbf{\\Sigma}^2 \\end{split} \\end{equation} XXXTWΛ​=UΣVT=UΣ2UT=U=Σ2​​​ sign ambiguity问题 sklearn的PCA代码中还考虑了svd的sign ambiguity问题，即每个奇异向量的符号在求解过程中是不确定的（例如，将uk\\mathbf{u}_kuk​和vk\\mathbf{v}_kvk​同时乘以-1也满足求解条件）。svd算法（包括eig）中的奇异向量符号只是确保数值稳定性的副产品，类似随机分配符号，并无实际意义。 sklearn使用svd_flip(u, v, u_based_descision=True)函数来确保输出确定性的奇异向量符号，例如，如果u_based_decision=True，则要求U\\mathbf{U}U的每一列奇异向量中绝对值最大的元素的符号始终为正，V\\mathbf{V}V也要相对的做出调整。 鉴于MATLAB是算法开发的标准之一，我很好奇MATLAB是如何处理SVD的sign ambiguity问题的。MATLAB的svd函数的官方文档中有这样一句话: Different machines and releases of MATLAB® can produce different singular vectors that are still numerically accurate. Corresponding columns in U and V can flip their signs, since this does not affect the value of the expression A = USV’. MATLAB的eig函数的官方文档中亦提到： For real eigenvectors, the sign of the eigenvectors can change. 可以看出MATLAB也未保证符号的确定性。同样在MATALB的社区里也有人问了这个问题，并引导我看了这篇Resolving the Sign Ambiguity in the Singular Value Decompostion的文献。 文献中指出，sklearn的svd_flip方法是一种临时方案（ad hoc），并未从数据分析或者解释的角度来解决sign ambiguity问题。解决sign ambiguity的核心是如何为奇异向量选择一个“有意义”的符号。什么叫“有意义”？比方说我们要研究4种品牌汽车的每公里耗油量，做了4次抽样，构成数据矩阵： X=[4223515111169101411691014]\\begin{equation} \\begin{split} \\mathbf{X} = \\begin{bmatrix} 4 &amp;22&amp;3 &amp;5 \\\\ 1 &amp;5 &amp;1 &amp;1 \\\\ 11&amp;69&amp;10&amp;14 \\\\ 11&amp;69&amp;10&amp;14 \\\\ \\end{bmatrix} \\end{split} \\end{equation} X=⎣⎡​411111​2256969​311010​511414​⎦⎤​​​​ 其中每一列是一种品牌汽车的耗油量，每一行为抽样情况，svd分解有X=UΣVT\\mathbf{X}=\\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^TX=UΣVT。我们来看一下numpy.linalg.svd计算得到的V\\mathbf{V}V的第一个奇异向量： v0=[−0.15−0.96−0.14−0.20]\\begin{equation} \\begin{split} \\mathbf{v}_0 = \\begin{bmatrix} -0.15 \\\\ -0.96 \\\\ -0.14 \\\\ -0.20 \\\\ \\end{bmatrix} \\end{split} \\end{equation} v0​=⎣⎡​−0.15−0.96−0.14−0.20​⎦⎤​​​​ v0\\mathbf{v}_0v0​实际指明了耗油量空间的一个向量，然而我们知道耗油量没有负值（如果有的话人类就拥有无限能源了），一个完全指向负的方向意义不大，如果改变v0\\mathbf{v}_0v0​的符号，变为： v0=[0.150.960.140.20]\\begin{equation} \\begin{split} \\mathbf{v}_0 = \\begin{bmatrix} 0.15 \\\\ 0.96 \\\\ 0.14 \\\\ 0.20 \\\\ \\end{bmatrix} \\end{split} \\end{equation} v0​=⎣⎡​0.150.960.140.20​⎦⎤​​​​ 结果就合理多了。 文献中指出，奇异向量的符号应当与大多数数据样本向量的符号相同，从几何上来看，奇异向量应当指向大多数向量指向的方向。下图是我从文献中截取的，深色蓝线是正确的奇异向量方向，浅色蓝线是数据向量。 翻译成数学语言（我按照自己的理解和习惯转化成优化问题，与文献的原始表述并不一致，不一定对，有兴趣的读者可以看原始文献😃)，纠正符号算法的核心是对于每一对奇异向量uk\\mathbf{u}_kuk​和vk\\mathbf{v}_kvk​，寻找符号sks_ksk​优化以下目标函数 arg max⁡sk∈{1,−1}sk(∑j=1NukTX⋅,j+∑i=1MXi,⋅vk)\\begin{equation} \\argmax_{s_k \\in \\{1,-1\\}}\\quad s_k \\left(\\sum_{j=1}^N \\mathbf{u}_k^T\\mathbf{X}_{\\cdot,j} + \\sum_{i=1}^M\\mathbf{X}_{i,\\cdot}\\mathbf{v}_k\\right) \\end{equation} sk​∈{1,−1}argmax​sk​(j=1∑N​ukT​X⋅,j​+i=1∑M​Xi,⋅​vk​)​​ 根据两项求和项的符号即可决定sks_ksk​的符号。对于可能存在的左右奇异向量符号冲突的情况（例如单独看左奇异向量有意义的符号是-1，单独看右奇异向量有意义的符号为1），该算法选择以求和绝对值最大的一项的符号为主（反应在上式就是两项求和）。文献中指出，该算法仅在上述求和项不为0的情况下有效（即在0附近奇异向量的符号可以为任意情况）。 Python版具体算法实现如下，Matlab可以使用这个版本： import warningsimport numpy as npdef sign_flip(u, s, vh=None): &quot;&quot;&quot;Flip signs of SVD or EIG. &quot;&quot;&quot; left_proj = 0 if vh is not None: left_proj = np.sum(s[:, np.newaxis]*vh, axis=-1) right_proj = np.sum(u*s, axis=0) total_proj = left_proj + right_proj signs = np.sign(total_proj) random_idx = (signs==0) if np.any(random_idx): signs[random_idx] = 1 warnings.warn(&quot;The magnitude is close to zero, the sign will become arbitrary.&quot;) u = u*signs if vh is not None: vh = signs[:, np.newaxis]*vh return u, s, vh","categories":[{"name":"machine learning","slug":"machine-learning","permalink":"https://mrswolf.github.io/categories/machine-learning/"}],"tags":[{"name":"machine learning","slug":"machine-learning","permalink":"https://mrswolf.github.io/tags/machine-learning/"},{"name":"matrix decomposition","slug":"matrix-decomposition","permalink":"https://mrswolf.github.io/tags/matrix-decomposition/"}]},{"title":"manjaro踩坑记(2022更新版)","slug":"manjaro踩坑记","date":"2019-05-23T16:00:00.000Z","updated":"2022-03-20T16:00:00.000Z","comments":false,"path":"my-manjaro-log/","link":"my-manjaro-log","permalink":"https://mrswolf.github.io/my-manjaro-log/","excerpt":"从2019年到2022年，manjaro发行版渡过了我的整个博士生涯。最近毕业重新装了系统，依然选择了最新的manjaro KDE Plasma 21.2.4（本来装了arch，大小问题不断被劝退了😜）。基本上这台linux主机要跟着我进入人生下一阶段，作为主力台式机也不打算再折腾了。安装过程中有一些新的学习体会（坑），在这里更新记录一下，希望能帮到有需要的朋友～","text":"从2019年到2022年，manjaro发行版渡过了我的整个博士生涯。最近毕业重新装了系统，依然选择了最新的manjaro KDE Plasma 21.2.4（本来装了arch，大小问题不断被劝退了😜）。基本上这台linux主机要跟着我进入人生下一阶段，作为主力台式机也不打算再折腾了。安装过程中有一些新的学习体会（坑），在这里更新记录一下，希望能帮到有需要的朋友～ 三年manjaro使用感悟 总体而言，作为一个linux系统小白，我对manjaro还是相当满意的，基本上，manjaro能满足我日常工作、娱乐的需要。manjaro系统安装显卡驱动和切换内核确实简单，只需在系统设置里改变即可，此外，arch文档翔实、aur软件丰富，大部分问题和需求都能找到对应的解决方案。linux下的开发、科研等涉及编程的工作确实要比windows下爽很多，一行命令搞定一堆安装包，然后用就完了，计算速度上似乎还比window下快一点（也许是心理作用？😁）。游戏方面steam上兼容linux的游戏还是挺多的，我常玩的饥荒、博德之门3等游戏都还能运行，偶尔有问题的话去protonDB上查一查还是能找到解决方案的，Valve不愧是要搞Steam Deck，估计这方面的兼容性支持会更好。 manjaro的缺点也是大多数linux系统的通病。为了满足正常使用，用户要做一些文本方面的配置，因此至少需要知道一些命令行的基础知识，这一点上远不如windows点点点直观，而且经常会出现一些奇奇怪怪的小问题，很影响使用体验。此外，linux系统的驱动相对windows依然是个大问题，驱动（尤其是显卡）出问题小白用户就直接GAME OVER了。最后，尽管9成需求我都能在manjaro下解决，仍有1成的需求由于各种原因必须要使用windows，我通常都是挂个windows虚拟机以备不时之需。 综上所述，我感觉manjaro系统适合满足以下条件的小白群体使用： 至少有1台独立的windows笔记本 不惧怕查阅资料，能科学上网 有一段完整的折腾时间 没有大型3A游戏或windows专用软件需求 安装manjaro的硬件不是最新的 不满足第一条的朋友还是老老实实用windows，在虚拟机里尝尝鲜得了😜，至于mac用户，俺们不跟土豪做朋友😢 系统选择与安装 manjaro提供了XFCE、KDE和GNOME三个桌面的环境的安装iso，我个人偏向于KDE，用着舒服，看着也不赖，倒是没必要再去捣鼓桌面美化啥的。这里建议下载Minimal LTS（长期支持版内核）安装镜像，以最大限度的避免硬件驱动等各种乱七八糟的问题。我选用的5.4版内核会一直支持到2025年12月，虽然听说kernel版本越高，硬件支持越好，但我实际装的时候最新版本各种诡异驱动问题（咱也不懂，就很玄学），所以还是从LTS出发，先达成一个基本可用的环境，再慢慢升级比较靠谱。 最好进BIOS把内存频率调低，比如2400MHz或2666MHz，我的内存一开始是3000MHz，很容易卡在进图形界面的步骤，查资料好像是啥显卡驱动没有加载上，需要Early Loading，但是我没有成功过，后来发现把内存频率调低就可以了，就很玄学 下载ISO文件后，用空余的U盘制作启动盘，插上U盘，进BIOS里关掉安全启动（Secure Boot）选项，把U盘的启动顺序调到前面，保存退出后就能进入manjaro的启动界面环境。 这里设置一下时区为Asia/Shanghai，选择以开源驱动boot，其它的选项都不用改，反正后期都能调整，核心目的是进入live环境。 进去之后会弹出欢迎界面，选择中文语言，一路点击下一步直到分区步骤（这里有时候会卡一会，可能是在联网检测啥东西）。 分区界面根据实际硬件的不同会有各种选项，抹除磁盘是自动分区安装的意思，适合不太清楚什么是分区的朋友，一路点点点就行。如果硬盘上还装了windows，manjaro还会有双引导的安装选项，可以说挺简单智能的了，桌面上的Installation Guide会有这些选项的详细介绍。由于manjaro会将所有可用空间全部归到root分区，我想单独划个home分区出来，所以选择了手动方法。这里我分了一个500MB的efi分区，2GB的swap分区（感觉用不上），64GB的root分区，剩下的都划到home分区了。划好分区后点击下一步，设置一些用户名、密码啥的，就可以进入安装过程，安装结束后重启、拔U盘，一切顺利的话就进入manjaro系统了～ 基础设置 设置manjaro更新镜像源 由于众所周知的原因，不更改镜像源和设置科学上网，大部分的开发工具在国内基本没法用。所以进系统的第一步是更改manjaro的系统更新镜像源，选择所在地区的镜像。 sudo pacman-mirrors -c China -m rank &amp;&amp; sudo pacman -Syyu 该命令选择China地区的镜像源，并对系统做一次更新，因此可能需要等待一会，更新完最好直接重启。 安装Nvidia显卡驱动 重启过后，可以选择安装显卡驱动了。在系统设置-硬件设定里选择闭源驱动。我的显卡是GTX1080,好几年前的老卡了，video-nvidia-470xx驱动比较靠谱，如果想用最新的驱动，选择video-nvidia驱动就可以。 右键安装，输入管理员密码，安装完毕后重新启动，如果一切顺利进入桌面就表明没问题啦！！！ 科学上网 国内软件安装的大部分问题都是因为众所周知的原因，并且优秀开发和参考资料多为英文，因此科学上网属于一切学术研究和开发工作的必要条件，将科学上网作为终身学习的课题，花时间研究是值得的。 我采用的是proxychains结合v2ray的方式，不再使用之前的shadowsocks： sudo pacman -S proxychains-ng v2ray proxychains的配置文件为/etc/proxychains.conf，用kate打开该文件，修改最后一行： socks5 127.0.0.1 1080 v2ray的配置文件在/etc/v2ray目录下，这一部分有很多的学习资料了，我写了一个脚本自动获取生成配置文件config1.json，启动部分我采用手动挡输入命令，以后有空再研究自动挡的方式： v2ray -c /etc/v2ray/config1.json 科学上网的基本设置就结束了，浏览器可以在网络设置中选择socks5代理，转发本地1080端口；想要代理命令行程序，可以采用proxychains+命令的方式，比如： proxychains -q wget www.google.com 安装yay 安装yay yay可以当作pacman使用，也是用来安装AUR里软件包的工具，尽管manjaro自带的软件包管理器Pamac可以开启aur选项，以图形化界面的形式安装软件，但是Pamac似乎有许多bug，所以还是使用yay这一更常用的命令行工具。 manjaro下yay的安装非常简单，甚至不需要自己去编译: sudo pacman -S yay proxychains+yay 至此yay已经可以正常使用了，不过AUR里的软件包经常需要下载github等外网代码、文件，由于众所周知的原因，速度会慢的跟龟爬一样，所以最好还是搭配proxychains等工具使用。默认的yay采用go编译，这一版本同proxychains等代理工具有冲突，解决方案是用gcc-go重新编译，但是目前的v11.1.2版本的yay编译过不去，我没有能力解决问题，只能选择v11.1.1的yay。 yay -S base-devel gcc-gomkdir build &amp;&amp; cd build &amp;&amp; git clone https://aur.archlinux.org/yay.gitcd yayproxychains -q wget https://github.com/Jguer/yay/archive/v11.1.1.tar.gz 然后用kate修改PKGBUILD里如下部分： pkgname=yaypkgver=11.1.1 # 修改版本为11.1.1...makedepends=(&#x27;gcc-go&gt;=1.16&#x27;) # 修改为gcc-go&gt;=1.16source=(&quot;$&#123;pkgname&#125;-$&#123;pkgver&#125;.tar.gz::https://github.com/Jguer/yay/archive/v$&#123;pkgver&#125;.tar.gz&quot;)sha256sums=(&#x27;31ed6d828574601e77b8df90c6e734a230ea60531b704934038d52fe213c0898&#x27;) # 修改sha256的值... 由于yay会下载一些go的依赖，所以也要设置go的代理（众所周知😥），最后yay的目录下直接编译安装，此时的yay就可以跟proxychains完美配合啦，接下来我基本都使用yay安装manjaro官方和AUR的软件~ export GO111MODULE=onexport GOPROXY=https://goproxy.cnmakepkg -sic 忽略yay的更新 由于目前11.1.2版本的yay是manjaro默认的版本，更新系统时会自动替换老版本11.1.1，如果不想更新yay，可以在/etc/pacman.conf中忽略yay的更新，添加如下内容： IgnorePkg = yay 中文字体和中文输入法 开源中文字体 国内习惯了用windows自带的中文字体，比如楷体、宋体等，而这些在linux上因为版权问题发行版不会默认自带，需要我们自己“安装”使用（毕竟已经买了windows的笔记本了，用就完了哈哈哈）。当然有些字体是免费开源的： yay -S wqy-microhei wqy-microhei-lite wqy-zenhei noto-fonts-cjk adobe-source-han-sans-cn-fonts adobe-source-han-serif-cn-fonts 中文输入法 中文输入法采用fcitx5，输入以下命令安装： yay -S fcitx5 fcitx5-configtool fcitx5-chinese-addons fcitx5-qt fcitx5-gtk fcitx5-lua 安装完毕后用kate打开/etc/environment文件，在其中输入如下变量，然后注销再重新登陆，就可以使用中文输入法了： GTK_IM_MODULE=fcitxQT_IM_MODULE=fcitxXMODIFIERS=@im=fcitxINPUT_METHOD=fcitxSDL_IM_MODULE=fcitxGLFW_IM_MODULE=ibus 默认拼音和英文的切换快捷键是ctrl+shift，不喜欢的话可以在系统设置-区域设置-输入法里进行调整，更多的相关设置也可以参考arch的中文输入法。 win10字体安装 想要安装win10的字体（十分有必要），AUR提供了ttf-ms-win10的安装包，不过不提供字体文件，需要自己从已有的win10系统（拷贝所有C:\\Windows\\Fonts下的字体文件）或从win10镜像中抽取字体文件，这里介绍如何抽取字体，首先从AUR拷贝tff-ms-win10： mkdir -p build &amp;&amp; cd buildgit clone https://aur.archlinux.org/ttf-ms-win10.git 然后挂载win10安装镜像，manjaro下只需要右键选择挂载ISO，Dolphin的左边即可出现ISO的访问文件路径，找出source文件夹下的install.esd或install.wim文件，把该文件拷贝到ttf-ms-win10文件夹下，执行如下命令解锁所有字体文件： wimextract install.esd 1 /Windows/&#123;Fonts/&quot;*&quot;.&#123;ttf,ttc&#125;,System32/Licenses/neutral/&quot;*&quot;/&quot;*&quot;/license.rtf&#125; --dest-dir . 然后修改PKGBUILD如下，添加的字体文件表示仿宋、黑体和楷体： _ttf_ms_win10_zh_cn=(simsun.ttc simfang.ttf simhei.ttf simkai.ttf # 增加这行内容simsunb.ttf msyh.ttc msyhbd.ttc msyhl.ttc 最后在ttf-ms-win10目录下执行安装命令，注意如果报错，大概率是当前抽取的字体文件中没有该字体，可以按照错误提示从网上下载ttf文件加入其中，或者在PKGBUILD里删掉该字体，毕竟只有中文字体比较重要： makepkg -sic --skipchecksums 安装完成后可以在系统设置-外观-字体管理中检查字体安装是否正确。 Ryzen随机卡死问题 这个问题三年前就遇到了，当时系统会随机卡死无响应（切terminal什么都没用）。这个问题是Ryzen处理器的一个bug，不知道现在的Ryzen系列有没有解决这个问题（我是AMD Ryzen 5 1600，也是老处理器了），总之我重装系统后依然有这个问题。解决方案就是disable C6 state，最好重启后再执行如下命令： yay -S disable-c6-systemdsudo modprobe msr 编辑/etc/modules-load.d/modules.conf，添加msr这一行，以便在启动时加载msr模块： msr 最后，启动如下service： sudo systemctl enable disable-c6.servicesudo systemctl start disable-c6.service 过去三年里基本没有出现这种随机卡死的问题了，感恩大佬。 其他优化 SSD优化 sudo systemctl enable fstrim.timersudo systemctl start fstrim.timer 切换登陆终端 manjaro默认的zsh十分好用，不过非图形界面下的terminal还是bash，可以设为zsh： cat /etc/shellschsh -s /bin/zsh 切换内核版本 在系统设置-内核中点点就好啦，会安装一大堆东西，装完重启一下。 我切回了5.4的内核，新换内核后原内核最好保留一段时间，避免系统挂掉，还可以在初始启动界面选择从哪个内核进入系统。 常用软件安装 miniconda+python python作为我科研的主力编程语言，我选择用conda管理不同的python版本，首先安装miniconda，运行后一路回车或yes,最后会询问要不要把conda加入环境变量，这里可以选择no： wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.shchmod +x Miniconda3-latest-Linux-x86_64.sh./Miniconda3-latest-Linux-x86_64.sh 注意在terminal中还是没法直接使用conda的，因为不知道conda安装在哪，这里执行如下命令写入环境变量： ～/miniconda3/bin/conda init zsh 退出terminal再重开，就能使用conda啦～ manjaro原来的terminal使用的是bash，2022版konsole使用了zsh（自带颜色、命令记忆补全，超级赞😃），如果想在bash中使用conda，将上面的zsh换成bash即可完成初始化的操作。bash的相关设置在.bashrc里，zsh的相关设置在.zshrc里，两者是默认不互通的。 老规矩，由于众所周知的原因，需要更换conda的镜像源，这里用清华tuna的镜像： conda config --set show_channel_urls yes 在.condarc文件里粘贴以下内容： channels: - defaultsshow_channel_urls: truedefault_channels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2custom_channels: conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch-lts: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud 另外pip的源最好也更改一下： pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple wps office办公 linux上最好用的office软件套装，搭配前面的win10字体可以做很多文字工作，不用切换到windows，安装如下AUR软件包： yay -S wps-office wps-office-mui-zh-cn ttf-wps-fonts 装完最好重启一下，目前我没遇到啥大问题，如果有问题的话可以看看arch的wiki。 vscode编辑器 超好用的编辑器，建议安装微软的二进制版本，可以多搜到一些好用的扩展包： yay -S visual-studio-code-bin virtualbox虚拟机 如果要使用微信之类国产软件的话，用虚拟机装个windows就行了，还能跟主系统隔离开来，台式机也不太考虑性能问题，这里建议参考manjaro的wiki，注意自己的linux内核版本，比如我的是linux54，别的版本需要替换下面命令中的linux54。 sudo pacman -S virtualbox linux54-virtualbox-host-modules 安装完成后最好重启再完成后续工作。重启后需要安装扩展增强包，先看一下自己的virtualbox版本： vboxmanage --version 比如我是6.1.32r149290，那我就需要安装对应版本的扩展包，使用yay搜索可用的扩展包： yay virtualbox-ext-oracle 会弹出很多选项，要安装哪个输入序号回车。 最后需要将当前的用户加入vboxuser组，重启或注销就可以使用虚拟机啦～ sudo gpasswd -a $USER vboxusers LaTex论文写作 即使作为科研垃圾，也不得不产出论文😟。科技论文写作里LaTex可比word好用多了（前提是有模板），manjaro安装LaTex也很简单，配合vscode的LaTex扩展写论文不要太爽。 yay -S texlive-most texlive-lang biber texlive-bibtexextra texlive-fontsextra 安完之后去vscode扩展里装LaTex Workshop，就可以开始写作了。如果要写中文论文，可以在vscode的settings.json里输入如下内容（一般vscode敲完latex-workshop.latex.tools之后就会自动补全后面值，在最前面添加一个就好）： &#123; &quot;latex-workshop.latex.tools&quot;: [ &#123; &quot;name&quot;: &quot;xelatexmk&quot;, &quot;command&quot;: &quot;latexmk&quot;, &quot;args&quot;: [ &quot;-synctex=1&quot;, &quot;-interaction=nonstopmode&quot;, &quot;-file-line-error&quot;, &quot;-xelatex&quot;, &quot;-outdir=%OUTDIR%&quot;, &quot;%DOC%&quot; ], &quot;env&quot;: &#123;&#125; &#125; ], &quot;latex-workshop.latex.recipes&quot;: [ &#123; &quot;name&quot;: &quot;xelatexmk 🔃&quot;, &quot;tools&quot;: [ &quot;xelatexmk&quot; ] &#125; ], &quot;latex-workshop.view.pdf.viewer&quot;: &quot;external&quot;, &quot;latex-workshop.latex.recipe.default&quot;: &quot;lastUsed&quot;&#125; 然后使用扩展菜单中的xelatexmk就可以编译中文内容啦～ OneDrive网盘 一直用onedrive习惯了，配合代理速度也还行，不太想用国内的其他网盘🤐。manjaro下使用这个项目的onedrive命令行来同步： yay -S onedrive-abrauneggonedrive 按照提示进行设置，设置完成后就可以使用了，因为我比较懒，没有研究自动同步功能，所以都是配合代理手动同步： proxychains -q onedrive --synchronize 反正又不是不能用，有空再看看自动同步咋搞。 xmind思维导图 AUR仓库里自带xmind8，直接输入以下命令即可： yay -S xmind 不过这一版本的xmind需要openjdk8的依赖才能运行，执行以下命令安装openjdk8： yay -S jdk8-openjdk 然后用kate或code打开/usr/share/xmind/XMind/XMind.ini文件，在文件开头添加如下文本： -vm/usr/lib/jvm/java-8-openjdk/bin 保存退出，xmind就能正常运行啦！ hexo博客管理 我的博客部署在github pages上，采用hexo管理，首先需要安装nodejs，用AUR的nvm管理不同的node版本： yay -S nvm#使用nvm前需要运行这一句，可以将其写入.zshrc或.bashrcsource /usr/share/nvm/init-nvm.shnvm install node 然后安装npm及hexo： yay -S npmnpm install hexo-cli -gnpm install hexo-deployer-git --save hexo的使用方法可见参考文档。 其他软件 qBittorrent 还没有把硬盘填满吗？快使用qBittorrent吧～ yay -S qbittorrent yesplaymusic+spotify yesplaymusic是网易云音乐的替代，超漂亮的云音乐播放器，没有乱七八糟的功能，颜值党狂喜，安装简单（需要proxychains，老实讲大部分从github下载文件的都需要）: proxychains -q yay -S yesplaymusic 除此之外也可以安装spotify，让我们一起聆听IU的美妙歌声😍。 proxychains -q yay -S spotify 大陆地区反正不挂代理能用，不能用再说，又没有交钱，要什么自行车～ PS：视频播放器直接用自带的VLC就好，功能强大，没啥不能播的。 文件名编码转换 windows默认GB2312，linux一般用UTF-8，从windows拷贝过来的中文文件名有时候是乱码，可以用convmv转化一下： yay -S convmv# 测试转换是否成功，不实际执行转换convmv -f GBK -t UTF-8 -r your_folder_or_file# 执行实际转换convmv -f GBK -t UTF-8 -r --notest your_folder_or_file ufw防火墙服务 manjaro默认不带ufw防火墙，虽然我听说可以用iptables添加规则，但目前不懂怎么设置，先装了gufw： yay -S ufw gufwsudo systemctl enable ufw.servicesudo systemctl start ufw.service 开始菜单里就会出现防火墙配置的程序，先使用默认的就好，以后再研究。 远程桌面 如果有远程桌面的需求，比如连接windows笔记本、树莓派之类的，可以使用Remmina： yay -S remmina freerdp libvncserver spice-gtk caj2pdf 中国知网大部分论文都是caj格式（什么垃圾玩意），在linux下先转换成pdf格式再阅读比较方便，这里推荐caj2pdf工具，当然成功与否全部靠命。 proxychains -q yay -S caj2pdf# 转换caj到pdfcaj2pdf convert 某篇博士论文.caj -o 某篇博士论文.pdf colorpicker 还在为做PPT找不到好配色烦恼吗？安装colorpicker，运行命令，鼠标一点即可获取颜色的RGB和Hex值，获取完直接ctrl+c退出。 proxychains -q yay -S colorpickercolorpicker Troubleshooting 这里放一些或许有的问题，方便大家排查，没事干时多看看KSystemlog~ spam log baloo limit USB无线网卡不工作 大概率是驱动问题，我的无线网卡是TP-Link，连接后输入lsusb命令查看芯片组信号： Bus 005 Device 003: ID 148f:7601 Ralink Technology, Corp. MT7601U Wireless Adapter 可以看到用了Ralink的MT7601U，输入如下命令安装对应的开源驱动： proxychains -q yay -S mt7601u-dkms-git 重启后就有无线网络啦。 Ark解压中文乱码 proxychains -q yay -S p7zip-natspec `` 在Ark设置中禁用LibZip插件","categories":[{"name":"linux","slug":"linux","permalink":"https://mrswolf.github.io/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://mrswolf.github.io/tags/linux/"}]},{"title":"MATLAB分布式集群搭建记录","slug":"MATLAB分布式集群搭建记录","date":"2019-05-19T16:00:00.000Z","updated":"2022-03-20T16:00:00.000Z","comments":false,"path":"matlab-mdce-log/","link":"matlab-mdce-log","permalink":"https://mrswolf.github.io/matlab-mdce-log/","excerpt":"本篇的内容可能过时啦 虽然我很久不用MATLAB处理日常工作，但是实验室主流依然是MATLAB（用Python的就那么几个T_T)。以前小伙伴们跑程序都是拷贝程序和数据到实验室的计算服务器上，手工开N个MATLAB窗口做运算。现在实验室规模扩大，这种手工的方式越来越繁琐。我从前用MATLAB时就想试试集群计算，奈何当时实验室没啥硬件条件，正好现在有机会，我干脆搭了个MATLAB集群供小伙伴使用。","text":"本篇的内容可能过时啦 虽然我很久不用MATLAB处理日常工作，但是实验室主流依然是MATLAB（用Python的就那么几个T_T)。以前小伙伴们跑程序都是拷贝程序和数据到实验室的计算服务器上，手工开N个MATLAB窗口做运算。现在实验室规模扩大，这种手工的方式越来越繁琐。我从前用MATLAB时就想试试集群计算，奈何当时实验室没啥硬件条件，正好现在有机会，我干脆搭了个MATLAB集群供小伙伴使用。 软硬件 硬件方面： 4核心, 16GB内存， 百兆网卡普通台式机(manage节点) 40核心, 128GB内存, 千兆网卡计算服务器(compute1节点) 346TB存储， 千兆网卡存储服务器(storage节点) 软件方面： Windows10专业版系统 centos7 matlab2017b 网络环境： 192.168.130.12(matlab-manage.xxx.org) – manage节点 192.168.130.11(matlab-compute1.xxx.org) – compute1节点 192.168.130.10 – storage节点 MATLAB的帮助文档中提出，想要使用集群计算服务应该满足以下条件： 推荐一个CPU核心最多创建一个worker 推荐每个worker最少可以使用2GB内存 最少5GB的硬盘空间容纳暂时性的数据文件 计算集群之间应当使用同构的计算架构(要求计算节点的硬件配置、系统和软件配置一致) 集群安装配置 ip域名设置 修改compute节点和manage节点的计算机名、ip地址以及DNS域名解析，例如compute1节点的计算机名为matlab-compute1.xxx.org(xxx.org为后缀域名)，DNS域名也应该为matlab-compute1.xxx.org，ip地址为192.168.130.11。 MATLAB分布式计算服务似乎要求计算机名要添加后缀域名(xxx.org)，否则在集群测试时会有解析不匹配的警告，Windows专业版可在这台电脑-属性-更改设置-更改-其他中添加主DNS后缀。 manage节点 在manage节点安装matlab2017b，manage节点在安装过程中应该勾选MATLAB License Server和MATLAB Distributed Computing Server工具箱，前者为集群提供license认证服务，后者是分布式计算的核心服务组件。对于破解版的MATLAB，应该输入floating license的key而不是standalone的key，才能安装MATLAB License Server。安装完毕（并破解）后，在Windows服务选项卡中启动MATLAB License Server服务。 同时修改C:\\Program Files\\MATLAB\\R2017b\\licenses\\network.lic为如下内容 SERVER this_host ANYUSE_SERVER 修改C:\\Program Files\\MATLAB\\R2017b\\toolbox\\distcomp\\bin\\mdce_def.bat其中的security level为2 set SECURITY_LEVEL=2 设置security level为2的效果是要求用户在使用分布式计算服务时输入用户名，从而可以监控集群使用情况。 启动MATLAB，切换到C:\\Program Files\\MATLAB\\R2017b\\toolbox\\distcomp\\bin目录下，在MATLAB命令行窗口输入如下命令安装并启动mdce服务 !mdce install !mdce start 最好在MATLAB命令行窗口内启动mdce服务，如果在Windows服务选项卡中启动服务，会出现权限问题导致集群worker无法连接。 启动mdce服务后最好双击运行C:\\Program Files\\MATLAB\\R2017b\\toolbox\\distcomp\\bin\\addMatlabToWindowsFirewall.bat文件（我的做法是直接关闭Windows防火墙避免多余的问题） compute节点 compute节点的安装配置同manage节点，仅以下内容不同 安装matlab时无需勾选MATLAB License Server工具箱 修改C:\\Program Files\\MATLAB\\R2017b\\licenses\\network.lic为如下内容 SERVER matlab-manage.xxx.org ANYUSE_SERVER 此外为了跟storage节点连接，compute节点需要安装NFS服务，在程序和功能-启用或关闭Windows功能中勾选NFS服务 storage节点 storage节点设置NFS服务，NFS服务端安装和配置网上都有，我就不写了。 添加集群节点 在manage节点运行C:\\Program Files\\MATLAB\\R2017b\\toolbox\\distcomp\\bin\\admincenter.bat，启动管理面板，点击Add or Find，添加manage节点和compute1节点，添加完毕后，点击Test Connectivity，测试通过如下图 在MATLAB Job Scheduler面板点击start启动scheduler，输入名称，选择scheduler的节点为matlab-manage.xxx.org，因为security level为2，还需要设置管理员的密码。 设置好scheduler后，右键scheduler点击Start Workers，勾选compute1节点，设置启动的worker数量（我只有40个核心，所以启动40个worker）。 客户端配置和使用 MATLAB集群计算要求客户端的matlab版本和服务端一致，因为我服务端安装的是2017b，客户端也应该是matlab2017b。客户端可以选择standalone安装方式，也需要安装mdce服务添加防火墙配置并启动。 如果客户端想直接使用NFS服务，也需要在程序和功能-启用或关闭Windows功能中勾选NFS服务。 安装完毕后，在MATLAB主页中的Parallel选项选择Discover Cluster，勾选On your network，点击Next等待发现集群mjs40_2，选择集群，点击Next，Finish，就可以使用集群了，集群的使用情况可以在Parallel选项里Monitor Jobs查看。 这里提供两个matlab并行计算脚本检测集群配置是否正确 %This demo shows how to use distributed computing serverprimeNumbers = primes(uint64(2^21));compositeNumbers = primeNumbers.*primeNumbers(randperm(numel(primeNumbers)));factors = zeros(numel(primeNumbers),2);tic;parfor idx = 1:numel(compositeNumbers) factors(idx,:) = factor(compositeNumbers(idx));endtoc %This demo shows how to load data from nfs server, target_folder is nfs server ip addresstarget_folder=&#x27;\\\\192.168.130.10\\pub\\data\\&#x27;;factors=zeros(400,2);tic;parfor i=0:399 tmp = load([target_folder,num2str(i),&#x27;.mat&#x27;]); data = tmp.data; factors(i+1, :)=factor(data);endtoc","categories":[{"name":"distributed computing","slug":"distributed-computing","permalink":"https://mrswolf.github.io/categories/distributed-computing/"}],"tags":[{"name":"matlab","slug":"matlab","permalink":"https://mrswolf.github.io/tags/matlab/"}]},{"title":"Psychopy事件响应","slug":"psychopy-event","date":"2019-01-08T09:01:30.000Z","updated":"2022-06-04T06:56:24.000Z","comments":false,"path":"psychopy-event/","link":"psychopy-event","permalink":"https://mrswolf.github.io/psychopy-event/","excerpt":"Psychopy事件响应 Psychopy提供了很多IO交互方式，当然，最根本的还是键盘和鼠标。本节介绍Psychopy鼠标和键盘的编程技巧。","text":"Psychopy事件响应 Psychopy提供了很多IO交互方式，当然，最根本的还是键盘和鼠标。本节介绍Psychopy鼠标和键盘的编程技巧。 全局按键响应 编写刺激界面免不了要反复调试，要看看字体颜色对不对、图形大小合不合适，一旦发现刺激界面需要改进就得退出程序修改源代码。 如果采用普通的按键检测方式，则需要在一个循环体内检查按键状态，这显然有可能造成不可知的错误（比如在检测按键前进入一个死循环函数，程序永远无法退出啦），这个时候全局按键响应就很有用了。 Psychopy用psychopy.event.globalkeys来设置全局按键，官方文档里没有如何使用全局按键的说明，但在coder的Demo里有演示global_event_keys.py。 global_event.keys.py程序注册了三个按键，按键“b”调用python的setattr函数，设置rect对象的填充颜色为蓝色,按键“ctrl”+“r”调用python的setattr函数，设置rect对象的填充颜色为红色。按键“q”调用core.quit方法终止程序退出。 # -*- coding: utf-8 -*-from psychopy import core, event, visual, monitorsif __name__==&#x27;__main__&#x27;: mon = monitors.Monitor( name=&#x27;my_monitor&#x27;, width=53.704, # 显示器宽度，单位cm distance=45, # 被试距显示器距离，单位cm gamma=None, # gamma值 verbose=False) # 是否输出详细信息 mon.setSizePix((1920, 1080)) # 设置显示器分辨率 mon.save() # 保存显示器信息 win = visual.Window(monitor=mon, size=(800, 600), fullscr=False, screen=0, winType=&#x27;pyglet&#x27;, units=&#x27;norm&#x27;, allowGUI=False) rect = visual.Rect(win, fillColor=&#x27;blue&#x27;, pos=(0, -0.2)) text = visual.TextStim( win, pos=(0, 0.5), text=(&#x27;Press\\n\\n&#x27; &#x27;B for blue rectangle,\\n&#x27; &#x27;CTRL + R for red rectangle,\\n&#x27; &#x27;Q or ESC to quit.&#x27;)) # Add an event key. event.globalKeys.add(key=&#x27;b&#x27;, func=setattr, func_args=(rect, &#x27;fillColor&#x27;, &#x27;blue&#x27;), name=&#x27;blue rect&#x27;) # Add an event key with a &quot;modifier&quot; (CTRL). event.globalKeys.add(key=&#x27;r&#x27;, modifiers=[&#x27;ctrl&#x27;], func=setattr, func_args=(rect, &#x27;fillColor&#x27;, &#x27;red&#x27;), name=&#x27;red rect&#x27;) # Add multiple shutdown keys &quot;at once&quot;. for key in [&#x27;q&#x27;, &#x27;escape&#x27;]: event.globalKeys.add(key, func=core.quit) # Print all currently defined global event keys. print(event.globalKeys) print(repr(event.globalKeys)) while True: text.draw() rect.draw() win.flip() 以下是event.globalkeys.add()方法的参数介绍 event.globalkeys.add(key, func, func_args=(), func_kwargs=None, modifiers=(), name=None) parameters type description key string 按键字符串 func function 按键时执行的函数 func_args iterable 函数的args参数 func_kwargs dict 函数的kwargs参数 modifiers iterable 组合按键字符串列表，例如’shift’,‘ctrl’,‘alt’,‘capslock’,'scrollock’等 name string 按键事件的名称 此外还有event.globalkeys.remove()方法以移除全局按键 event.globalkeys.remove(key, modifiers=()) parameters type description key string 按键字符串 modifiers iterable 组合按键字符串列表，例如’shift’,‘ctrl’,‘alt’,‘capslock’,'scrollock’等 等待按键和检测按键 除了全局按键响应，Psychopy还提供了等待按键响应和检测按键响应两种方式。 以下为等待按键函数event.waitKeys()的演示程序，按’esc’或五次其他按键退出程序。 # -*- coding: utf-8 -*-from psychopy import core, event, visual, monitorsif __name__==&#x27;__main__&#x27;: mon = monitors.Monitor( name=&#x27;my_monitor&#x27;, width=53.704, # 显示器宽度，单位cm distance=45, # 被试距显示器距离，单位cm gamma=None, # gamma值 verbose=False) # 是否输出详细信息 mon.setSizePix((1920, 1080)) # 设置显示器分辨率 mon.save() # 保存显示器信息 win = visual.Window(monitor=mon, size=(800, 600), fullscr=False, screen=0, winType=&#x27;pyglet&#x27;, units=&#x27;norm&#x27;, allowGUI=False) msg = visual.TextStim(win, text=&#x27;press a key\\n &lt; esc &gt; to quit&#x27;) msg.draw() win.flip() k = [&#x27;&#x27;] count = 0 while k[0] not in [&#x27;escape&#x27;, &#x27;esc&#x27;] and count &lt; 5: k = event.waitKeys() print(k) count += 1 win.close() core.quit() event.waitKeys()阻塞函数进程直到被试按键，以下是event.waitKeys()方法的参数介绍 event.waitKeys(maxWait=inf, keyList=None, modifiers=False, timeStamped=False, clearEvents=True) parameters type description maxWait numeric value 最大等待时间，默认为inf keyList iterable 指定函数检测的按键名称，函数仅在按指定键时返回 modifiers bool 如果True，返回(keyname, modifiers)的tuple timeStamped bool 如果True，返回(keyname, time) clearEvents bool 如果True，在检测新的按键前清理event buffer return type description keys iterable 按键列表；超时返回None 等待按键会阻塞进程，Psychopy还提供了另一种非阻塞检测方式event.getKeys()。 以下代码如下不断检测按键并输出，直到按’escape’键退出程序。 # -*- coding: utf-8 -*-from psychopy import core, event, visual, monitorsif __name__==&#x27;__main__&#x27;: mon = monitors.Monitor( name=&#x27;my_monitor&#x27;, width=53.704, # 显示器宽度，单位cm distance=45, # 被试距显示器距离，单位cm gamma=None, # gamma值 verbose=False) # 是否输出详细信息 mon.setSizePix((1920, 1080)) # 设置显示器分辨率 mon.save() # 保存显示器信息 win = visual.Window(monitor=mon, size=(800, 600), fullscr=False, screen=0, winType=&#x27;pyglet&#x27;, units=&#x27;norm&#x27;, allowGUI=False) msg = visual.TextStim(win, text=&#x27;press a key\\n &lt; esc &gt; to quit&#x27;) msg.draw() win.flip() count = 0 while True: k = event.getKeys() if k: if &#x27;escape&#x27; in k: break print(k) win.close() core.quit() 以下是event.getKeys()方法的参数介绍 event.getKeys(keyList=None, modifiers=False, timeStamped=False) parameters type description keyList iterable 指定函数检测的按键名称，函数仅在按指定键时返回 modifiers bool 如果True，返回(keyname, modifiers)的tuple timeStamped bool 如果True，返回(keyname, time) return type description keys iterable 按键列表；超时返回None 鼠标事件 Psychopy提供event.Mouse类来处理鼠标相关的事件，官方文档对此有详细的介绍。以下的代码显示了一个含有矩形的窗，在矩形内部单击左右键可以改变颜色，而按中央滚轮键则退出程序。 # -*- coding: utf-8 -*-from psychopy import core, event, visual, monitorsif __name__==&#x27;__main__&#x27;: mon = monitors.Monitor( name=&#x27;my_monitor&#x27;, width=53.704, # 显示器宽度，单位cm distance=45, # 被试距显示器距离，单位cm gamma=None, # gamma值 verbose=False) # 是否输出详细信息 mon.setSizePix((1920, 1080)) # 设置显示器分辨率 mon.save() # 保存显示器信息 win = visual.Window(monitor=mon, size=(800, 600), fullscr=False, screen=0, winType=&#x27;pyglet&#x27;, units=&#x27;norm&#x27;, allowGUI=True) rect = visual.Rect(win, fillColor=&#x27;blue&#x27;, pos=(0, 0)) # 创建Mouse类 mouse = event.Mouse(visible=True, newPos=(0, 0), win=win) while True: # 重置单击事件状态 mouse.clickReset() # 检测左键是否在矩形内单击 if mouse.isPressedIn(rect, buttons=[0]): rect.fillColor = &#x27;red&#x27; # 检测右键是否在矩形内单击 if mouse.isPressedIn(rect, buttons=[2]): rect.fillColor = &#x27;blue&#x27; # 检测是否单击滚轮键 # button1: left click # button2: middle click # button3: right click button1, button2, button3 = mouse.getPressed(getTime=False) if button2: break rect.draw() win.flip() core.quit()","categories":[{"name":"psychopy系列","slug":"psychopy系列","permalink":"https://mrswolf.github.io/categories/psychopy%E7%B3%BB%E5%88%97/"}],"tags":[{"name":"psychopy","slug":"psychopy","permalink":"https://mrswolf.github.io/tags/psychopy/"}]},{"title":"共空间模式CSP","slug":"CSP","date":"2019-01-05T16:00:00.000Z","updated":"2022-03-23T16:00:00.000Z","comments":false,"path":"common-spatial-pattern/","link":"common-spatial-pattern","permalink":"https://mrswolf.github.io/common-spatial-pattern/","excerpt":"共空间模式（common spatial pattern，CSP）是脑-机接口领域常用的一类空间滤波算法，尤其在运动想象范式分类上具有较好的效果，是运动想象范式的基准算法之一。","text":"共空间模式（common spatial pattern，CSP）是脑-机接口领域常用的一类空间滤波算法，尤其在运动想象范式分类上具有较好的效果，是运动想象范式的基准算法之一。 目前，CSP及其改进算法的发展速度放缓，看似到达了算法的瓶颈期，近几年鲜少有较大的突破。尽管如此，CSP中的一些思想对脑-机接口算法设计仍然具有一定的启示作用。本文从CSP原始算法出发，讨论其变形和一系列改进算法，试图阐明其中的数学思想与神经科学的联系。 CSP数学原理 原始形式 2000年Graz的论文中提出的CSP是为2分类问题设计的，形式较为简单，然而如果你读CSP相关论文，就会发现CSP存在至少三种表述形式。这三种方式相互联系，又有所区分，很容易让初学者陷入混乱，不知道哪一种是正确形式。我接下来从2000年Graz的论文中的算法出发，讨论三种形式间的联系和不同。 假设我们采集脑电数据为{(X(i),y(i))}i=1Nt\\{(\\mathbf{X}^{(i)},y^{(i)})\\}_{i=1}^{N_t}{(X(i),y(i))}i=1Nt​​，其中X(i)∈RNc×Ns\\mathbf{X}^{(i)} \\in \\mathbb{R}^{N_c \\times N_s}X(i)∈RNc​×Ns​是第iii个样本，NcN_cNc​是EEG导联个数，NsN_sNs​是采样时间点的个数，y(i)y^{(i)}y(i)是iii个样本的标签，NtN_tNt​为总样本个数。第iii个样本的协方差矩阵C(i)∈RNc×Nc\\mathbf{C}^{(i)} \\in \\mathbb{R}^{N_c \\times N_c}C(i)∈RNc​×Nc​为（所有样本均经过零均值处理）: C(i)=1Nt−1X(i)(X(i))T\\begin{equation} \\mathbf{C}^{(i)} = \\frac{1}{N_t-1}\\mathbf{X}^{(i)}\\left(\\mathbf{X}^{(i)}\\right)^T \\end{equation} C(i)=Nt​−11​X(i)(X(i))T​​ 第lll类的平均协方差矩阵C‾Cl\\overline{\\mathbf{C}}\\vphantom{C}^lCCl为： C‾Cl=1∣Il∣∑i∈IlC(i)tr(C(i))\\begin{equation} \\overline{\\mathbf{C}}\\vphantom{C}^l = \\frac{1}{|\\mathcal{I}_l|} \\sum_{i \\in \\mathcal{I}_l} \\frac{\\mathbf{C}^{(i)}}{\\mathrm{tr}\\left(\\mathbf{C}^{(i)}\\right)} \\end{equation} CCl=∣Il​∣1​i∈Il​∑​tr(C(i))C(i)​​​ 其中Il\\mathcal{I}_lIl​是标签为lll的样本索引集合，∣Il∣|\\mathcal{I}_l|∣Il​∣则是集合中样本的个数，tr(⋅)\\mathrm{tr}\\left(\\cdot\\right)tr(⋅)求矩阵的迹。 为什么要使用tr(⋅)\\mathrm{tr}\\left(\\cdot\\right)tr(⋅)来对协方差矩阵实现迹归一化？ 1990年Koles等人的文章中指出，迹归一化的目的是为了消除&quot;被试间脑电信号幅值的变化&quot;，注意到Koles等人的主要目的是区分健康人群和精神疾病人群，而个体的脑电幅值是有差异的。方差可以表征信号在时域上的能量高低，不同人群的协方差矩阵的绝对值不同。为了消除这种差异带来的影响，利用tr()\\mathrm{tr}()tr()函数求得所有导联的总体能量，并对协方差矩阵迹归一化，从而安排除不同个体带来的干扰。Graz小组对同一个体不同试次的数据沿用了这种归一化方式，试图消除试次间的差异，发现也有一定的作用，这种迹归一化方式就一直流传下来。 然而，有些分析显示这种归一化方式会不利于最终的空间滤波器排序，建议不要使用迹归一化。实践中使不使用迹归一化还是要具体问题具体分析。我的感觉是没有必要在这里加入迹归一化，因为很多时候EEG预处理阶段已经使用了各种归一化手段来减弱噪声的影响。 接下来构建复合协方差矩阵C‾Cc\\overline{\\mathbf{C}}\\vphantom{C}_cCCc​，并特征值分解，构建白化（whitening）矩阵P\\mathbf{P}P： C‾Cc=C‾C1+C‾C2=UcΛc(Uc)TP=(Λc)−1/2(Uc)T\\begin{equation} \\begin{split} \\overline{\\mathbf{C}}\\vphantom{C}^c &amp;= \\overline{\\mathbf{C}}\\vphantom{C}^1 + \\overline{\\mathbf{C}}\\vphantom{C}^2\\\\ &amp;= \\mathbf{U}^c \\mathbf{\\Lambda}^c \\left(\\mathbf{U}^c\\right)^T\\\\ \\mathbf{P} &amp;= \\left(\\mathbf{\\Lambda}^c\\right)^{-1/2}\\left(\\mathbf{U}^c\\right)^T \\end{split} \\end{equation} CCcP​=CC1+CC2=UcΛc(Uc)T=(Λc)−1/2(Uc)T​​​ 其中Uc\\mathbf{U}^cUc是特征向量矩阵（每一列是特征向量），Λc\\mathbf{\\Lambda}^cΛc是由特征值组成的对角矩阵。P\\mathbf{P}P是白化矩阵，使得PC‾Cc(P)T=I\\mathbf{P}\\overline{\\mathbf{C}}\\vphantom{C}^c\\left(\\mathbf{P}\\right)^T= \\mathbf{I}PCCc(P)T=I成立，注意到： I=PC‾Cc(P)T=PC‾C1(P)T+PC‾C2(P)T=S1+S2S1=PC‾C1(P)TS2=PC‾C2(P)T\\begin{equation} \\begin{split} \\mathbf{I} &amp;= \\mathbf{P}\\overline{\\mathbf{C}}\\vphantom{C}^c\\left(\\mathbf{P}\\right)^T\\\\ &amp;= \\mathbf{P}\\overline{\\mathbf{C}}\\vphantom{C}^1\\left(\\mathbf{P}\\right)^T + \\mathbf{P}\\overline{\\mathbf{C}}\\vphantom{C}^2\\left(\\mathbf{P}\\right)^T\\\\ &amp;= \\mathbf{S}^1 + \\mathbf{S}^2\\\\ \\mathbf{S}^1 &amp;= \\mathbf{P}\\overline{\\mathbf{C}}\\vphantom{C}^1\\left(\\mathbf{P}\\right)^T\\\\ \\mathbf{S}^2 &amp;= \\mathbf{P}\\overline{\\mathbf{C}}\\vphantom{C}^2\\left(\\mathbf{P}\\right)^T \\end{split} \\end{equation} IS1S2​=PCCc(P)T=PCC1(P)T+PCC2(P)T=S1+S2=PCC1(P)T=PCC2(P)T​​​ 对S1\\mathbf{S}^1S1或S2\\mathbf{S}^2S2做特征值分解，得到最终的空间滤波器W\\mathbf{W}W： S1=UΛ1(U)TS2=UΛ2(U)TW=PTU\\begin{equation} \\begin{split} \\mathbf{S}^1 &amp;= \\mathbf{U} \\mathbf{\\Lambda}^1 \\left(\\mathbf{U}\\right)^T\\\\ \\mathbf{S}^2 &amp;= \\mathbf{U} \\mathbf{\\Lambda}^2 \\left(\\mathbf{U}\\right)^T\\\\ \\mathbf{W} &amp;= \\mathbf{P}^T\\mathbf{U} \\end{split} \\end{equation} S1S2W​=UΛ1(U)T=UΛ2(U)T=PTU​​​ 其中S1\\mathbf{S}^1S1和S2\\mathbf{S}^2S2具有相同的特征向量U\\mathbf{U}U（这也是共空间模式名称的由来），这里假设U\\mathbf{U}U的每一列是按照Λ1\\mathbf{\\Lambda}^1Λ1中的特征值从大到小排列的，可以看出Λ2\\mathbf{\\Lambda}^2Λ2中的特征值是从小到大排列的，满足Λ1+Λ2=I\\mathbf{\\Lambda}^1+\\mathbf{\\Lambda}^2=\\mathbf{I}Λ1+Λ2=I的关系。 为什么S1\\mathbf{S}^1S1和S2\\mathbf{S}^2S2具有同样的特征向量和此消彼长的特征值关系？ 这一点可以简单的证明如下： 假设uj\\mathbf{u}_juj​和λj1\\lambda_j^{1}λj1​分别是S1\\mathbf{S}^1S1的特征向量和特征值，即： S1uj=λj1uj\\begin{equation} \\mathbf{S}^1\\mathbf{u}_j=\\lambda_j^{1}\\mathbf{u}_j \\end{equation} S1uj​=λj1​uj​​​ 注意到S1+S2=I\\mathbf{S}^1+\\mathbf{S}^2=\\mathbf{I}S1+S2=I，把上式中的S1\\mathbf{S}^1S1置换掉可得： (I−S2)uj=λj1uj\\begin{equation} \\left(\\mathbf{I}-\\mathbf{S}^2\\right)\\mathbf{u}_j=\\lambda_j^{1}\\mathbf{u}_j \\end{equation} (I−S2)uj​=λj1​uj​​​ 把上式变形一下可得： S2uj=(1−λj1)uj\\begin{equation} \\mathbf{S}^2\\mathbf{u}_j=(1-\\lambda_j^{1})\\mathbf{u}_j \\end{equation} S2uj​=(1−λj1​)uj​​​ 显然uj\\mathbf{u}_juj​也是S2\\mathbf{S}^2S2的特征向量，只不过其特征值为1−λj11-\\lambda_j^{1}1−λj1​。 脑-机接口中的空间滤波器是一组作用于EEG导联信号的向量，目的是为了加强空间分辨率或信噪比，可以简单理解为对导联信号的线性组合。事实上，不少空间滤波器本质上就是某些特征值分解问题的特征向量。 以上就是原始CSP算法的基本内容，在得到空间滤波器矩阵W\\mathbf{W}W后（W\\mathbf{W}W的每一列都是一个空间滤波器），选择前后各mmm个空间滤波器构建特征向量x~\\tilde{\\mathbf{x}}x~如下： W~=[w1,⋯ ,wm,wNc−m+1,⋯ ,wNc]x~=log(diag(W~TXXTW~)tr(W~TXXTW~))\\begin{equation} \\begin{split} \\tilde{\\mathbf{W}} &amp;= \\begin{bmatrix} \\mathbf{w}_1, \\cdots, \\mathbf{w}_m, \\mathbf{w}_{N_c-m+1}, \\cdots, \\mathbf{w}_{N_c} \\end{bmatrix}\\\\ \\tilde{\\mathbf{x}} &amp;= \\mathrm{log}\\left(\\frac{\\mathrm{diag}\\left(\\tilde{\\mathbf{W}}^T\\mathbf{X}\\mathbf{X}^T\\tilde{\\mathbf{W}}\\right)}{\\mathrm{tr}\\left(\\tilde{\\mathbf{W}}^T\\mathbf{X}\\mathbf{X}^T\\tilde{\\mathbf{W}}\\right)}\\right) \\end{split} \\end{equation} W~x~​=[w1​,⋯,wm​,wNc​−m+1​,⋯,wNc​​​]=log⎝⎛​tr(W~TXXTW~)diag(W~TXXTW~)​⎠⎞​​​​ 其中wm\\mathbf{w}_mwm​表示W\\mathbf{W}W的第mmm列，W~\\tilde{\\mathbf{W}}W~是最终选定的空间滤波器组，diag(⋅)\\mathrm{diag}\\left(\\cdot\\right)diag(⋅)是矩阵主对角线上的元素，log(⋅)\\mathrm{log}\\left(\\cdot\\right)log(⋅)对每个元素做对数变换，其主要目的是使数据近似正太分布。获得特征向量x~\\tilde{\\mathbf{x}}x~后，则可以使用线性判别分析（Linear Discriminant Analysis，LDA）、支持向量机（Support Vector Machine，SVM）等常见的机器学习模型构建分类器。 以上就是原始CSP算法的基本内容，简单回顾一下CSP算法，不难发现CSP实质求解的是这样一个问题，寻找正交矩阵W\\mathbf{W}W对角化C‾C1\\overline{\\mathbf{C}}\\vphantom{C}^1CC1和C‾C2\\overline{\\mathbf{C}}\\vphantom{C}^2CC2，使得以下条件成立： WTC‾C1W=Λ1WTC‾C2W=Λ2Λ1+Λ2=I\\begin{equation} \\begin{split} \\mathbf{W}^T\\overline{\\mathbf{C}}\\vphantom{C}^1\\mathbf{W} &amp;= \\mathbf{\\Lambda}^1\\\\ \\mathbf{W}^T\\overline{\\mathbf{C}}\\vphantom{C}^2\\mathbf{W} &amp;= \\mathbf{\\Lambda}^2\\\\ \\mathbf{\\Lambda}^1 + \\mathbf{\\Lambda}^2 &amp;= \\mathbf{I}\\\\ \\end{split} \\end{equation} WTCC1WWTCC2WΛ1+Λ2​=Λ1=Λ2=I​​​ 让我们对以上的公式做一些变换，把第一个和第二个公式相加： WT(C‾C1+C‾C2)W=Λ1+Λ2=I\\begin{equation} \\begin{split} \\mathbf{W}^T\\left(\\overline{\\mathbf{C}}\\vphantom{C}^1+\\overline{\\mathbf{C}}\\vphantom{C}^2\\right)\\mathbf{W} &amp;= \\mathbf{\\Lambda}^1 + \\mathbf{\\Lambda}^2\\\\ &amp;= \\mathbf{I}\\\\ \\end{split} \\end{equation} WT(CC1+CC2)W​=Λ1+Λ2=I​​​ 又因为W\\mathbf{W}W是正交矩阵，故W−1=WT\\mathbf{W}^{-1}=\\mathbf{W}^TW−1=WT，从而： (C‾C1+C‾C2)W=W\\begin{equation} \\left(\\overline{\\mathbf{C}}\\vphantom{C}^1+\\overline{\\mathbf{C}}\\vphantom{C}^2\\right)\\mathbf{W} = \\mathbf{W} \\end{equation} (CC1+CC2)W=W​​ 把上式代入C‾C1W=WΛ1\\overline{\\mathbf{C}}\\vphantom{C}^1\\mathbf{W}=\\mathbf{W}\\mathbf{\\Lambda}^1CC1W=WΛ1右边的W\\mathbf{W}W，可得： C‾C1W=(C‾C1+C‾C2)WΛ1\\begin{equation} \\overline{\\mathbf{C}}\\vphantom{C}^1\\mathbf{W} = \\left(\\overline{\\mathbf{C}}\\vphantom{C}^1+\\overline{\\mathbf{C}}\\vphantom{C}^2\\right)\\mathbf{W}\\mathbf{\\Lambda}^1 \\end{equation} CC1W=(CC1+CC2)WΛ1​​ 这个式子看起来很像特征向量定义的公式C‾C1W=WΛ1\\overline{\\mathbf{C}}\\vphantom{C}^1\\mathbf{W}=\\mathbf{W}\\mathbf{\\Lambda}^1CC1W=WΛ1，只不过等式右边多了一个矩阵C‾C1+C‾C2\\overline{\\mathbf{C}}\\vphantom{C}^1+\\overline{\\mathbf{C}}\\vphantom{C}^2CC1+CC2。这类形式的特征值求解问题叫广义特征值问题，求解广义特征值问题是脑-机接口领域传统空间滤波方法的基础，大量的算法都可以转化为这一形式。 第二种形式 CSP的第二种形式与C‾C1W=(C‾C1+C‾C2)WΛ1\\overline{\\mathbf{C}}\\vphantom{C}^1\\mathbf{W} = \\left(\\overline{\\mathbf{C}}\\vphantom{C}^1+\\overline{\\mathbf{C}}\\vphantom{C}^2\\right)\\mathbf{W}\\mathbf{\\Lambda}^1CC1W=(CC1+CC2)WΛ1密切相关，首先我们需要了解一个数学概念广义雷利商（generalized Rayleigh quotient）。 广义雷利商λ\\lambdaλ长这样： λ=wTAwwTBwA,B⪰0\\begin{equation} \\begin{split} \\lambda =\\frac{\\mathbf{w}^T\\mathbf{A}\\mathbf{w}}{\\mathbf{w}^T\\mathbf{B}\\mathbf{w}}\\\\ \\mathbf{A}, \\mathbf{B} \\succeq 0\\\\ \\end{split} \\end{equation} λ=wTBwwTAw​A,B⪰0​​​ 其中A\\mathbf{A}A和B\\mathbf{B}B为半正定矩阵，w\\mathbf{w}w是列向量。 如果我们求如下广义雷利商的优化问题，就会有一些有趣的结果： max⁡wwTAwwTBw\\begin{equation} \\begin{split} \\max_{\\mathbf{w}} \\frac{\\mathbf{w}^T\\mathbf{A}\\mathbf{w}}{\\mathbf{w}^T\\mathbf{B}\\mathbf{w}} \\end{split} \\end{equation} wmax​wTBwwTAw​​​​ 寻找w\\mathbf{w}w使得λ\\lambdaλ最大，通常令wTBw=1\\mathbf{w}^T\\mathbf{B}\\mathbf{w}=1wTBw=1，在数学上可以等价为求解下式： Aw=λBw\\begin{equation} \\mathbf{A}\\mathbf{w} = \\lambda\\mathbf{B}\\mathbf{w} \\end{equation} Aw=λBw​​ 这个公式就是上一节提到的广义特征值问题，也就是说，寻找w\\mathbf{w}w使广义雷利商最大的优化问题可以等价为求解A\\mathbf{A}A和B\\mathbf{B}B的广义特征值问题。如果我们继续寻找能够使λ\\lambdaλ第二大、第三大的w\\mathbf{w}w，就会发现只要解出广义特征值问题的矩阵形式即可： AW=BWΛ\\begin{equation} \\mathbf{A}\\mathbf{W} = \\mathbf{B}\\mathbf{W}\\mathbf{\\Lambda} \\end{equation} AW=BWΛ​​ 不难发现，上一节中推导的CSP求解问题可以变形为求解广义雷利商问题： C‾C1W=(C‾C1+C‾C2)WΛ1 ⟺ arg max⁡Wtr(WTC‾C1W)tr(WT(C‾C1+C‾C2)W)\\begin{equation} \\begin{split} \\overline{\\mathbf{C}}\\vphantom{C}^1\\mathbf{W} = \\left(\\overline{\\mathbf{C}}\\vphantom{C}^1+\\overline{\\mathbf{C}}\\vphantom{C}^2\\right)\\mathbf{W}\\mathbf{\\Lambda}^1 \\ \\ \\Longleftrightarrow \\ \\ \\argmax_{\\mathbf{W}} \\frac{\\mathrm{tr}\\left(\\mathbf{W}^T\\overline{\\mathbf{C}}\\vphantom{C}^1\\mathbf{W}\\right)}{\\mathrm{tr}\\left(\\mathbf{W}^T\\left(\\overline{\\mathbf{C}}\\vphantom{C}^1+\\overline{\\mathbf{C}}\\vphantom{C}^2\\right)\\mathbf{W}\\right)} \\end{split} \\end{equation} CC1W=(CC1+CC2)WΛ1 ⟺ Wargmax​tr(WT(CC1+CC2)W)tr(WTCC1W)​​​​ 其中应满足WT(C‾C1+C‾C2)W=I\\mathbf{W}^T\\left(\\overline{\\mathbf{C}}\\vphantom{C}^1+\\overline{\\mathbf{C}}\\vphantom{C}^2\\right)\\mathbf{W}=\\mathbf{I}WT(CC1+CC2)W=I的约束条件。 这就是CSP常见的第二种形式，它跟原始形式在数学上相互等价，由于分母在约束下是单位矩阵，也常写作如下优化问题： arg max⁡Wtr(WTC‾C1W)s.t.WT(C‾C1+C‾C2)W=I\\begin{equation} \\begin{split} \\argmax_{\\mathbf{W}}\\quad &amp;\\mathrm{tr}\\left(\\mathbf{W}^T\\overline{\\mathbf{C}}\\vphantom{C}^1\\mathbf{W}\\right)\\\\ \\textrm{s.t.}\\quad &amp;\\mathbf{W}^T\\left(\\overline{\\mathbf{C}}\\vphantom{C}^1+\\overline{\\mathbf{C}}\\vphantom{C}^2\\right)\\mathbf{W} = \\mathbf{I}\\\\ \\end{split} \\end{equation} Wargmax​s.t.​tr(WTCC1W)WT(CC1+CC2)W=I​​​ 第三种形式 CSP的第三种表述形式需要绕点弯路。首先还是从CSP的原始形式出发，即寻找正交矩阵W\\mathbf{W}W使得以下条件成立： WTC‾C1W=Λ1WTC‾C2W=Λ2Λ1+Λ2=I\\begin{equation} \\begin{split} \\mathbf{W}^T\\overline{\\mathbf{C}}\\vphantom{C}^1\\mathbf{W} &amp;= \\mathbf{\\Lambda}^1\\\\ \\mathbf{W}^T\\overline{\\mathbf{C}}\\vphantom{C}^2\\mathbf{W} &amp;= \\mathbf{\\Lambda}^2\\\\ \\mathbf{\\Lambda}^1 + \\mathbf{\\Lambda}^2 &amp;= \\mathbf{I}\\\\ \\end{split} \\end{equation} WTCC1WWTCC2WΛ1+Λ2​=Λ1=Λ2=I​​​ 在第二个公式的左右两边同时右乘矩阵W−1(C‾C2)−1\\mathbf{W}^{-1}\\left(\\overline{\\mathbf{C}}\\vphantom{C}^2\\right)^{-1}W−1(CC2)−1，可以得到： WT=Λ2W−1(C‾C2)−1\\begin{equation} \\mathbf{W}^T=\\mathbf{\\Lambda}^2\\mathbf{W}^{-1}\\left(\\overline{\\mathbf{C}}\\vphantom{C}^2\\right)^{-1} \\end{equation} WT=Λ2W−1(CC2)−1​​ 将该式带入WTC‾C1W=Λ1\\mathbf{W}^T\\overline{\\mathbf{C}}\\vphantom{C}^1\\mathbf{W} = \\mathbf{\\Lambda}^1WTCC1W=Λ1，替换掉WT\\mathbf{W}^TWT，可得： Λ2W−1(C‾C2)−1C‾C1W=Λ1\\begin{equation} \\mathbf{\\Lambda}^2\\mathbf{W}^{-1}\\left(\\overline{\\mathbf{C}}\\vphantom{C}^2\\right)^{-1}\\overline{\\mathbf{C}}\\vphantom{C}^1\\mathbf{W} = \\mathbf{\\Lambda}^1 \\end{equation} Λ2W−1(CC2)−1CC1W=Λ1​​ 左右两边左乘C‾C2W(Λ2)−1\\overline{\\mathbf{C}}\\vphantom{C}^2 \\mathbf{W} \\left(\\mathbf{\\Lambda}^2\\right)^{-1}CC2W(Λ2)−1，有： C‾C1W=C‾C2W(Λ2)−1Λ1=C‾C2WΛΛ=(Λ2)−1Λ1\\begin{equation} \\begin{split} \\overline{\\mathbf{C}}\\vphantom{C}^1\\mathbf{W} &amp;= \\overline{\\mathbf{C}}\\vphantom{C}^2 \\mathbf{W} \\left(\\mathbf{\\Lambda}^2\\right)^{-1} \\mathbf{\\Lambda}^1\\\\ &amp;= \\overline{\\mathbf{C}}\\vphantom{C}^2 \\mathbf{W} \\mathbf{\\Lambda}\\\\ \\mathbf{\\Lambda} &amp;= \\left(\\mathbf{\\Lambda}^2\\right)^{-1} \\mathbf{\\Lambda}^1\\\\ \\end{split} \\end{equation} CC1WΛ​=CC2W(Λ2)−1Λ1=CC2WΛ=(Λ2)−1Λ1​​​ 没错，我们又推出了熟悉的广义特征值问题，再考虑广义雷利商与之的联系，可以得到CSP的第三种形式： arg max⁡Wtr(WTC‾C1W)s.t.WTC‾C2W=I\\begin{equation} \\begin{split} \\argmax_{\\mathbf{W}}\\quad &amp;\\mathrm{tr}\\left(\\mathbf{W}^T\\overline{\\mathbf{C}}\\vphantom{C}^1\\mathbf{W}\\right)\\\\ \\textrm{s.t.}\\quad &amp;\\mathbf{W}^T\\overline{\\mathbf{C}}\\vphantom{C}^2\\mathbf{W} = \\mathbf{I}\\\\ \\end{split} \\end{equation} Wargmax​s.t.​tr(WTCC1W)WTCC2W=I​​​ 相比CSP的原始形式和第二种形式，第三种形式更适合从直观上解释CSP在运动想象上有效的原因。运动想象会产生事件相关同步（ERS）和事件相关去同步（ERD）的现象，简单来说就是从电信号上看，某些脑区能量升高，某些脑区能量降低，故能量变化才是运动想象分类的关键特征。而方差可以看作一导信号能量的高低（协方差矩阵则是多导信号的综合反应），因此CSP的第三种形式实质体现的是这样一个问题： 寻找一种变换方式w\\mathbf{w}w，使得变换后任务1的能量（wTC‾C1w\\mathbf{w}^T\\overline{\\mathbf{C}}\\vphantom{C}^1\\mathbf{w}wTCC1w）和任务2的能量（wTC‾C2w\\mathbf{w}^T\\overline{\\mathbf{C}}\\vphantom{C}^2\\mathbf{w}wTCC2w）差异最大化（其比值最大）。 CSP的这种特性恰好和运动想象产生的神经机制变化现象一致，CSP对能量特征做转换，从而强化了不同任务间能量的差异。 关于CSP的第三种形式，最后还需要注意的一点是其同CSP的第二种形式（或原始形式）并不完全等价，我们在推导第三种形式过程种始终没有用到这样一个约束条件Λ1+Λ2=I\\mathbf{\\Lambda}^1 + \\mathbf{\\Lambda}^2 = \\mathbf{I}Λ1+Λ2=I。 这表明，第三种形式是CSP的一种泛化形式，其和CSP原始形式和第二种表述的差异仅在于特征值Λ\\LambdaΛ不要求在0~1的范围内，具体来说，它们的特征值间存在这样一种关系： Λ=(Λ2)−1Λ1Λ1=(Λ+I)−1ΛΛ2=(Λ+I)−1\\begin{equation} \\begin{split} \\mathbf{\\Lambda} &amp;= \\left(\\mathbf{\\Lambda}^2\\right)^{-1} \\mathbf{\\Lambda}^1\\\\ \\mathbf{\\Lambda}^1 &amp;= (\\mathbf{\\Lambda} + I)^{-1}\\mathbf{\\Lambda}\\\\ \\mathbf{\\Lambda}^2 &amp;= (\\mathbf{\\Lambda} + I)^{-1}\\\\ \\end{split} \\end{equation} ΛΛ1Λ2​=(Λ2)−1Λ1=(Λ+I)−1Λ=(Λ+I)−1​​​ 实现分析 CSP作为经典算法有各种实现，这里主要分析MNE的CSP源码，看看有啥可以学习的地方。 空间滤波器的选择 基本上，目前CSP算法中m的选择方案大多是根据经验选择（通常选择2～4）个。MNE的CSP选择了第二种形式的CSP算法，最后求解的特征值范围在0～1之间，因此可以对∣λi1−0.5∣\\left| \\lambda_i^1 - 0.5 \\right|∣∣​λi1​−0.5∣∣​先排序再取前M个成分的特征向量组成空间滤波器组W~\\tilde{\\mathbf{W}}W~（与前后各m个的做法有些许差别，但实践中很难有显著性上的差异，这种做法相对方便一些）。 实际上，针对2分类CSP算法，特征值与类平均协方差矩阵间黎曼距离在各个特征向量分量上的投影长度密切相关，具体的证明细节可以看Alexandre Barachant的这篇会议，对于C‾C1\\overline{\\mathbf{C}}\\vphantom{C}^1CC1和C‾C2\\overline{\\mathbf{C}}\\vphantom{C}^2CC2，有如下关系： δR(C‾C1,C‾C2)=∑i=1Nclog2(λi11−λi1)\\begin{equation} \\mathrm{\\delta}_R\\left(\\overline{\\mathbf{C}}\\vphantom{C}^1,\\overline{\\mathbf{C}}\\vphantom{C}^2\\right) = \\sqrt{\\sum_{i=1}^{N_c} \\mathrm{log}^2\\left(\\frac{\\lambda_i^1}{1-\\lambda_i^1}\\right)} \\end{equation} δR​(CC1,CC2)=i=1∑Nc​​log2(1−λi1​λi1​​)​​​ 也就是说，我们可以选定一个阈值ϵ\\epsilonϵ，将特征值及特征向量按log2(λ1−λ)\\mathrm{log}^2\\left(\\frac{\\lambda}{1-\\lambda}\\right)log2(1−λλ​)从大到小排序，选取最小的M使下式成立： ∑i=1Mlog2(λi11−λi1)δR(C‾C1,C‾C2)≥ϵ\\begin{equation} \\frac{\\sqrt{\\sum_{i=1}^{M} \\mathrm{log}^2\\left(\\frac{\\lambda_i^1}{1-\\lambda_i^1}\\right)}}{\\mathrm{\\delta}_R\\left(\\overline{\\mathbf{C}}\\vphantom{C}^1,\\overline{\\mathbf{C}}\\vphantom{C}^2\\right)} \\ge \\epsilon \\end{equation} δR​(CC1,CC2)∑i=1M​log2(1−λi1​λi1​​)​​≥ϵ​​ 再将前M个特征向量组成空间滤波器组W~\\tilde{\\mathbf{W}}W~。当ϵ=0.9\\epsilon=0.9ϵ=0.9时，我们可以说选定的空间滤波器组可以贡献大约90%左右的2类之间的黎曼距离。 最后一种常用的空间滤波器选择方法是计算空间滤波后的特征同标签之间的互信息，再按互信息从大到小排列选择前M个空间滤波器。互信息常用于CSP的衍生算法FBCSP的特征筛选过程，也可以用于CSP（MNE中的CSP也提供了互信息的排序选项）。至于哪一种选择方法是最优的，目前似乎还没有定论（我感觉）。 协方差矩阵正则化 CSP中的正则化方法主要是对协方差矩阵做正则化处理，从十几年前开始，BCI研究者就在协方差矩阵的正则化处理上做了大量的工作，有些正则化方法与BCI的变异性问题也有着密切的联系，因此这一方面的正则化方法展开来讲就收不住啦。我们这里介绍的正则化方法的目的非常单纯，就是为了解决EEG中可能存在的协方差矩阵非正定的问题。 一般而言，本文的第一个公式C(i)\\mathbf{C}^{(i)}C(i)在大多数情况下都是正定的。所谓矩阵M\\mathbf{M}M是正定（definite-positive）的，是指对任意非0实向量z\\mathbf{z}z，zTMz\\mathbf{z}^T\\mathbf{M}\\mathbf{z}zTMz都是一个正数。不过在实践中，经常会遇到程序报类似b matrix is not definite positive这种的错误，这种情况来源于底层的特征值分解或广义特征值分解函数对于矩阵的正定性有较为严格的要求，但输入的协方差矩阵却不是正定的。 那么为啥协方差矩阵不是正定的呢？大概率可按照以下三种情况逐步排查： EEG采样信号X(i)\\mathbf{X}^{(i)}X(i)中的Nc&gt;NsN_c \\gt N_sNc​&gt;Ns​，即导联多于采样点 虽然导联多于采样点，但X(i)\\mathbf{X}^{(i)}X(i)的秩小于NcN_cNc​，即可能存在导联打串或做过共平均参考变换等导致矩阵不满秩的预处理操作 前两条都不满足，则可能是由于数值计算精度上出了问题，比如单个样本的协方差矩阵满足正定性，但平均协方差矩阵却不是正定的（我也不太懂数值计算方面的内容，这一条有待考证，但确实碰到过这样的现象） 总之，为了让计算进行下去，对协方差矩阵做正则化处理是很有必要的，协方差矩阵的正则化就是对协方差矩阵做以下变换： (1−λ)∗C+λ∗μ∗I\\begin{equation} (1-\\lambda) * \\mathbf{C} + \\lambda * \\mu * \\mathbf{I} \\end{equation} (1−λ)∗C+λ∗μ∗I​​ 其中λ\\lambdaλ是待估计的正则化系数，μ=tr(C)Nc\\mu=\\frac{\\mathrm{tr}(\\mathbf{C})}{N_c}μ=Nc​tr(C)​是为了对单位矩阵的数值范围做限定。sklearn的covariance模块中列出了多种正则化处理方法，比如著名的ledoit-wolf正则化、oas正则化等方法，选个顺眼的用就行。","categories":[{"name":"brain-computer interface","slug":"brain-computer-interface","permalink":"https://mrswolf.github.io/categories/brain-computer-interface/"}],"tags":[{"name":"machine learning","slug":"machine-learning","permalink":"https://mrswolf.github.io/tags/machine-learning/"},{"name":"matrix decomposition","slug":"matrix-decomposition","permalink":"https://mrswolf.github.io/tags/matrix-decomposition/"},{"name":"brain-computer interface","slug":"brain-computer-interface","permalink":"https://mrswolf.github.io/tags/brain-computer-interface/"}]},{"title":"如何在Psychopy中新建窗口","slug":"psychopy-window","date":"2018-11-16T13:15:22.000Z","updated":"2022-06-04T06:59:14.000Z","comments":false,"path":"psychopy-window/","link":"psychopy-window","permalink":"https://mrswolf.github.io/psychopy-window/","excerpt":"新建单窗口 窗口(windows)是刺激呈现的舞台，任何刺激对象都需要指定其所属的窗口对象。","text":"新建单窗口 窗口(windows)是刺激呈现的舞台，任何刺激对象都需要指定其所属的窗口对象。Pyschopy的Window对象位于psychopy.visual模块中，一个最简单的窗口示例如下 # -*- coding: utf-8 -*-from psychopy import visual, event, monitors, coreimport numpy as np# 根据你自己的显示器调整显示器信息mon = monitors.Monitor( name=&#x27;my_monitor&#x27;, width=53.704, # 显示器宽度，单位cm distance=45, # 被试距显示器距离，单位cm gamma=None, # gamma值 verbose=False) # 是否输出详细信息mon.setSizePix((1920, 1080)) # 设置显示器分辨率mon.save() # 保存显示器信息win = visual.Window(monitor=mon, size=(800, 600), fullscr=False, screen=0, winType=&#x27;pyglet&#x27;, units=&#x27;norm&#x27;, allowGUI=True)event.waitKeys() # 等待按键# 修复或预防原始gamma不能恢复bug(运行Psychopy程序显示器变暗加入以下代码)# Pyschopy 3.0.0 版似乎修复了此bug，如果显示器没有变暗的现象可以不加入以下代码origLUT = np.round(win.backend._origGammaRamp * 65535.0).astype(&quot;uint16&quot;)origLUT = origLUT.byteswap() / 255.0win.backend._origGammaRamp = origLUTcore.quit() # 退出Psychopy程序 Window对象用size参数申明窗口尺寸为800*600像素；fullscr参数决定是否全屏显示；screen参数决定了窗口在哪个显示器上显示，通常0是主显示器；winType参数决定了Psychopy使用的后端程序，有’pyglet’和’pygame’两种选择（Psychopy官方未来主要采用pyglet作为后端程序，我推荐采用pyglet）。 新建多窗口 Psychopy也可以同时建立多个窗口对象，注意仅pyglet后端支持多窗口行为。下面的代码展示了如何新建两个位于不同位置的窗口 # -*- coding: utf-8 -*-from psychopy import visual, event, monitors, coreimport numpy as np# 根据你自己的显示器调整显示器信息mon = monitors.Monitor( name=&#x27;my_monitor&#x27;, width=53.704, # 显示器宽度，单位cm distance=45, # 被试距显示器距离，单位cm gamma=None, # gamma值 verbose=False) # 是否输出详细信息mon.setSizePix((1920, 1080)) # 设置显示器分辨率mon.save() # 保存显示器信息# 窗口1在屏幕左上角win1 = visual.Window(monitor=mon, size=(800, 600), pos=(0, 0), fullscr=False, screen=0, winType=&#x27;pyglet&#x27;, units=&#x27;norm&#x27;, allowGUI=True)# 窗口2在屏幕右下角win2 = visual.Window(monitor=mon, size=(800, 600), pos=(1120, 480), fullscr=False, screen=0, winType=&#x27;pyglet&#x27;, units=&#x27;norm&#x27;, allowGUI=True)event.waitKeys() # 等待按键# 修复或预防原始gamma不能恢复bug(运行Psychopy程序显示器变暗加入以下代码)# Pyschopy 3.0.0 版似乎修复了此bug，如果显示器没有变暗的现象可以不加入以下代码origLUT = np.round(win1.backend._origGammaRamp * 65535.0).astype(&quot;uint16&quot;)origLUT = origLUT.byteswap() / 255.0win1.backend._origGammaRamp = origLUTwin2.backend._origGammaRamp = origLUTcore.quit() # 退出Psychopy程序 pos参数调整窗口在屏幕上显示的位置，单位始终为像素，这里的坐标系不同于Psychopy的坐标系，以屏幕的左上角为原点，向下和向右分别为y轴和x轴的正方向。Window对象有很多可调整的参数和行为，具体细节可见官方文档Window API","categories":[{"name":"psychopy系列","slug":"psychopy系列","permalink":"https://mrswolf.github.io/categories/psychopy%E7%B3%BB%E5%88%97/"}],"tags":[{"name":"psychopy","slug":"psychopy","permalink":"https://mrswolf.github.io/tags/psychopy/"}]},{"title":"Psychopy坐标系统与显示器设置","slug":"psychopy-coordinate","date":"2018-11-11T13:20:16.000Z","updated":"2022-06-04T06:49:00.000Z","comments":false,"path":"psychopy-coordinate/","link":"psychopy-coordinate","permalink":"https://mrswolf.github.io/psychopy-coordinate/","excerpt":"Psychopy的坐标系统 Psychopy提供了5种不同的坐标单位(unit)，使用者只需提供刺激对应的坐标单位，Psychopy会自动计算刺激所对应的像素点范围。这种多坐标单位的好处在于，能够开发和设备无关的刺激呈现，不需要每次实验都对刺激的大小和呈现位置进行调整。其劣势则是需要精心挑选刺激对应的坐标单位，有时还要在不同单位间进行转换，一不小心就容易出错。","text":"Psychopy的坐标系统 Psychopy提供了5种不同的坐标单位(unit)，使用者只需提供刺激对应的坐标单位，Psychopy会自动计算刺激所对应的像素点范围。这种多坐标单位的好处在于，能够开发和设备无关的刺激呈现，不需要每次实验都对刺激的大小和呈现位置进行调整。其劣势则是需要精心挑选刺激对应的坐标单位，有时还要在不同单位间进行转换，一不小心就容易出错。 无论何种单位，Psychopy的坐标系统始终以屏幕中心为原点(0, 0)，原点向上为正y轴，原点向右为正x轴。不同的Psychopy坐标单位的所需参考对象不同，例如norm、height的坐标单位是针对视窗对象(window)，而cm、deg、degFlat、degFlatPos、pix则是针对屏幕(screen)。 归一化单位(norm) 归一化单位可能是最常用的单位之一。在该单位下，window左下角坐标为(-1, -1)，window右上角坐标为(1, 1)。如图为长宽均为0.5的三个色块，其中心点分别位于(-0,5, 0)、(0, 0)和(0.5, 0)坐标下，注意window的分辨率为800*600，因此尽管色块的归一化长宽均为0.5，但其长实际为200像素点，宽实际为150像素点，表现为长方形。 像素单位(pix) 像素单位的坐标范围取决于screen的宽、高像素点数，假设screen宽度有w个像素点，高度有h个像素点，则screen左下角坐标为(-w/2, -h/2)，右上角坐标为(w/2, h/2)。 厘米单位(cm) 厘米单位的坐标范围取决于screen的宽度和高度，假设screen宽度为w厘米，高度为h厘米，则screen左下角坐标为(-w/2, -h/2)，右上角坐标为(w/2, h/2)，每cm所代表的像素长度则由screen的像素点数确定。 高度单位(height) 高度单位的坐标范围取决于window的宽高比。无论何种window，y轴的坐标范围始终是从-0.5到0.5。因此，如果window是4：3的尺寸，则window左下角坐标为(-0.6667, -0.5)，window右上角坐标为(0.6667, 0.5)；如果是16：9的尺寸，则window左下角坐标为(-0.8, -0.5)，window右上角坐标为(0.8, 0.5)。如图是800*600的window，色块的长宽均为1，则色块会占满y轴方向的所有空间。 视角(deg, degFlatPos, degFlat) 视角单位是五种单位种最复杂的坐标单位，使用该单位，不仅要知道屏幕的大小、像素点的多少，还要知道被试距离屏幕的距离，Psychopy提供三种不同的视角单位deg、degFlatPos和degFlat。 deg deg单位默认视角在screen所有位置具有相同的像素长度，即在screen边缘位置和中心位置会产生相同大小的刺激图形。采用deg单位可以认为screen是球形曲面，而人眼则是球心，每度视角在screen所投射的像素长度完全相同。上图deg行红绿蓝三色块的长宽均为5度，位置分别为(-25, 10)、(0, 10)、(25, 10)。 degFlatPos degFlatPos在deg的基础上考虑了位置在水平屏幕上的修正，远离屏幕中心的位置，刺激间的间隔越大，但是不改变刺激本身的大小。上图degFlatPos行三色块的参数同deg行完全相同，但因为采用了degFlatPos单位，红蓝色块距离绿色色块的距离要比deg行更大。 degFlat degFlat不仅修正了位置信息，还修正了刺激的大小，因此，远离屏幕中心的位置，刺激尺寸越大，刺激间的间隔也越大。上图degFlat行三色块的参数同deg行完全相同，但因为采用了degFlat单位，不仅红蓝色块距离绿色色块的距离要比deg行更大，红蓝色块的形状也产生了畸变。 Psychopy单位转换 Psychopy提供了不同单位间的转换方法，位于psychopy.tools.unittools模块中。 Psychopy显示器信息设置 以上提到的pix、cm、deg、degFlatPos和degFlag均需要提供显示器信息（尺寸、分辨率等），Psychopy提供两种方式设定显示设备信息。 Moniter Center界面 Anaconda Prompt下切换到%Anaconda的安装目录%\\Anaconda\\Lib\\site-packages\\psychopy\\monitors目录，输入命令 python MonitorCenter.py Moniter类 Moniter类位于psychopy.monitors模块中，负责显示器参数和刺激环境设置。 # Create my primary monitormon = monitors.Monitor( &#x27;monitor1&#x27;, width=53.704, # width of the monitor in cm distance=114, # distance from viewer to the screen in cm notes=&quot;This is my primary monitor&quot;)mon.setSizePix((1920, 1080)) # set pixel size of the monitormon.save() # save the monitor information to disk# Reuse my primary monitormon = monitors.Monitor(&#x27;monitor1&#x27;)# Change the distance from viewer to the screenmon.setDistance(200) Monitor.save()函数保存的显示器信息位于%APPDATA%psychopy3monitors文件夹下，保存过一次后可以直接在Monitor类中以name调用。","categories":[{"name":"psychopy系列","slug":"psychopy系列","permalink":"https://mrswolf.github.io/categories/psychopy%E7%B3%BB%E5%88%97/"}],"tags":[{"name":"psychopy","slug":"psychopy","permalink":"https://mrswolf.github.io/tags/psychopy/"}]},{"title":"博客测试","slug":"blog-test","date":"2018-11-09T13:03:15.000Z","updated":"2018-11-09T13:03:15.000Z","comments":false,"path":"blog-test/","link":"blog-test","permalink":"https://mrswolf.github.io/blog-test/","excerpt":"","text":"swolf的博客开通啦！本博客记录我在学习过程中的心得体会。","categories":[],"tags":[]},{"title":"Psychopy安装和使用","slug":"psychopy-install","date":"2018-11-09T13:03:15.000Z","updated":"2022-06-04T06:49:00.000Z","comments":false,"path":"psychopy-install/","link":"psychopy-install","permalink":"https://mrswolf.github.io/psychopy-install/","excerpt":"什么是Psychopy Psychopy是基于Python的心理学实验设计软件，由英国诺丁汉大学的Jon Peirce主持开发。Psychopy结合了OpenGL的图形优势和Python的语法特性，给科学家们提供了快速构建高性能的图形刺激界面的工具。","text":"什么是Psychopy Psychopy是基于Python的心理学实验设计软件，由英国诺丁汉大学的Jon Peirce主持开发。Psychopy结合了OpenGL的图形优势和Python的语法特性，给科学家们提供了快速构建高性能的图形刺激界面的工具。 我为什么选择Psychopy 在我研究生阶段，我做脑机接口实验编写刺激界面的工具主要是Matlab平台的Psychtoolbox。很早之前，我也用过e-Prime，但很快就放弃了。e-Prime提供GUI界面，简单易学，但是无法设计复杂的刺激界面。相比之下，Psychtoolbox能够实现大多数脑-机接口刺激界面，同时基于Matlab平台，集成了大量简单方便的函数，对科研人员的编程要求不高，基本上是科研人员的第一选择。 然而对我而言，我一直不喜欢Matlab，理由有三： Matlab不是一门真正的通用编程语言。Matlab本质是为不懂CS的科研人员设计的编程语言，很难进行普通程序的开发，例如GUI、网络编程等等。脑机接口一个很重要的方面是开发和机器交互的程序，这些程序有时很注重性能，Matlab开发这些功能不太方便。 Matlab是收费的商业软件，其工具包的价格不是穷学生能承受的。尽管天朝存在“破解版”这种Matlab版本，MathWorks公司对科研人员使用破解版也视而不见，但谁知道以后会怎么样呢？为了不受制于人，我决定转向开源软件阵营。 最重要的一点，学Matlab找不到工作。研究生转博士期间，我也跟着校招参加了不少面试。很遗憾，在脑机接口领域，对口的工作几乎没有；就算扩大了从生物医学工程专业来说，大部分工作机会还是集中在医疗图像领域。这些领域的招聘要求中可没有熟练使用Matlab这一项，大部分还是C/C++、Java等通用编程语言。 综上所述，我在博士阶段毅然决然的放弃了Matlab，放弃了以前所有的代码，转向了Python。而在Python平台下，Psychopy几乎是唯一选择（个人认为）。Psychopy目前虽然仍处在开发阶段，还有不少bug，官方文档也不完善，但是官方社区和开发者相当活跃，使用人数也越来越多，借着Python语言的上升势头，我认为不久之后Psychopy很可能成为神经科学及脑机接口设计实验的首选框架。 当然由于Psychopy还很年轻，我在Psychopy实践过程中遇到许多问题。写Psychopy系列博文的目的是记录我的开发经验，给想使用Psyhcopy却遇到问题的朋友提供帮助。 安装Anaconda和Psychopy Anaconda是一个用于科学计算的Python发行版，集成了大量Python科学计算所需的环境库，提供包管理和环境管理的功能，免去了手动安装Python及其各种工具包的麻烦。Anaconda支持Linux、Mac和Windows系统，在Windows下几乎是科学计算的唯一选择。我推荐安装Anaconda或Miniconda的最新版本，Anaconda的下载地址可在其官网或清华TUNA镜像站找到。 Psychopy在2019年10月8号release了3.2.4 (PyPi上为3.2.3)，相比3.0.0添加了很多特性，修复了大量bug。尽管官方提供了多种安装方式，我仍建议使用Anaconda或Miniconda安装（pip会存在一些编译依赖缺失问题）。 Psychopy在Python3.6版本下较为稳定，在Anaconda Prompt中键入如下命令创建环境并安装: conda create -n psypy3 python=3.6conda activate psypy3conda install numpy scipy matplotlib pandas pyopengl pillow lxml openpyxl xlrd configobj pyyaml gevent greenlet msgpack-python psutil pytables requests[security] cffi seaborn wxpython cython pyzmq pyserialconda install -c conda-forge pyglet pysoundfile python-bidi moviepy pyosfpip install zmq json-tricks pyparallel sounddevice pygame pysoundcard psychopy_ext psychopy 在python控制台中运行如下命令检查Psychopy版本 import psychopyprint(psychopy.__version__) 使用Psychopy Psychopy提供两种刺激界面设计方式，一种是类似e-Prime的GUI界面Builder，另一种是普通的脚本编写方式Coder。 Builder 在Anaconda Prompt中切换到Psychopy的app安装目录，Windows下通常为cd %Anaconda的安装目录%\\Anaconda\\Lib\\site-packages\\psychopy\\app，在该目录下运行命令 python psychopyApp.py -b Builder的使用在Builder - building experiments in a GUI文档中有详细的介绍，builder很适合设计一些简单的刺激界面，设计完成的界面也可以转换为Coder中的脚本程序。 Coder 在Anaconda Prompt中切换到Psychopy的app安装目录，Windows下通常为%Anaconda的安装目录%\\Anaconda\\Lib\\site-packages\\psychopy\\app，在该目录下运行命令 python psychopyApp.py -c Coder的使用在Coder - writing experiments with scripts文档中有详细的介绍，相比Builder，Coder提供的编程设计方式更加灵活，可以实现更为复杂的刺激界面。当然Coder本身只是提供了开发环境，脚本编写可以在任何编辑器下进行，我很少直接使用Coder，通常会使用Pycharm和Sublime Text vscode 来编写程序。 Psychopy相关资源 Psychopy的官方文档更新不算及时，大部分文档还是基于Python2的版本Psychopy官方文档已更新至Python3，并不再支持Python2。官方文档和demo仍然是学习Psychopy的不二之选。如果有问题在官方文档里没有说明，Google也没有相关信息的话，可以去Psychopy的论坛问问或Github提个issue。 Psychopy 官方文档 Psychopy API手册 Psychopy 论坛 Psychopy Github仓库","categories":[{"name":"psychopy系列","slug":"psychopy系列","permalink":"https://mrswolf.github.io/categories/psychopy%E7%B3%BB%E5%88%97/"}],"tags":[{"name":"psychopy","slug":"psychopy","permalink":"https://mrswolf.github.io/tags/psychopy/"}]}],"categories":[{"name":"machine learning","slug":"machine-learning","permalink":"https://mrswolf.github.io/categories/machine-learning/"},{"name":"tools","slug":"tools","permalink":"https://mrswolf.github.io/categories/tools/"},{"name":"signal processing","slug":"signal-processing","permalink":"https://mrswolf.github.io/categories/signal-processing/"},{"name":"program","slug":"program","permalink":"https://mrswolf.github.io/categories/program/"},{"name":"linux","slug":"linux","permalink":"https://mrswolf.github.io/categories/linux/"},{"name":"distributed computing","slug":"distributed-computing","permalink":"https://mrswolf.github.io/categories/distributed-computing/"},{"name":"psychopy系列","slug":"psychopy系列","permalink":"https://mrswolf.github.io/categories/psychopy%E7%B3%BB%E5%88%97/"},{"name":"brain-computer interface","slug":"brain-computer-interface","permalink":"https://mrswolf.github.io/categories/brain-computer-interface/"}],"tags":[{"name":"machine learning","slug":"machine-learning","permalink":"https://mrswolf.github.io/tags/machine-learning/"},{"name":"matrix decomposition","slug":"matrix-decomposition","permalink":"https://mrswolf.github.io/tags/matrix-decomposition/"},{"name":"vscode","slug":"vscode","permalink":"https://mrswolf.github.io/tags/vscode/"},{"name":"python","slug":"python","permalink":"https://mrswolf.github.io/tags/python/"},{"name":"jupyter","slug":"jupyter","permalink":"https://mrswolf.github.io/tags/jupyter/"},{"name":"signal processing","slug":"signal-processing","permalink":"https://mrswolf.github.io/tags/signal-processing/"},{"name":"c++","slug":"c","permalink":"https://mrswolf.github.io/tags/c/"},{"name":"linux","slug":"linux","permalink":"https://mrswolf.github.io/tags/linux/"},{"name":"deep learning","slug":"deep-learning","permalink":"https://mrswolf.github.io/tags/deep-learning/"},{"name":"matlab","slug":"matlab","permalink":"https://mrswolf.github.io/tags/matlab/"},{"name":"psychopy","slug":"psychopy","permalink":"https://mrswolf.github.io/tags/psychopy/"},{"name":"brain-computer interface","slug":"brain-computer-interface","permalink":"https://mrswolf.github.io/tags/brain-computer-interface/"}]}