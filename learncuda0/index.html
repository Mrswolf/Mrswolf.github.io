<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1,maximum-scale=5,viewport-fit=cover"><title>cuda编程-基本概念 | swolf's blog</title><meta name="description" content="当今科学计算越来越复杂，各种模型越来越大，使用SVM做分类的时代早已一去不复返，通过GPU加速应用的重要性不言而喻。我在MRI领域工作的这几年，经常碰高度复杂的MRI应用，Matlab计算时间在几小时到几天不等，即使高度优化的C++的程序也需要10至30分钟左右。这些应用的CPU利用率已然接近100%，因此通过GPU进一步优化程序成为最可行的手段。"><meta property="og:type" content="article"><meta property="og:title" content="cuda编程-基本概念"><meta property="og:url" content="https://mrswolf.github.io/learncuda0/index.html"><meta property="og:site_name" content="swolf&#39;s blog"><meta property="og:description" content="当今科学计算越来越复杂，各种模型越来越大，使用SVM做分类的时代早已一去不复返，通过GPU加速应用的重要性不言而喻。我在MRI领域工作的这几年，经常碰高度复杂的MRI应用，Matlab计算时间在几小时到几天不等，即使高度优化的C++的程序也需要10至30分钟左右。这些应用的CPU利用率已然接近100%，因此通过GPU进一步优化程序成为最可行的手段。"><meta property="og:locale" content="en_US"><meta property="article:published_time" content="2024-05-17T16:00:00.000Z"><meta property="article:modified_time" content="2024-05-17T16:00:00.000Z"><meta property="article:author" content="swolf"><meta property="article:tag" content="脑机接口,神经科学,机器学习,swolf,计算机,脑科学,Python,深度学习,BCI,MRI,Deep Learning"><meta name="twitter:card" content="summary"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><link rel="icon" href="/images/favicon.ico" type="image/x-icon"><meta name="google-site-verification" content="dN2SLd9-x4zqC5VsgpROSppupvj3_eOvkDfKG32YSdc"><link rel="stylesheet" href="/css/iconfont.min.css"><link rel="stylesheet" href="/css/common.min.css"><link href="//cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.css" rel="stylesheet" crossorigin="anonymous"><script async src="https://www.googletagmanager.com/gtag/js?id=G-2JPFK1G1RZ"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-2JPFK1G1RZ")</script><meta name="generator" content="Hexo 6.3.0"></head><body><header class="header header-fixture"><div class="profile-search-wrap flex sm:block"><div class="profile sm:text-center md:px-1 lg:px-3 sm:pb-4 sm:pt-6"><a id="avatar" role="link" href="https://mrswolf.github.io" class="inline-block lg:w-16 lg:h-16 w-8 h-8 m-2" target="_blank" rel="noopener" rel="noreferrer"><img src="/images/avatar.jpg" class="rounded-full" alt="avatar"></a><h2 id="name" class="hidden lg:block">swolf</h2><h3 id="title" class="hidden lg:block">Engineer &amp; Coder &amp; Researcher</h3><small id="location" class="hidden lg:block"><i class="iconfont icon-map-icon"></i> Earth, Universe</small></div><div class="search flex-1 flex lg:inline-block sm:hidden lg:px-4 lg:mt-2 lg:mb-4 lg:w-full"><form id="search-form" class="my-auto flex-1 lg:border lg:border-solid lg:border-gray-200"><div class="input-group table bg-gray-100 lg:bg-white w-full"><input id="search-input" type="text" placeholder="Search" class="inline-block w-full bg-gray-100 lg:bg-white p-1"> <span class="table-cell"><button name="search tigger button" disabled><i class="iconfont icon-search m-2"></i></button></span></div></form><div id="content-json" data-placeholder="Search" class="invisible hidden">/content.json</div><script id="search-teamplate" type="text/html" data-path="/content.json"><div>
        <div class="search-header bg-gray-400">
            <input id="actual-search-input" model="keyword" ref="input" class="inline-block w-full h-10 px-2 py-1" placeholder="Search" type="text">
        </div>
        <div class="search-result bg-gray-200">
            {{#each searchPosts}}
            <a href="/{{ path }}" class="result-item block px-2 pb-3 mb-1 pt-1 hover:bg-indigo-100">
                <i class="iconfont icon-file"></i>
                <h1 class="result-title inline font-medium text-lg">{{ title }}</h1>
                <p class="result-content text-gray-600 text-sm">{{{ text }}}</p>
            </a>
            {{/each}}
        </div>
    </div></script></div><button name="menu toogle button" id="menu-toggle-btn" class="block sm:hidden p-3" role="button" aria-expanded="false"><i class="iconfont icon-hamburger"></i></button></div><nav id="menu-nav" class="hidden sm:flex flex-col"><div class="menu-item menu-home" role="menuitem"><a href="/."><i class="iconfont icon-home" aria-hidden="true"></i> <span class="menu-title">Home</span></a></div><div class="menu-item menu-archives" role="menuitem"><a href="/archives"><i class="iconfont icon-archive" aria-hidden="true"></i> <span class="menu-title">Archives</span></a></div><div class="menu-item menu-categories" role="menuitem"><a href="/categories"><i class="iconfont icon-folder" aria-hidden="true"></i> <span class="menu-title">Categories</span></a></div><div class="menu-item menu-tags" role="menuitem"><a href="/tags"><i class="iconfont icon-tag" aria-hidden="true"></i> <span class="menu-title">Tags</span></a></div><div class="menu-item menu-repository" role="menuitem"><a href="/repository"><i class="iconfont icon-project" aria-hidden="true"></i> <span class="menu-title">Repository</span></a></div><div class="menu-item menu-publications" role="menuitem"><a href="/publications"><i class="iconfont icon-folder" aria-hidden="true"></i> <span class="menu-title">Publications</span></a></div><div class="menu-item menu-about" role="menuitem"><a href="/about"><i class="iconfont icon-cup" aria-hidden="true"></i> <span class="menu-title">About</span></a></div><div class="social-links flex sm:flex-col lg:hidden mt-5"><span class="social-item text-center"><a target="_blank" rel="noopener" href="https://github.com/mrswolf"><i class="iconfont social-icon icon-github"></i> <span class="menu-title hidden lg:inline">menu.github</span> </a></span><span class="social-item text-center"><a href="/atom.xml"><i class="iconfont social-icon icon-rss"></i> <span class="menu-title hidden lg:inline">menu.rss</span></a></span></div></nav></header><section class="main-section"><main class="flex-1 px-4 py-14 md:px-5 lg:px-8 lg:py-4 relative min-h-screen"><article class="content article article-archives article-type-list" itemscope><header class="article-header"><h1 itemprop="name text-lg"><a class="article-title" href="/learncuda0" target="_blank" itemprop="url">cuda编程-基本概念</a></h1><p class="article-meta mb-3 text-xs"><span class="article-date"><i class="iconfont icon-calendar-check"></i><time datetime="2024-05-17T16:00:00.000Z" itemprop="datePublished">May18, 2024</time></span><span class="article-date"><i class="iconfont icon-calendar-check"></i><time datetime="2024-05-17T16:00:00.000Z" itemprop="datePublished">May18, 2024</time></span><span class="article-category"><i class="iconfont icon-folder"></i> <a class="article-category-link" href="/categories/cuda/">cuda</a> </span><span class="_partial/post-comment"><i class="icon icon-comment"></i> <a href="/learncuda0/#comments" class="article-comment-link">Comments </a></span><span class="post-wordcount" itemprop="wordCount">Word Count: 3k(words)</span> <span class="post-readcount" itemprop="timeRequired">Read Count: 12(minutes)</span></p></header><div class="marked-body article-body"><p>当今科学计算越来越复杂，各种模型越来越大，使用SVM做分类的时代早已一去不复返，通过GPU加速应用的重要性不言而喻。我在MRI领域工作的这几年，经常碰高度复杂的MRI应用，Matlab计算时间在几小时到几天不等，即使高度优化的C++的程序也需要10至30分钟左右。这些应用的CPU利用率已然接近100%，因此通过GPU进一步优化程序成为最可行的手段。<span id="more"></span></p><p>GPU市场依然是NVIDIA一家独大，AMD想要与之抗衡还需要时间。尤其在生态方面，NVIDIA基于CUDA的护城河太深，以至于科学计算大部分都使用N家的GPU。如果你已经拥有N家的GPU,恐怕使用CUDA编写核函数加速应用仍然是最好的选择。</p><p>在开始之前，介绍下我的基本配置和驱动情况：</p><ul><li>CPU: AMD Ryzen5 1600</li><li>RAM: 8G x 4</li><li>GPU: NVIDIA GTX 1080 x 2 (Driver: 550.67 CUDA: 12.4)</li><li>System: Manjaro Linux</li></ul><h2 id="基本编程模型">基本编程模型</h2><p>CUDA为GPU的硬件结构做了和CPU类似的抽象。以我的CPU配置为例，从硬件抽象来说，Ryzen5 1600有8个核心，每个核心有2个线程，因此可以同时运行16个线程。从软件抽象来说，我们则可以创建上百个逻辑线程，每一时刻最多有16个线程同时交给CPU运行，线程间的调度由操作系统负责，保证所有线程都有执行的可能性。</p><p>CUDA的GPU编程模型和CPU类似，thread是模型中的最小运行单位。从软件抽象来说，CUDA划分了grid、block和thread三个不同的层级，block包含一组固定数目的thread，而grid包含一组固定数目的block。比如以64个threads组成一个block,以10个blocks组成一个grid，那么这个grid实际上有10*64=640个thread。每个thread都会基于其所在的block有一个唯一的索引号（从0开始索引），而每个block都会基于所在的grid同样有一个唯一的索引号（从0开始索引）。因此每个thread在grid中的全局索引号是可以唯一确定下来的。block和grid总共可以有xyz三个维度（可以把它们想象成立方体）。</p><p>这种grid、block、thread的设计主要是为了耦合科学计算，因为科学计算的本质是去操作向量、矩阵等高维度的数组。比如我们希望相加两个<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>32</mn><mo>×</mo><mn>32</mn></mrow><annotation encoding="application/x-tex">32 \times 32</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.7278em;vertical-align:-.0833em"></span><span class="mord">32</span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:.6444em"></span><span class="mord">32</span></span></span></span>的矩阵<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">A</mi><mo separator="true">,</mo><mi mathvariant="bold">B</mi></mrow><annotation encoding="application/x-tex">\mathbf{A},\mathbf{B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8805em;vertical-align:-.1944em"></span><span class="mord mathbf">A</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathbf">B</span></span></span></span>，那么可以设计为1个2维block，每个block在xy方向各含有32个threads，每个thread只负责一次简单标量加法<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">C</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo>=</mo><msub><mi mathvariant="bold">A</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo>+</mo><msub><mi mathvariant="bold">B</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\mathbf{C}_{i,j} = \mathbf{A}_{i,j}+\mathbf{B}_{i,j}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.9722em;vertical-align:-.2861em"></span><span class="mord"><span class="mord mathbf">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3117em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:.05724em">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:.9722em;vertical-align:-.2861em"></span><span class="mord"><span class="mord mathbf">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3117em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:.05724em">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:.9722em;vertical-align:-.2861em"></span><span class="mord"><span class="mord mathbf">B</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3117em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:.05724em">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span></span></span></span>。理想情况下，我们可以让GPU同时执行这<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>32</mn><mo>×</mo><mn>32</mn></mrow><annotation encoding="application/x-tex">32 \times 32</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.7278em;vertical-align:-.0833em"></span><span class="mord">32</span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:.6444em"></span><span class="mord">32</span></span></span></span>个线程，在一次运算周期内完成矩阵加法的操作（相比让CPU每个线程执行标量加法的模型，我们总共需要循环<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>32</mn><mo>∗</mo><mn>32</mn><mi mathvariant="normal">/</mi><mn>16</mn><mo>=</mo><mn>64</mn></mrow><annotation encoding="application/x-tex">32 * 32 / 16=64</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6444em"></span><span class="mord">32</span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord">32/16</span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:.6444em"></span><span class="mord">64</span></span></span></span>次运算周期来覆盖所有的元素，所以说多就是快，多就是好）。</p><p>GPU在硬件上的抽象则没有CPU那么好懂，有几个NVIDIA的基本概念需要澄清。streaming multiprocessor(SM) 类比于CPU的核心，是互相独立的组件；cuda core是物理上运行1个thread的计算单元，类比于CPU的线程；一个SM可以包含很多的cuda cores（类比于CPU核心有两个线程）。与CPU不同的地方在于，GPU对SM里的cuda cores做了进一步的划分，<strong>每32个cuda cores视作一个warp，它是启动thread计算的最小单元</strong>。需要16个threads来做计算，GPU会启动一个warp（32个threads）。需要48个threads？GPU会启动两个warps（64个threads）。多出来的threads会执行同样的计算，只不过运行的结果会被丢弃掉。</p><p>实际运行时，一个或多个blocks由GPU负责调度交给一个SM去运行，硬件会将block中的threads按32划分成warps并执行。一个SM能同时执行多少个blocks取决于程序的设计方式和对资源的占用多少，这也是CUDA优化的重点之一，即尽量让SM的利用率变高。比如GTX 1080总共有20个SMs，每个SM包含128个cuda cores，因此每个SM可以最多同时运行4个warps。如果我的程序需要40个blocks，每个block包含48个threads，并且每个thread占用的资源都很少，理论上来说，GTX 1080可以同时在一个SM上启动2个block的计算，所有的SMs都能有效利用，只不过每个SM里有32个threads是在做无用的计算。相比之下，RTX 4080有76个SMs，因此可以同时处理更多的blocks。</p><p>CPU和GPU有独立的内存空间，因此数据必须先从CPU移动到GPU上，处理结束后再从GPU移动到CPU上。类比于熟知的DRAM、L3 cache、L2 cache和L1 cache，GPU分为global memory、local memory、L2 cache、shared memory、L1 cache和registers。最主要的区别当然是存取速度有着数量级上的提升。global memory是最慢的，但是所有thread都能访问，我们使用<code>cudaMalloc</code>分配内存也都在global memory上，host（CPU）也能够访问global memory。L2 cache充当global memory和SMs的桥梁，所有的SMs都能访问。shared memory只有同一个block中的thread能够访问，它的速度大概是global memory的100倍以上。L1 cache和shared memory也类似。registers则是每个thread独有的，速度最快。local memory则有点特别，它是在register放不下时候的缓存区，本质上就是global memory，每个thread只能访问自己的部分。</p><h2 id="NVCC编译">NVCC编译</h2><p>CUDA的核函数使用NVIDIA自定义的<a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#c-language-extensions">C++扩展语法</a>编写，不属于C++标准的一部分，因此常见的编译器是不支持CUDA代码的编译，必须使用NVIDIA的nvcc编译器。nvcc编译器做的事情类似于对常规编译器的扩展：</p><ul><li>先将CUDA核函数与常规C++代码分离开</li><li>将核函数编译成汇编（PTX）或者二进制（cubin）</li><li>用CUDA runtime函数替换原始代码中的核函数调用部分</li><li>调用常规C++编译器处理剩下的编译工作</li></ul><p>最基本的nvcc命令如下所示，将<code>vecadd.cu</code>编译成可执行二进制文件<code>vecadd</code>：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">nvcc vecadd.cu</span><br><span class="line">        -gencode <span class="built_in">arch</span>=compute_50,code=sm_50</span><br><span class="line">        -gencode <span class="built_in">arch</span>=compute_61,code=sm_61</span><br><span class="line">        -gencode <span class="built_in">arch</span>=compute_70,code=\&quot;compute_70,sm_70\&quot;</span><br><span class="line">        -o vecadd</span><br></pre></td></tr></table></figure><p>其中<code>arch</code>用来控制基于何种虚拟架构生成汇编代码，而<code>code</code>控制基于汇编如何生成和优化目标架构的二进制文件。所有的虚拟架构都以<code>compute_xy</code>表示，而真实架构都以<code>sm_xy</code>表示。我们主要重点关注<code>code</code>的部分即可。上述命令的意思是生成目标架构为<code>sm_50</code>的二进制，然后生成目标架构为<code>sm_61</code>的二进制，最后生成目标架构为<code>sm_70</code>的二进制和虚拟架构为<code>compute_70</code>的PTX代码。这些二进制和代码都被封装在同一个文件中，在实际执行时由CUDA运行时负责选择最接近的架构。</p><p>搞这么多复杂的架构主要还是为了兼容性的问题。NVIDIA有以下规定：</p><ul><li><code>code=sm_xy</code>生成的二进制只能在符合compute capability的<code>x.y</code>设备上运行，跨版本的二进制文件是不允许的</li><li>基于<code>arch=compute_ab</code>生成的PTX可以编译更高版本的二进制文件<code>code=sm_xy</code></li><li>基于<code>arch=compute_xy</code>生成的PTX不可以编译低版本的二进制文件<code>code=sm_ab</code></li><li>如果<code>code=compute_xy</code>，则可以依赖JIT编译在compute capability的<code>x.z</code>设备上运行，其中z&gt;=y</li><li>如果<code>code=compute_ab</code>，则可以依赖JIT编译在compute capability的<code>x.y</code>设备上运行，其中x.y的版本要大于a.b版本</li><li>基于<code>arch=compute_ab</code>生成的PTX不能编译高版本<code>code=compute_xy</code>，因为此时已经没办法附加更高版本的PTX代码进入最终文件，同理高版本<code>arch=compute_xy</code>也没有办法编译低版本<code>code=compute_ab</code></li></ul><p>GTX 1080的Compute Capability只到6.1，<a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities">只支持最基本硬件特性</a>。因此，上述编译的可执行文件可以在我的GPU上运行（因为存在和我的GPU架构匹配的二进制）。根据规则，下面命令编译的文件也可以通过JIT在我的GPU上运行，尽管二进制的目标架构是<code>sm_70</code>：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">nvcc vecadd.cu -gencode <span class="built_in">arch</span>=compute_61,code=\&quot;compute_61,sm_70\&quot; -o vecadd</span><br></pre></td></tr></table></figure><p>在实施最简单的demo前，首先让我们看下GTX 1080的基本参数：</p><ul><li>SMs: 20</li><li>Warps(CUDA Cores)/SM: 4(128)</li><li>Global Memory(GB): 8</li><li>L2 Cache(KB): 2048</li><li>Shared Memory(KB)/SM: 96</li><li>L1 Cache(KB)/SM: 48</li></ul><h2 id="矢量加法Demo">矢量加法Demo</h2><p>一个最简单的矢量加法demo如下：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> N 10000</span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">addvec</span><span class="params">(<span class="type">int</span> *a, <span class="type">int</span> *b, <span class="type">int</span> *c)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> iThreadIdx = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">    <span class="type">int</span> iStride = blockDim.x * gridDim.x;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = iThreadIdx; i &lt; N; i += iStride)</span><br><span class="line">    &#123;</span><br><span class="line">        c[i] = a[i] + b[i];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">initArray</span><span class="params">(<span class="type">int</span> *p, <span class="type">int</span> value)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; N; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        p[i] = value;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> *a, *b, *c;</span><br><span class="line">    a = (<span class="type">int</span> *)<span class="built_in">malloc</span>(N * <span class="built_in">sizeof</span>(<span class="type">int</span>));</span><br><span class="line">    b = (<span class="type">int</span> *)<span class="built_in">malloc</span>(N * <span class="built_in">sizeof</span>(<span class="type">int</span>));</span><br><span class="line">    c = (<span class="type">int</span> *)<span class="built_in">malloc</span>(N * <span class="built_in">sizeof</span>(<span class="type">int</span>));</span><br><span class="line"></span><br><span class="line">    <span class="built_in">initArray</span>(a, <span class="number">1</span>);</span><br><span class="line">    <span class="built_in">initArray</span>(b, <span class="number">2</span>);</span><br><span class="line">    <span class="built_in">initArray</span>(c, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> *da, *db, *dc;</span><br><span class="line">    <span class="built_in">cudaMalloc</span>(&amp;da, N * <span class="built_in">sizeof</span>(<span class="type">int</span>));</span><br><span class="line">    <span class="built_in">cudaMalloc</span>(&amp;db, N * <span class="built_in">sizeof</span>(<span class="type">int</span>));</span><br><span class="line">    <span class="built_in">cudaMalloc</span>(&amp;dc, N * <span class="built_in">sizeof</span>(<span class="type">int</span>));</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(da, a, N * <span class="built_in">sizeof</span>(<span class="type">int</span>), cudaMemcpyHostToDevice);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(db, b, N * <span class="built_in">sizeof</span>(<span class="type">int</span>), cudaMemcpyHostToDevice);</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> iThreads = <span class="number">64</span>;</span><br><span class="line">    <span class="type">int</span> iBlocks = (N - <span class="number">1</span>) / iThreads + <span class="number">1</span>;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Threads: &quot;</span> &lt;&lt; iThreads &lt;&lt; <span class="string">&quot; Blocks: &quot;</span> &lt;&lt; iBlocks &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    addvec&lt;&lt;&lt;iBlocks, iThreads&gt;&gt;&gt;(da, db, dc);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(c, dc, N * <span class="built_in">sizeof</span>(<span class="type">int</span>), cudaMemcpyDeviceToHost);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        std::cout &lt;&lt; c[i] &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    std::cout &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaFree</span>(da);</span><br><span class="line">    <span class="built_in">cudaFree</span>(db);</span><br><span class="line">    <span class="built_in">cudaFree</span>(dc);</span><br><span class="line">    <span class="built_in">free</span>(a);</span><br><span class="line">    <span class="built_in">free</span>(b);</span><br><span class="line">    <span class="built_in">free</span>(c);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>带有<code>__global__</code>的返回值为空的函数<code>addvec</code>就是在GPU上每个thread都运行的核函数，这里的<code>threadIdx</code>、<code>blockIdx</code>、<code>blockDim</code>和<code>gridDim</code>都是CUDA自己定义的内部变量，是uint的三元组，分别提供前面编程模型中提到的thread、block的索引以及block中thread的数量、block的数量。核函数就是利用这些信息让每个thread做相同的标量加法运算，但作用于不同的元素位置。此外，因为我们的元素数目要大于实际可用的cuda cores，所以要利用循环分批次处理数据。</p><p><code>main</code>函数中分别在host（CPU）和device（GPU）上申请内存，并将CPU的内容复制到GPU内存中。<code>addvec&lt;&lt;&lt;iBlocks, iThreads&gt;&gt;&gt;(da, db, dc)</code>调用核函数并立即返回（异步执行），通过<code>cudaDeviceSynchronize</code>来等待所有thread计算完毕。最后将计算完成的内容copy回host的内存上。考虑到GTX 1080的架构，每个block我选择64个threads（2个warps），这样理想情况下每个SM可以塞进两个blocks的计算。</p><h2 id="用Nsight分析程序性能">用Nsight分析程序性能</h2><p>NVIDIA提供Nsight Systems工具来帮助分析程序性能，执行以下命令：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">nsys profile --stats=<span class="literal">true</span> ./vecadd</span><br></pre></td></tr></table></figure><p>会对我们的vecadd可执行文件做详细的分析，输出各种运行时间、调用次数的报告。需要注意的是，nsys执行过程相当漫长，terminal上会显示“The target application terminated. One or more process it created re-parented. Waiting for termination of re-parented processes.”的提示，我一开始还以为是程序出错，其实是正常现象，只要耐心等待就好。执行完毕后，会在当前文件夹下生成<code>.nsys-rep</code>和<code>.sqlite</code>文件，以便后续追踪分析。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Time (%)  Total Time (ns)  Instances  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)             Name            </span><br><span class="line">--------  ---------------  ---------  --------  --------  --------  --------  -----------  ---------------------------</span><br><span class="line">   100.0             2881          1    2881.0    2881.0      2881      2881          0.0  addvec(int *, int *, int *)</span><br></pre></td></tr></table></figure><p>在上例中，我们的<code>addvec</code>核函数显示总运行时间是2.8ms；如果稍作修改，比如<code>iThreads=48</code>，运行时间则为：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Time (%)  Total Time (ns)  Instances  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)             Name            </span><br><span class="line">--------  ---------------  ---------  --------  --------  --------  --------  -----------  ---------------------------</span><br><span class="line">   100.0             3007          1    3007.0    3007.0      3007      3007          0.0  addvec(int *, int *, int *)</span><br></pre></td></tr></table></figure><p>可以看出时间稍长了一些，符合我们的预期，因为有些warp里的thread计算被浪费掉了。</p></div><blockquote class="copyright"><p><strong>Link to this article : </strong><a class="permalink" href="https://mrswolf.github.io/learncuda0/">https://mrswolf.github.io/learncuda0/</a></p><p><strong>This article is available under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener noreferrer">Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)</a> License</strong></p></blockquote></article></main><aside id="sidebar" class="aside aside-fixture"><div class="toc-sidebar"><nav id="toc" class="article-toc"><h3 class="toc-title">Catalogue</h3><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.</span> <span class="toc-text">基本编程模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#NVCC%E7%BC%96%E8%AF%91"><span class="toc-number">2.</span> <span class="toc-text">NVCC编译</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9F%A2%E9%87%8F%E5%8A%A0%E6%B3%95Demo"><span class="toc-number">3.</span> <span class="toc-text">矢量加法Demo</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%A8Nsight%E5%88%86%E6%9E%90%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD"><span class="toc-number">4.</span> <span class="toc-text">用Nsight分析程序性能</span></a></li></ol></nav></div></aside></section><footer class="hidden lg:block fixed bottom-0 left-0 sm:w-1/12 lg:w-1/6 bg-gray-100 z-40"><div class="footer-social-links"><a target="_blank" rel="noopener" href="https://github.com/mrswolf"><i class="iconfont icon-github"></i> </a><a href="/atom.xml"><i class="iconfont icon-rss"></i></a></div></footer><div id="mask" class="hidden mask fixed inset-0 bg-gray-900 opacity-75 z-40"></div><div id="search-view-container" class="hidden shadow-xl"></div><script src="/js/dom-event.min.js"></script><script src="/js/local-search.min.js"></script><script src="//cdn.jsdelivr.net/npm/lightgallery.js@1.1.3/dist/js/lightgallery.min.js"></script><script src="/js/light-gallery.min.js"></script></body></html>